[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Room: Hinds Hall 021\nTime: M/W 3:45PM - 5:05PM\nInstructor: Collin Capano\nEmail: cdcapano@syr.edu\nOffice hours: Tuesdays 12PM - 1PM and Wednesdays 2PM - 3PM in Hinds Hall 323G\nCourse website: https://su-ist356-m003-fall-2025.github.io/course-home",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Room: Hinds Hall 021\nTime: M/W 3:45PM - 5:05PM\nInstructor: Collin Capano\nEmail: cdcapano@syr.edu\nOffice hours: Tuesdays 12PM - 1PM and Wednesdays 2PM - 3PM in Hinds Hall 323G\nCourse website: https://su-ist356-m003-fall-2025.github.io/course-home",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nApproaches for building pipelines in data analytics using the Python programming language; data cleaning, extraction, wrangling, API’s, web scraping. Building data products. Programming experience required.\n\nAdditional Course Description\nThis course is a tour of programming techniques for building data pipelines for analytics. It will not just emphasize exploratory approaches, but also techniques to build extract transform load pipelines to run code in production. Throughout the course we will learn how to source data from a variety of sources (files, data streams, APIs, web scraping, etc.) and ultimately transform data as to prepare it for dashboards or machine learning. You will also learn some simple data visualization but that is not the primary emphasis of the course.\n\n\nPrerequisites\nThis courses uses the Python programming language. Proficiency n any programming language is the only pre-requisite. Students should have a clear understanding of these concepts:\n\nInput, output, variables and data types\nControl flow statements (if, for, while)\nFunctions (function definition, calling, parameters, return values)\nData structures (lists, dictionaries)\nUsing code in other libraries\n\n\n\nAudience: IST256 or IST356?\nThis course is intended as a follow up course to IST256. It is also appropriate for students with prior experience with programming who have an interest in data analytics.\n\nIST256 is for students with little to no programming experience. The course content is 75% python fundamentals and 25% python for data analytics.\nIST356 is for students with prior programming experience. The course content is 25% python fundamentals and 75% python for data analytics.\n\n\n\nCredits\n3 credits\n\n\nCourse Fees\nNone",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUpon completion of this course, students will be able to:\n\nExplain techniques for sourcing or transforming data, and be able to justify the choice of technique\nSolve data-oriented problems using programming techniques\nEvaluate different code modules and application programming interfaces for suitability\nApply data transformational programming techniques to build a larger data pipelines\nCreate production quality data pipelines from exploratory code",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks-and-supplies",
    "href": "syllabus.html#textbooks-and-supplies",
    "title": "Syllabus",
    "section": "Textbooks And Supplies",
    "text": "Textbooks And Supplies\n\nTextbooks\nThere is no textbook you need to purchase for this course. All required readings are available freely online, and in many cases, the online sources are the most up-to-date references to the relevant course material. See reading list section for additional details.\n\nMain Text\n\nPython Programming for Data Science, by T. Beuzen (2021) https://www.tomasbeuzen.com/python-programming-for-data-science/\n\n\n\n\nBring Your Own Device\nYou are expected to bring your computer to each class session. This class is very hands-on, and you will be programming in class often.\n\nSoftware to install\nThis course requires you to install software on your computer. The intro section of this course provides detailed instructions.\n\nVisual Studio Code editor\nGit Source Code Managment\nMiniconda\n\n\n\nAccounts\n\nWe will use Github for assignment submission.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-requirements-and-expectations",
    "href": "syllabus.html#course-requirements-and-expectations",
    "title": "Syllabus",
    "section": "Course Requirements and Expectations",
    "text": "Course Requirements and Expectations\n\nAttendance and Participation\nIn-class participation is the best way to learn and absorb the material. As such, you are expected to attend and participate in every class session. Your grade will be partially based on attendance (see below for details). If you must miss class, you are responsible for making up the work and catching up on what you missed. Lectures will be recorded and posted to the course website after the class.\n\n\nAssignments\nThe assignments are programming / problem solving activities that you will complete outside of class. The assignment due dates are posted on the course schedule. Consider these assignments formative assessments - practice so you can get better at computational thinking, problem solving and writing code. It is important to reflect upon your work and take an honest assessment of your abilities as you complete each assignment.\nEach Wednesday we will review the homework assignment. Students will be asked to contribute to the discussion.\nFactors affecting your grade:\n\nIs the code working and correct?\nWere the instructions followed? (e.g. commit after each function is written)\nGood reflection: specific, uses terminology, actionable\nHanded in on time, so your prof only needs to review submissions 1 time.\n\n\n\nExams\nExams are summative assessments. They are designed to test your knowledge of the material. The exams will be issued in class on the dates posted on the course schedule. Exams are closed-book but you may bring one 8.5x11 sheet of paper with your notes on it. Exams length is 60 minutes. They will be a mix of mix of multiple choice, short answer and code tracing and code writing.\n\n\nProject\nThe project is your opportunity to demonstrate what you have learned in an experiential fashion. In your final project you will be expected to create a data pipeline of your choosing.\n\nThe pipeline should incorporate techniques we learned in the course and the more techniques you incorporate correctly the higher your grade.\nIt is expected that you will be able to explain your choices, and they will be appropriate for the problem.\nThe pipeline should be first written exploratory, and then refactored into a production quality pipeline. Both pipelines should be submitted.\nThere should be a simple data visualization or dashboard from the pipeline output as to demonstrate its usefulness.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\n\n\n\n\n\n\n\n\n\n\nAssessment\nType\nLearning Outcomes\nQuantity\nPoints Each\nPoints Total\n\n\n\n\nAttendance\nFormative\n\n24\n1\n24\n\n\nAssignments\nFormative\n1, 2\n8\n3\n24\n\n\nExams\nSummative\n3, 4\n2\n24\n48\n\n\nProject\nSummative\n5\n1\n24\n24\n\n\nTotal\n\n\n\n\n120\n\n\n\n\nGrading Table\nThe following grading scale translates your total points earned into a letter grade to be submitted to the University registrar.\n\n\n\n\n\n\n\n\n\nStudent Achievement\nTotal Points Earned\nRegistrar Grade\nGrade Points\n\n\n\n\nMastery\n114 - 120\nA\n4.000\n\n\n\n108 - 113\nA-\n3.666\n\n\nSatisfactory\n102 - 107\nB+\n3.333\n\n\n\n96 - 101\nB\n3.000\n\n\n\n90 - 95\nB-\n2.666\n\n\nLow Passing\n84 - 89\nC+\n2.333\n\n\n\n78 - 83\nC\n2.000\n\n\n\n72 - 77\nC-\n1.666\n\n\nUnsatisfactory\n60 - 71\nD\n1.000\n\n\n\n0 - 59\nF\n0.000",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#use-of-ai-in-this-course",
    "href": "syllabus.html#use-of-ai-in-this-course",
    "title": "Syllabus",
    "section": "Use of AI in this Course",
    "text": "Use of AI in this Course\nBased on the assignments in this course and our specified learning outcomes, the full use of artificial intelligence as a tool, with disclosure and citation, is permitted in this course. Students do not need to ask permission to use these tools before starting an assignment or exam, but they must explicitly and fully indicate which tools were used and describe how they were used.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a pre-eminent and inclusive student-focused research institution, Syracuse University considers academic integrity at the forefront of learning, serving as a core value and guiding pillar of education. Syracuse University’s Academic Integrity Policy provides students with the necessary guidelines to complete academic work with integrity throughout their studies. Students are required to uphold both course-specific and university-wide academic integrity expectations such as crediting your sources, doing your own work, communicating honestly, and supporting academic integrity. The full Syracuse University Academic Integrity Policy can be viewed by visiting the Syracuse University Policies website.\nUpholding Academic Integrity includes the protection of faculty’s intellectual property. Students should not upload, distribute, or share instructors’ course materials, including presentations, assignments, exams, or other evaluative materials without permission. Using websites that charge fees or require uploading of course material (e.g., Chegg, Course Hero) to obtain exam solutions or assignments completed by others, which are then presented as your own violates academic integrity expectations in this course and may be classified as a Level 3 violation. All academic integrity expectations that apply to in-person assignments, quizzes, and exams also apply online.\nStudents found in violation of the policy are subject to grade sanctions determined by the course instructor and non-grade sanctions determined by the School or College where the course is offered. Students may not drop or withdraw from courses in which they face a suspected violation. Any established violation in this course may result in course failure regardless of violation level.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#class-schedule",
    "href": "syllabus.html#class-schedule",
    "title": "Syllabus",
    "section": "Class Schedule",
    "text": "Class Schedule\nNote: schedule subject to change as we progress through the semester.\n\n\n\nWeek\nDates\nTopic\n\n\n\n\n1\n8/25, 8/29\nIntro; CLI and Conda\n\n\n2\n9/3\nPython review - 1\n\n\n3\n9/8, 9/10\nPython review - 2\n\n\n4\n9/15, 9/17\nUI\n\n\n5\n9/22, 9/24\nData wrangling - 1\n\n\n6\n9/29, 10/1\nData wrangling - 2\n\n\n7\n10/6, 10/8\nData wrangling - 3\n\n\n8\n10/15\nData wrangling - 4\n\n\n9\n10/20, 10/22\nExam 1 (10/20); Web APIs - 1\n\n\n10\n10/27, 10/29\nWeb APIs - 2\n\n\n11\n11/3, 11/5\nWeb scraping - 1\n\n\n12\n11/10, 11/12\nWeb scraping - 2\n\n\n13\n11/17, 11/19\nData visualization - 1\n\n\n14\n12/1, 12/3\nData visualization - 2\n\n\n15\n12/8\nExam 2\n\n\n16\n12/15\nProject due",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\n\n\n\n\n\n\nDate\n\n\n\n\n\nMon. 8/25\nFirst day of class\n\n\nMon. 9/1\nLabor day - No class\n\n\nMon. 9/15\nAcademic/Financial drop deadline; Religious observance notification deadline\n\n\nMon. 10/13\nFall break - No class\n\n\nFri. 11/21\nWithdrawal deadline\n\n\n11/23-11/30\nThanksgiving Break - No class\n\n\nMon. 12/8\nLast day of class",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Syllabus"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-2.html",
    "href": "06_web_scraping/scraping-2.html",
    "title": "2. More Scraping + actionability",
    "section": "",
    "text": "Using locators to find elements on the page is a fundamental part of web scraping. In this notebook, we’ll learn how to use Playwright to find elements on the page using different types of locators.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "2. More Scraping + actionability"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-2.html#scraping-html-tables",
    "href": "06_web_scraping/scraping-2.html#scraping-html-tables",
    "title": "2. More Scraping + actionability",
    "section": "Scraping HTML tables",
    "text": "Scraping HTML tables\nWe saw previously that it’s easy to scrape HTML tables into a pandas dataframe using pd.read_html.\nPreviously, we provided read_html a URL. However, you can also give the read_url a page as read by Playwright. Here’s an example:\n\n# pw-pdtable.py\n\nfrom io import StringIO\nfrom playwright.sync_api import Playwright, sync_playwright\nimport pandas as pd\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://su-ist356-m003-fall-2025.github.io/course-home/syllabus.html\")\n\n    # ---------------------\n    # use pandas read_html to parse the HTML\n    # get a list of all tables on the page\n    dfs = pd.read_html(StringIO(page.content()))\n\n    # print the first table\n    print(dfs[0])\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "2. More Scraping + actionability"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-2.html#scraping-the-next-adjcent-element",
    "href": "06_web_scraping/scraping-2.html#scraping-the-next-adjcent-element",
    "title": "2. More Scraping + actionability",
    "section": "Scraping the next adjcent element",
    "text": "Scraping the next adjcent element\nSometimes you need to use one selector to find the element, but what we want is to scrape the next element right after the page.\nTo find the next adjacent sibling element, you use: .query_selector('~ *').\nHere’s an example of using this to select the first element in the Course Info section of the course syllabus (use the Inspect tool in your web browser to understand what’s going on here):\n\n# pw-scrape_next_example.py\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://su-ist356-m003-fall-2025.github.io/course-home/syllabus.html\")\n    \n    # ---------------------\n    # Let's get course info from the syllabus\n    info_details = page.query_selector(\"section#course-info &gt; h2.anchored\")\n    # Note: we could have alternatively selected the entire course\n    # section, then pulled out the h2 element, like this:\n    #course_info = page.query_selector(\"section#course-info\")\n    #info_details = course_info.query_selector(\"h2.anchored\")\n    # But the first way is more direct, as we only need the h2 element.\n    print(info_details.inner_text())\n    next_element = info_details.query_selector('~ *')\n    print(next_element.inner_text())\n\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\nUsing the next selector is useful if the information you’re trying to scrape doesn’t have a unique identifier, but something preceeding it (like a section header) does. It’s also useful if you’re not sure if, or don’t want to assume that, the website has a particular CSS selector for the information you want to retrieve.\n\n\n\n\n\n\nCautionCode Challenge 2.1\n\n\n\nScrape all the course info from the course syllabus:\nhttps://su-ist356-m003-fall-2025.github.io/course-home/syllabus.html\nDon’t assume any CSS identifier for the course info, just step through the section until you have retrieve it all.\nHint: Running query_selector('~ *') on the last child element in a parent element (like a section) will return None.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://su-ist356-m003-fall-2025.github.io/course-home/syllabus.html\")\n    \n    # ---------------------\n    # Let's get course info from the syllabus\n    info_details = page.query_selector(\"section#course-info &gt; h2.anchored\")\n    print(info_details.inner_text())\n    next_element = info_details.query_selector('~ *')\n    # The following while loop will continue until next_element is None. That\n    # will happen once we've retrieved all available info from the section.\n    while next_element:\n        print(next_element.inner_text())\n        next_element = next_element.query_selector('~ *')\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "2. More Scraping + actionability"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-2.html#downloading-an-image",
    "href": "06_web_scraping/scraping-2.html#downloading-an-image",
    "title": "2. More Scraping + actionability",
    "section": "Downloading an Image",
    "text": "Downloading an Image\nYou can use playwright to download an image by getting the src attribute.\nHere’s an example in which we download the Syracuse University logo from the university’s website:\nNote: the downloaded file is an SVG file.\n\n# pw-image_example.py\n\nfrom playwright.sync_api import Playwright, sync_playwright\nimport requests\n\ndef download_image(url): \n    filename = url.split(\"/\")[-1]\n    response = requests.get(url) \n    with open(filename, 'wb') as file: \n        file.write(response.content)\n    return filename\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    site = \"https://www.syracuse.edu/\"\n    page.goto(site)\n    # ---------------------\n    image = page.query_selector(\"a.site-header-logo-link &gt; img\")\n    image_source = image.get_attribute(\"src\")\n    print(f\"Downloading: {image_source}\")\n    filename = download_image(image_source)\n    print(f\"Saved to: {filename}\")\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "2. More Scraping + actionability"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-2.html#playwright-codegen",
    "href": "06_web_scraping/scraping-2.html#playwright-codegen",
    "title": "2. More Scraping + actionability",
    "section": "Playwright Codegen",
    "text": "Playwright Codegen\nPlaywright has a codegen feature that can help you generate code to interact with a webpage.\npython -m playwright codegen \nFor example, using code gen, we can generate the code needed to search for IST 356 in the course catalog:\n\n# pw-codegen_example.py\nimport re\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://coursecatalog.syracuse.edu/course-search/\")\n    page.get_by_role(\"textbox\", name=\"Keyword\").fill(\"IST 356\")\n    page.get_by_role(\"button\", name=\"SEARCH\").click()\n    page.get_by_role(\"link\", name=\"IST 356 Programming\").click()\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\n\n\n\n\n\n\nCautionCode Challenge 2.2\n\n\n\nUse the playwright codegen to extract the SU football schedule from https://cuse.com\nInput the year, output the schedule.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom playwright.sync_api import Playwright, sync_playwright, expect\nimport pandas as pd\nfrom time import sleep\nimport sys\n\ndef run(playwright: Playwright, year) -&gt; str:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(f\"https://cuse.com/sports/football/schedule/{year}\")\n    page.wait_for_load_state(\"load\")\n    sleep(1)\n    page.get_by_role(\"tab\", name=\"Table View not selected\").click()\n    sleep(1)\n    dfs = pd.read_html(page.content())\n    \n    \n    # ---------------------\n    context.close()\n    browser.close()\n    return dfs[0].to_html()\n\n\nwith sync_playwright() as playwright:\n    \n    html_table = run(playwright, year = 2023)\n    print (html_table)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "2. More Scraping + actionability"
    ]
  },
  {
    "objectID": "06_web_scraping/index.html",
    "href": "06_web_scraping/index.html",
    "title": "6. Web Scraping",
    "section": "",
    "text": "Introduction to Web Scraping\nMore Scraping + actionability",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/index.html#tutorials",
    "href": "06_web_scraping/index.html#tutorials",
    "title": "6. Web Scraping",
    "section": "",
    "text": "Introduction to Web Scraping\nMore Scraping + actionability",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/index.html#lectures",
    "href": "06_web_scraping/index.html#lectures",
    "title": "6. Web Scraping",
    "section": "Lectures",
    "text": "Lectures",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-2.html",
    "href": "05_web_apis/webapi-2.html",
    "title": "2. Posting with REST API and caching results",
    "section": "",
    "text": "So far we have only seen GET requests. But there are other HTTP methods that are used to perform different operations such as:\n\nPOST: Create a new resource\nPUT: Update an existing resource\nDELETE: Delete a resource\n\nFrom the perspective of requests these behave the same as a GET. The only subtle difference is that you can pass a data parameter when there are large payloads which cannot fit on the URL.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "2. Posting with REST API and caching results"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-2.html#http-methods-beyond-get",
    "href": "05_web_apis/webapi-2.html#http-methods-beyond-get",
    "title": "2. Posting with REST API and caching results",
    "section": "",
    "text": "So far we have only seen GET requests. But there are other HTTP methods that are used to perform different operations such as:\n\nPOST: Create a new resource\nPUT: Update an existing resource\nDELETE: Delete a resource\n\nFrom the perspective of requests these behave the same as a GET. The only subtle difference is that you can pass a data parameter when there are large payloads which cannot fit on the URL.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "2. Posting with REST API and caching results"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-2.html#example-of-a-post-request",
    "href": "05_web_apis/webapi-2.html#example-of-a-post-request",
    "title": "2. Posting with REST API and caching results",
    "section": "Example of a POST request",
    "text": "Example of a POST request\nLet’s do an example with the Azure sentiment API from the IoT Portal. This REST API requires the text to analyze, and will return the sentiment or “mood” of the text. Since there can be a substantial amount of text, the API requires the POST method, and the data to be sent in the body of the request.\n\n\n\n\n\n\nNote\n\n\n\nRecall that to run the commands below, you will need to get your API key from https://cent.ischool-iot.net. Log in, then copy the API key given there. In the code blocks below, replace the YOURAPIKEYHERE with what you copied.\n\n\nFirst, try it out in the IoT portal by clicking on the /api/azure/sentiment drop-down menu, then clicking “Execute”. For the text, enter: “I love IST356. It is the best course I’ve ever taken.”\nNow let’s convert the curl command the portal gave to Python requests code.\n\nimport requests\n\n'''\ncurl -X 'POST' \\\n  'https://cent.ischool-iot.net/api/azure/sentiment' \\\n  -H 'accept: application/json' \\\n  -H 'X-API-KEY: APIKEY' \\\n  -H 'Content-Type: application/x-www-form-urlencoded' \\\n  -d 'text=I%20love%20IST356.%20It%20is%20the%20best%20course%20I'\\''ve%20ever%20taken.'\n'''\n\napikey = 'YOURAPIKEYHERE'\nurl = 'https://cent.ischool-iot.net/api/azure/sentiment'\nheaders = {\n    'accept': 'application/json',\n    'X-API-KEY': apikey,\n    'Content-Type': 'application/x-www-form-urlencoded'\n}\ndata = {\n    \"text\": \"I love IST356. It is the best course I've ever taken.\"\n}\nresponse = requests.post(url, headers=headers, data=data)\nresponse.raise_for_status()\nresults = response.json()\nsentiment = results['results']['documents'][0]['sentiment']\nprint(sentiment)\n\n\n\n\n\n\n\nCautionCode Challenge 2.1\n\n\n\nFor this challenge, use Azure entity recognition API to extract entities from the following text.\nThe Philadelphia Eagles are a better team than the New York Giants this year. The Giants have lost 6 games and are at the bottom of the NFC East, while the Eagles are at the top of the division.\nUsing the API output, print each extracted entity and its type.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests\n\napikey = 'YOURAPIKEYHERE'\nurl = \"https://cent.ischool-iot.net/api/azure/entityrecognition\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"X-API-KEY\": apikey,\n    \"Content-Type\": \"application/x-www-form-urlencoded\",\n}\ndata = {\n    \"text\": \"The Philadelphia Eagles are a better team than the New York Giants this year. The Giants have lost 6 games and are at the bottom of the NFC East, while the Eagles are at the top of the division.\"\n}\nresponse = requests.post(url, headers=headers, data=data)\nresponse.raise_for_status()\ndata = response.json()\nfor d in data['results']['documents'][0]['entities']:\n    print(d['text'], d['category'])",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "2. Posting with REST API and caching results"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-2.html#caching-strategies",
    "href": "05_web_apis/webapi-2.html#caching-strategies",
    "title": "2. Posting with REST API and caching results",
    "section": "Caching Strategies",
    "text": "Caching Strategies\nWhen you are making a lot of requests to an API, it is a good idea to cache the results. We don’t want to make the same request over and over again if we don’t have to as this can effect our rate limits and thus our pricing.\nCaching can be done in a number of ways. The simplest method is a Python dictionary where the key is the request and the value is the response.\nThe caching strategy looks like this:\n\nCheck if the request is in the cache\nIf it is, we have a CACHE hit: return the response from the cache.\nIf it is not, call the API to get the response. Add it to the cache for the future.\n\nYou can do caching yourself by using Python dictionaries. However, if you want to cache results across sessions then you will need to save the dictionary to disk so it can be loaded in the future. You can use the Python pickle library to do that.\nHere’s an example with the Google Geocoding API on the IoT Portal:\n\nimport os\nimport pickle\n\npickle_file = 'geocode_cache.pkl'\nif os.path.exists(pickle_file):\n    with open(pickle_file, 'rb') as fp:\n        cache = pickle.load(fp)\nelse:\n    cache = {}\n\nlocation = 'Syracuse, NY'\ncache_key = location.lower()\n\nfor i in range(3):\n    try:\n        geo = cache[cache_key]\n        print('Used cache')\n    except KeyError:\n        print('Making request')\n        url = 'https://cent.ischool-iot.net/api/google/geocode'\n        headers = { 'X-API-KEY': apikey }\n        params = {'location': location}\n        response = requests.get(url, params=params, headers=headers)\n        response.raise_for_status()\n        geo = response.json()\n        # cache\n        cache[cache_key] = geo\n    print(geo['results'][0]['geometry']['location'])\n\n# save cache to disk for future use\nwith open(pickle_file, 'wb') as fp:\n    pickle.dump(cache, fp)\n\nThe first time you run that, you will see the request is only made the first time through the loop. If you run that code again, even the first time will come from the cache, since it will read it from disk.\n\nUsing requests-cache\nThe requests-cache Python package makes caching easy for you. It takes care of caching things to disk for you and loading them. It also lets you do things like set expiration times on cache entries, so you don’t end up using stagnant data.\n\n\n\n\n\n\nNote\n\n\n\nrequests-cache is not in the Python standard library, so you’ll need to install it. As always, you can do that by:\n\nIn a terminal, activate your ist356 conda environment.\nRun pip install requests-cache.\n\n\n\nHere’s the above example again, but using requests-cache:\n\nimport requests_cache\n\nlocation = 'Syracuse, NY'\n\n# start up a cached session. We need to explicitly set the allowable_methods\n# for it to cache posts\nrs = requests_cache.CachedSession(cache_name='geocode', allowable_methods=('GET', 'POST', 'HEAD')) \n# If you want to clear the cache:\n#rs.cache.clear()\n\nfor i in range(3):\n    url = 'https://cent.ischool-iot.net/api/google/geocode'\n    headers = { 'X-API-KEY': apikey }\n    params = {'location': location}\n    response = rs.get(url, params=params, headers=headers)\n    response.raise_for_status()\n    if response.from_cache:\n        print('Used cache')\n    else:\n        print('Made request')\n    geo = response.json()\n    print(geo['results'][0]['geometry']['location'])\n\nThat’s it! If you rerun the code above, you’ll find that it will use the cache the first time through. To clear the cache, uncomment the rs.cache.clear() line.\n\n\n\n\n\n\nCautionCode Challenge 2.2\n\n\n\nSend requests to the Azure sentiment API for each one of the sentences below. Use the requests_cache module to cache responses. For each of the following sentences, output the sentence, the sentiment, and if the results came from the API or the cache.\n\ntexts = [\n    \"I love the Syracuse Orange.\", \n    \"I hate the Duke Blue Devils.\",\n    \"I love the Syracuse Orange.\", \n    \"I don't like the Duke Blue Devils.\"\n]\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests_cache\n\ntexts = [\n    \"I love the Syracuse Orange.\", \n    \"I hate the Duke Blue Devils.\",\n    \"I love the Syracuse Orange.\", \n    \"I don't like the Duke Blue Devils.\"\n]\n\nrs = requests_cache.CachedSession(cache_name='sentiment', allowable_methods=('GET', 'POST', 'HEAD')) \nheaders = { 'x-api-key': apikey }\nurl = \"https://cent.ischool-iot.net/api/azure/sentiment\"\nfor text in texts:\n    data = {'text': text}\n    response = rs.post(url, headers=headers, data=data)\n    response.raise_for_status()\n    if response.from_cache:\n        from_cache = \"CACHED\"\n    else:\n        from_cache = \"NOT CACHED\"\n    results = response.json()\n    sentiment = results['results']['documents'][0]['sentiment']\n    print(text, sentiment, from_cache)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "2. Posting with REST API and caching results"
    ]
  },
  {
    "objectID": "05_web_apis/index.html",
    "href": "05_web_apis/index.html",
    "title": "5. Web APIs",
    "section": "",
    "text": "REST APIs\nPosting with REST API and caching results\nCreating your own APIs with FastAPI",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs"
    ]
  },
  {
    "objectID": "05_web_apis/index.html#tutorials",
    "href": "05_web_apis/index.html#tutorials",
    "title": "5. Web APIs",
    "section": "",
    "text": "REST APIs\nPosting with REST API and caching results\nCreating your own APIs with FastAPI",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs"
    ]
  },
  {
    "objectID": "05_web_apis/index.html#lectures",
    "href": "05_web_apis/index.html#lectures",
    "title": "5. Web APIs",
    "section": "Lectures",
    "text": "Lectures",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-4.html",
    "href": "04_data_wrangling/pandas-4.html",
    "title": "5. Basic data cleaning with Pandas",
    "section": "",
    "text": "In this lesson we will start learning how to clean a dataframe data and loop over it. To follow along with the commands below, download the following scratch notebook to your ist356 directory, then open it with VS Code:\nDownload scratch-pandas-4.ipynb\nFor this tutorial, we will be using some data representing a pretend restaurant’s transactions. The file is:\nchecks_data = 'https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/dining/check-data.csv'\n(This link is provided in the scratch notebook.)\nLet’s load the file with Pandas and sample a few of its rows to see what it contains:\nimport pandas as pd\n\nchecks = pd.read_csv(checks_data)\nchecks.sample(10)\n\n\n\n\n\n\n\n\ncheck\ndate\nparty size\ntotal items on check\ntotal amount of check\ngratuity\n\n\n\n\n30\n2705\n2024-07-08\n10\n19\n$838.85\n$671.08\n\n\n25\n4031\n2024-08-12\n6\n14\n$655.48\n$65.55\n\n\n8\n1066\n2024-08-20\n10\n22\n$485.76\n$77.72\n\n\n29\n4590\n2024-05-08\n3\n5\n$220.40\n$22.04\n\n\n38\n2341\n2024-06-03\n7\n16\n$1,118.88\n$313.29\n\n\n14\n3676\n2024-02-25\n1\n1\n$19.89\n$1.99\n\n\n34\n1368\n2024-12-21\n10\n25\n$2,193.00\n$372.81\n\n\n49\n3404\n2024-07-19\n9\n26\n$2,382.90\n$71.49\n\n\n13\n3867\n2024-05-02\n4\n14\n$499.10\n$119.78\n\n\n6\n2527\n2024-03-27\n6\n21\n$921.48\n$55.29\nLet’s use the info method to get some more information about the columns:\nchecks.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 6 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   check                  50 non-null     int64 \n 1   date                   50 non-null     object\n 2   party size             50 non-null     int64 \n 3   total items on check   50 non-null     int64 \n 4   total amount of check  50 non-null     object\n 5   gratuity               50 non-null     object\ndtypes: int64(3), object(3)\nmemory usage: 2.5+ KB\nThere’s something odd here! Note that the data type of some of the columns (e.g., total amount of check) are object instead of floats, as you might expect. As we’ll see below, this is because of the $ in the values; that will cause issues when we try to work with these columns. We’ll learn how to “clean” these columns so that we can do useful things with them.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "5. Basic data cleaning with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-4.html#apply",
    "href": "04_data_wrangling/pandas-4.html#apply",
    "title": "5. Basic data cleaning with Pandas",
    "section": "Apply",
    "text": "Apply\nThe apply method allows us to execute a function over a Series or the entire DataFrame.\nThe general syntax:\n\nSeries.apply(func) &lt;== call function func for every item in the Series.\nDataFrame.apply(func, axis=1) &lt;== call function func for every row in the DataFrame (axis=1 =&gt; row).\nDataFrame.apply(func, axis=0) &lt;== call function func for every column in the DataFrame (axis=0 =&gt; col).\n\nNote that the first argument apply is the function itself, not the function applied to some data. For example, suppose we define a function called sq that squares the input values:\n\ndef sq(x):\n    return x**2\n\nTo apply this to one of the columns in our DataFrame (say, the party size column):\n\nchecks['party size'].apply(sq)\n\n0      64\n1       9\n2      25\n3       4\n4      36\n5       1\n6      36\n7      64\n8     100\n9       1\n10     36\n11    100\n12     16\n13     16\n14      1\n15     25\n16     25\n17      9\n18     64\n19      4\n20     25\n21      1\n22     64\n23      4\n24    100\n25     36\n26     36\n27      1\n28     16\n29      9\n30    100\n31      9\n32      9\n33     36\n34    100\n35     16\n36      4\n37     81\n38     49\n39     49\n40      9\n41     49\n42     49\n43     25\n44     49\n45     81\n46      1\n47      1\n48     81\n49     81\nName: party size, dtype: int64\n\n\n\nWhy apply?\nIn the above example, you might wonder why we don’t just apply the function directly to the Series, rather than use apply. Afterall, in the first pandas tutorial we learned that Series are vectorized just like numpy arrays. In other words, why not just do:\n\nsq(checks['party size'])\n\n0      64\n1       9\n2      25\n3       4\n4      36\n5       1\n6      36\n7      64\n8     100\n9       1\n10     36\n11    100\n12     16\n13     16\n14      1\n15     25\n16     25\n17      9\n18     64\n19      4\n20     25\n21      1\n22     64\n23      4\n24    100\n25     36\n26     36\n27      1\n28     16\n29      9\n30    100\n31      9\n32      9\n33     36\n34    100\n35     16\n36      4\n37     81\n38     49\n39     49\n40      9\n41     49\n42     49\n43     25\n44     49\n45     81\n46      1\n47      1\n48     81\n49     81\nName: party size, dtype: int64\n\n\nIn this case, you could just run the function on the series. Where apply is useful is when you have more complicated functions, in particular, ones that need to do different things depending on what the input data is. For example, suppose we define the following function to group parties into small, medium, and large depending on how many people are in the party:\n\ndef classify_size(x):\n    if x &lt; 4:\n        return 'small'\n    elif x &lt; 8:\n        return 'medium'\n    else:\n        return 'large'\n\nIf we try to run this on the party size Series, we get an error:\n\nclassify_size(checks['party size'])\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/tmp/ipykernel_11312/1113461677.py in ?()\n----&gt; 1 classify_size(checks['party size'])\n\n/tmp/ipykernel_11312/2519828553.py in ?(x)\n      1 def classify_size(x):\n----&gt; 2     if x &lt; 4:\n      3         return 'small'\n      4     elif x &lt; 8:\n      5         return 'medium'\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/generic.py in ?(self)\n   1578     @final\n   1579     def __nonzero__(self) -&gt; NoReturn:\n-&gt; 1580         raise ValueError(\n   1581             f\"The truth value of a {type(self).__name__} is ambiguous. \"\n   1582             \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n   1583         )\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\n\nThis is because if statements cannot be vectorized like this. In contrast, the apply method does allow us to apply the function to the series:\n\nchecks['party size'].apply(classify_size)\n\n0      large\n1      small\n2     medium\n3      small\n4     medium\n5      small\n6     medium\n7      large\n8      large\n9      small\n10    medium\n11     large\n12    medium\n13    medium\n14     small\n15    medium\n16    medium\n17     small\n18     large\n19     small\n20    medium\n21     small\n22     large\n23     small\n24     large\n25    medium\n26    medium\n27     small\n28    medium\n29     small\n30     large\n31     small\n32     small\n33    medium\n34     large\n35    medium\n36     small\n37     large\n38    medium\n39    medium\n40     small\n41    medium\n42    medium\n43    medium\n44    medium\n45     large\n46     small\n47     small\n48     large\n49     large\nName: party size, dtype: object\n\n\nThis is because the apply method takes care to cycle over every element in the series and apply the function.\n\n\nCleaning data with apply\nThe ability of apply to apply more complicated functions involving if statements makes it extremely useful for cleaning datasets. By “cleaning” we mean reformatting data and/or removing spurios values, so that we can use it without issue.\nFor example, say we want to add price per item to our DataFrame, defined as:\nprice per item = total amount of check / total items on check\nThe problem is total amount of check is an object, not a float. This means we cannot do math on it:\n\n# This will raise a TypeError because of the dollar sign and commas!!!\nchecks['price_per_item'] = checks['total amount of check'] / checks['total items on check']\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:218, in _na_arithmetic_op(left, right, op, is_cmp)\n    217 try:\n--&gt; 218     result = func(left, right)\n    219 except TypeError:\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/computation/expressions.py:242, in evaluate(op, a, b, use_numexpr)\n    240     if use_numexpr:\n    241         # error: \"None\" not callable\n--&gt; 242         return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n    243 return _evaluate_standard(op, op_str, a, b)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/computation/expressions.py:73, in _evaluate_standard(op, op_str, a, b)\n     72     _store_test_result(False)\n---&gt; 73 return op(a, b)\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 # This will raise a TypeError because of the dollar sign and commas!!!\n----&gt; 2 checks['price_per_item'] = checks['total amount of check'] / checks['total items on check']\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.&lt;locals&gt;.new_method(self, other)\n     72             return NotImplemented\n     74 other = item_from_zerodim(other)\n---&gt; 76 return method(self, other)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/arraylike.py:210, in OpsMixin.__truediv__(self, other)\n    208 @unpack_zerodim_and_defer(\"__truediv__\")\n    209 def __truediv__(self, other):\n--&gt; 210     return self._arith_method(other, operator.truediv)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/series.py:6154, in Series._arith_method(self, other, op)\n   6152 def _arith_method(self, other, op):\n   6153     self, other = self._align_for_op(other)\n-&gt; 6154     return base.IndexOpsMixin._arith_method(self, other, op)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/base.py:1391, in IndexOpsMixin._arith_method(self, other, op)\n   1388     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n   1390 with np.errstate(all=\"ignore\"):\n-&gt; 1391     result = ops.arithmetic_op(lvalues, rvalues, op)\n   1393 return self._construct_result(result, name=res_name)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:283, in arithmetic_op(left, right, op)\n    279     _bool_arith_check(op, left, right)  # type: ignore[arg-type]\n    281     # error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\n    282     # \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\n--&gt; 283     res_values = _na_arithmetic_op(left, right, op)  # type: ignore[arg-type]\n    285 return res_values\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:227, in _na_arithmetic_op(left, right, op, is_cmp)\n    219 except TypeError:\n    220     if not is_cmp and (\n    221         left.dtype == object or getattr(right, \"dtype\", None) == object\n    222     ):\n   (...)    225         # Don't do this for comparisons, as that will handle complex numbers\n    226         #  incorrectly, see GH#32047\n--&gt; 227         result = _masked_arith_op(left, right, op)\n    228     else:\n    229         raise\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:163, in _masked_arith_op(x, y, op)\n    161     # See GH#5284, GH#5035, GH#19448 for historical reference\n    162     if mask.any():\n--&gt; 163         result[mask] = op(xrav[mask], yrav[mask])\n    165 else:\n    166     if not is_scalar(y):\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\nHow do we fix this? Let’s write a function to convert string values like this: $4,590.45 into floats like this: 4590.45\n\ndef clean_currency(value:str) -&gt; float:\n    '''\n    This function will take a string value and remove the dollar sign and commas\n    and return a float value.\n    '''\n    return float(value.replace(',', '').replace('$', ''))\n\n\n# tests\nassert clean_currency('$1,000.00') == 1000.00\nassert clean_currency('$1,000') == 1000.00\nassert clean_currency('1,000') == 1000.00\nassert clean_currency('$1000') == 1000.00\n\nWith our function written we can use apply() to transform the series:\n\nchecks['total_amount_of_check_cleaned'] = checks['total amount of check'].apply(clean_currency)\nchecks['price_per_item'] = checks['total_amount_of_check_cleaned'] / checks['total items on check']\nchecks.sample(10)\n\n\n\n\n\n\n\n\ncheck\ndate\nparty size\ntotal items on check\ntotal amount of check\ngratuity\ntotal_amount_of_check_cleaned\nprice_per_item\n\n\n\n\n22\n1336\n2024-08-30\n8\n28\n$1,199.80\n$275.95\n1199.80\n42.85\n\n\n9\n2968\n2024-12-28\n1\n3\n$122.97\n$23.36\n122.97\n40.99\n\n\n49\n3404\n2024-07-19\n9\n26\n$2,382.90\n$71.49\n2382.90\n91.65\n\n\n7\n1564\n2024-09-23\n8\n11\n$928.40\n$204.25\n928.40\n84.40\n\n\n21\n4440\n2024-06-11\n1\n3\n$168.96\n$10.14\n168.96\n56.32\n\n\n0\n2827\n2024-05-06\n8\n12\n$415.08\n$107.92\n415.08\n34.59\n\n\n44\n2053\n2024-12-14\n7\n23\n$588.11\n$164.67\n588.11\n25.57\n\n\n17\n3795\n2024-02-21\n3\n7\n$212.38\n$46.72\n212.38\n30.34\n\n\n2\n3685\n2024-12-07\n5\n5\n$252.95\n$50.59\n252.95\n50.59\n\n\n23\n1194\n2024-07-06\n2\n6\n$453.06\n$72.49\n453.06\n75.51\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember its a really good idea to track lineage when you are building a data pipeline.\nNEVER replace columns, always create new ones.\n\n\n\n\n\n\n\n\nCautionCode Challenge 5.1\n\n\n\nLet’s take what we did so far, and create a dataset that would be better prepared for analysis / machine learning. We’ll display it in a Streamlit app.\n\nCreate a script called st-cleaned_checks.py.\nAdd your clean_currency function to the script.\nLoad the checks dataset into a Pandas DataFrame and:\n\nClean the total amount of check and gratuity columns.\nCalculate the price_per_item as total amount of check / total items on check.\nCalcualte the price_per_person as total amont of check / party size.\nCalcualte the items_per_person as total items on check / party size.\nCalcualte the tip_percentage as the total amount of check / gratuity.\n\nDisplay the cleaned DataFrame along with a description of its data (i.e., display the output of describe) in a Streamlit app.\nTest that your app works by running it.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nThe st-cleaned_checks.py script:\n\n\nimport streamlit as st\nimport pandas as pd\n\ndef clean_currency(value:str) -&gt; float:\n    '''\n    This function will take a string value and remove the dollar sign and commas\n    and return a float value.\n    '''\n    return float(value.replace(',', '').replace('$', ''))\n\nst.title(\"Dining Check Data\")\n\n# load\nchecks = pd.read_csv('https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/dining/check-data.csv')\n\n# transformations\nchecks['total_amount_of_check_cleaned'] = checks['total amount of check'].apply(clean_currency)\nchecks['gratuity_cleaned'] = checks['gratuity'].apply(clean_currency)\nchecks['price_per_item'] = checks['total_amount_of_check_cleaned'] / checks['total items on check']\nchecks['price_per_person'] = checks['total_amount_of_check_cleaned'] / checks['party size']\nchecks['items_per_person'] = checks['total items on check'] / checks['party size']\nchecks['tip_percentage'] = checks['gratuity_cleaned'] / checks['total_amount_of_check_cleaned']\n\nst.dataframe(checks, width=1000)\n\nst.header(\"Summary:\")\nst.dataframe(checks.describe())\n\n\nTest it by running the following in the terminal (make sure to activate your conda environment first):\n\npython -m streamlit run st-cleaned_checks.py",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "5. Basic data cleaning with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-4.html#using-lambdas-to-apply-functions-to-multiple-columns",
    "href": "04_data_wrangling/pandas-4.html#using-lambdas-to-apply-functions-to-multiple-columns",
    "title": "5. Basic data cleaning with Pandas",
    "section": "Using lambdas to apply functions to multiple columns",
    "text": "Using lambdas to apply functions to multiple columns\nSo far we’ve used apply with functions that take in a single Series. What do we do if we need a function to operate on multiple columns in a DataFrame? For that, we can use Python lambda functions.\nAn example using our checks data:\nMarketing wants you to build some key performance indicators (KPIs) using the checks data. A KPI is a statistic that summarizes some larger data set, so the data can be more easily tracked over time. For example a letter grade such as an A- is a KPI summary of all your graded efforts to date.\nIn this example, marketing wants you to build two KPIs:\nKPI 1: Whales\nMarketing has decided to group customers into the following categories: - big eaters: Customers who are in the top 25% (i.e., above the 0.75 quantile) for items per person. - big spenders: Customers who are in the top 25% for price per person. - whale: Customers who are in the top 25% for both items per person and price per person.\nKPI 2: Tippers\nMarketing has decided to further group customers into the following categories based on their tipping: - light: Customers who are in the botton 25% (i.e., below the 0.25 quantile) by tip percentage. - heavy: Customers who are in the top 25% (i.e., above the 0.75 quantile) by tip percentage.\nTo calculate percentiles we will use the quantile Series method in Pandas. This returns the value at which X% of the data is below the given percentile.\nBefore we can apply our KPI’s we must write the functions!\n\nchecks['gratuity_cleaned'] = checks['gratuity'].apply(clean_currency)\nchecks['price_per_item'] = checks['total_amount_of_check_cleaned'] / checks['total items on check']\nchecks['price_per_person'] = checks['total_amount_of_check_cleaned'] / checks['party size']\nchecks['items_per_person'] = checks['total items on check'] / checks['party size']\nchecks['tip_percentage'] = checks['gratuity_cleaned'] / checks['total_amount_of_check_cleaned']\n\nTo categorize whales:\n\ndef detect_whale(\n        items_per_person:float, \n        price_per_person:float, \n        items_per_person_75th_pctile:float, \n        price_per_person_75_pctile:float) -&gt; str:\n    if items_per_person &gt; items_per_person_75th_pctile and price_per_person &gt; price_per_person_75_pctile:\n        return 'whale'\n    if items_per_person &gt; items_per_person_75th_pctile:\n        return 'big eater'\n    if price_per_person &gt; price_per_person_75_pctile:\n        return 'big spender'\n    return ''\n\nLet’s test our function using the quantile method:\n\n# tests\nppp_75 = checks['price_per_person'].quantile(0.75)\nipp_75 = checks['items_per_person'].quantile(0.75)\nprint(ppp_75, ipp_75)\nassert detect_whale(5, 250, 3, 175) == 'whale'\nassert detect_whale(5, 100, 3, 175) == 'big eater'\nassert detect_whale(1, 250, 3, 175) == 'big spender'\nassert detect_whale(1, 100, 3, 175) == ''\n\n158.35666666666668 3.0\n\n\nNow we want to apply the detect_whale function to the checks DataFrame. But detect_whale requires two columns as input, the items_per_person and price_per_person. How do we do that?\nWe can use a lambda function to quickly define a small function that will return the required columns when provided a row. In general, the syntax for a lambda function is lambda ARGS: FUNC. For example, lambda a, b: a+b will return the sum of the two arguments its given.\nIn our case, we can use a lambda function to pull out the needed columns and give them to apply, like so:\n\nchecks['whale'] = checks.apply(lambda row: detect_whale(row['items_per_person'], row['price_per_person'], ipp_75, ppp_75), axis=1)\nchecks.sample(25)\n\n\n\n\n\n\n\n\ncheck\ndate\nparty size\ntotal items on check\ntotal amount of check\ngratuity\ntotal_amount_of_check_cleaned\nprice_per_item\ngratuity_cleaned\nprice_per_person\nitems_per_person\ntip_percentage\nwhale\n\n\n\n\n31\n1945\n2024-02-05\n3\n7\n$132.86\n$21.26\n132.86\n18.98\n21.26\n44.286667\n2.333333\n0.160018\n\n\n\n28\n2446\n2024-12-15\n4\n12\n$575.64\n$28.78\n575.64\n47.97\n28.78\n143.910000\n3.000000\n0.049997\n\n\n\n25\n4031\n2024-08-12\n6\n14\n$655.48\n$65.55\n655.48\n46.82\n65.55\n109.246667\n2.333333\n0.100003\n\n\n\n13\n3867\n2024-05-02\n4\n14\n$499.10\n$119.78\n499.10\n35.65\n119.78\n124.775000\n3.500000\n0.239992\nbig eater\n\n\n9\n2968\n2024-12-28\n1\n3\n$122.97\n$23.36\n122.97\n40.99\n23.36\n122.970000\n3.000000\n0.189965\n\n\n\n42\n1361\n2024-11-21\n7\n14\n$65.80\n$16.45\n65.80\n4.70\n16.45\n9.400000\n2.000000\n0.250000\n\n\n\n3\n1957\n2024-02-15\n2\n2\n$42.44\n$8.91\n42.44\n21.22\n8.91\n21.220000\n1.000000\n0.209943\n\n\n\n38\n2341\n2024-06-03\n7\n16\n$1,118.88\n$313.29\n1118.88\n69.93\n313.29\n159.840000\n2.285714\n0.280003\nbig spender\n\n\n33\n3842\n2024-03-31\n6\n6\n$147.12\n$5.88\n147.12\n24.52\n5.88\n24.520000\n1.000000\n0.039967\n\n\n\n16\n3694\n2024-11-03\n5\n17\n$1,574.37\n$173.18\n1574.37\n92.61\n173.18\n314.874000\n3.400000\n0.110000\nwhale\n\n\n19\n3718\n2024-10-30\n2\n5\n$464.70\n$120.82\n464.70\n92.94\n120.82\n232.350000\n2.500000\n0.259996\nbig spender\n\n\n46\n3621\n2024-06-23\n1\n2\n$138.76\n$19.43\n138.76\n69.38\n19.43\n138.760000\n2.000000\n0.140026\n\n\n\n5\n2191\n2024-01-06\n1\n3\n$17.85\n$1.96\n17.85\n5.95\n1.96\n17.850000\n3.000000\n0.109804\n\n\n\n43\n1186\n2024-09-21\n5\n16\n$298.72\n$74.68\n298.72\n18.67\n74.68\n59.744000\n3.200000\n0.250000\nbig eater\n\n\n35\n2486\n2024-01-27\n4\n13\n$569.01\n$108.11\n569.01\n43.77\n108.11\n142.252500\n3.250000\n0.189997\nbig eater\n\n\n15\n2386\n2024-03-31\n5\n12\n$1,147.80\n$137.74\n1147.80\n95.65\n137.74\n229.560000\n2.400000\n0.120003\nbig spender\n\n\n1\n2443\n2024-06-09\n3\n10\n$286.40\n$31.50\n286.40\n28.64\n31.50\n95.466667\n3.333333\n0.109986\nbig eater\n\n\n34\n1368\n2024-12-21\n10\n25\n$2,193.00\n$372.81\n2193.00\n87.72\n372.81\n219.300000\n2.500000\n0.170000\nbig spender\n\n\n10\n2809\n2024-12-30\n6\n6\n$104.46\n$1.04\n104.46\n17.41\n1.04\n17.410000\n1.000000\n0.009956\n\n\n\n32\n1440\n2024-11-30\n3\n8\n$589.04\n$141.37\n589.04\n73.63\n141.37\n196.346667\n2.666667\n0.240001\nbig spender\n\n\n29\n4590\n2024-05-08\n3\n5\n$220.40\n$22.04\n220.40\n44.08\n22.04\n73.466667\n1.666667\n0.100000\n\n\n\n22\n1336\n2024-08-30\n8\n28\n$1,199.80\n$275.95\n1199.80\n42.85\n275.95\n149.975000\n3.500000\n0.229997\nbig eater\n\n\n48\n4161\n2024-06-22\n9\n28\n$1,385.16\n$235.48\n1385.16\n49.47\n235.48\n153.906667\n3.111111\n0.170002\nbig eater\n\n\n20\n3393\n2024-08-26\n5\n6\n$302.64\n$24.21\n302.64\n50.44\n24.21\n60.528000\n1.200000\n0.079996\n\n\n\n14\n3676\n2024-02-25\n1\n1\n$19.89\n$1.99\n19.89\n19.89\n1.99\n19.890000\n1.000000\n0.100050\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 5.2\n\n\n\nLet’s build on the Streamlit app you made in Challenge 5.1, and add our KPIs to it.\n\nCopy st-cleaned_checks.py to a new file called st-checks_kpi.py.\nAdd the detect_whale function defined above to st-checks_kpi.py.\nWrite a new function detect_tipper(tip_pct, tip_pcy_75th_pctile, tip_pct_25_pctile) that should return either “light” or “heavy” depending on whether the tip is below the 0.25 quantile or above the 0.75 quantile, respectively. If neither, the function should just return an empty string.\nApply the detect_whale and detect_tipper functions to the DataFrame, storing the results in new columns called whale and tipper, respectively.\nAs with before, diplay the modified DataFrame and the output of describe in a Streamlit app.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nIt’s easiest to do the copy using the command line. In the terminal, run:\n\ncp st-cleaned_checks.py st-checks_kpi.py\n\nNow modify st-checks_kpi.py. It should look like:\n\n\nimport streamlit as st\nimport pandas as pd\n\ndef clean_currency(value:str) -&gt; float:\n    '''\n    This function will take a string value and remove the dollar sign and commas\n    and return a float value.\n    '''\n    return float(value.replace(',', '').replace('$', ''))\n\ndef detect_whale(\n        items_per_person:float, \n        price_per_person:float, \n        items_per_person_75th_pctile:float, \n        price_per_person_75_pctile:float) -&gt; str:\n    if items_per_person &gt; items_per_person_75th_pctile and price_per_person &gt; price_per_person_75_pctile:\n        return 'whale'\n    if items_per_person &gt; items_per_person_75th_pctile:\n        return 'big eater'\n    if price_per_person &gt; price_per_person_75_pctile:\n        return 'big spender'\n    \n    return ''\n\n\ndef detect_tipper(tip_pct:float, tip_pct_75th_pctile:float, tip_pct_25th_pctile:float) -&gt; str:\n    if tip_pct &gt; tip_pct_75th_pctile:\n        return 'heavy tipper'\n    if tip_pct &lt; tip_pct_25th_pctile:\n        return 'light tipper'\n    return ''\n\nst.title(\"Dining Check Data\")\n\n# load\nchecks = pd.read_csv('https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/dining/check-data.csv')\n\n# transformations\nchecks['total_amount_of_check_cleaned'] = checks['total amount of check'].apply(clean_currency)\nchecks['gratuity_cleaned'] = checks['gratuity'].apply(clean_currency)\nchecks['price_per_item'] = checks['total_amount_of_check_cleaned'] / checks['total items on check']\nchecks['price_per_person'] = checks['total_amount_of_check_cleaned'] / checks['party size']\nchecks['items_per_person'] = checks['total items on check'] / checks['party size']\nchecks['tip_percentage'] = checks['gratuity_cleaned'] / checks['total_amount_of_check_cleaned']\n\n## The new stuff adding KPIs:\n# get KPI boundaries\nppp_75 = checks['price_per_person'].quantile(0.75)\nipp_75 = checks['items_per_person'].quantile(0.75)\ntp_75 = checks['tip_percentage'].quantile(0.75)\ntp_25 = checks['tip_percentage'].quantile(0.25)\n\n# Calcualte KPI's\nchecks['whale'] = checks.apply(lambda row: detect_whale(row['items_per_person'], row['price_per_person'], ipp_75, ppp_75), axis=1)\nchecks['tipper'] = checks.apply(lambda row: detect_tipper(row['tip_percentage'], tp_75, tp_25), axis=1)\n\n# Now display\nst.dataframe(checks, width=1000)\n\nst.header(\"Summary:\")\nst.dataframe(checks.describe())",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "5. Basic data cleaning with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-4.html#looping-over-dataframes",
    "href": "04_data_wrangling/pandas-4.html#looping-over-dataframes",
    "title": "5. Basic data cleaning with Pandas",
    "section": "Looping over Dataframes",
    "text": "Looping over Dataframes\nIf you must run a for loop over your DataFrames, there are two choices:\n\ndf.iterrows() dict-like iteration\ndf.itertuples() named-tuple like iteration (faster)\n\nLet’s do an example where we display the check number, whale and tipper for “heavy tipper” checks.\n\n## Using the iterrows() method\nprint(\"Total Amount of Whale Checks\")\nfor i,row in checks.iterrows():\n    if row['whale'] == 'whale':\n        print(i, row['check'], row['total_amount_of_check_cleaned'])\n\n\n# Same example with the itertuples() method\nprint(\"Total Amount of Whale Checks\")\nfor row in checks.itertuples():\n    if row.whale == 'whale':\n        print(row.check, row.total_amount_of_check_cleaned)\n\n\n# Of course you don't need a loop to do this:\nchecks[checks['whale'] == 'whale'][['check', 'total_amount_of_check_cleaned']]",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "5. Basic data cleaning with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html",
    "href": "04_data_wrangling/pandas-2.html",
    "title": "3. Data I/O with Pandas",
    "section": "",
    "text": "In this tutorial we’ll learn about how to load and save files using Pandas, including how to handle different file formats (csv, json, Excel, etc.).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#pandas-reads-data-in-a-variety-of-formats",
    "href": "04_data_wrangling/pandas-2.html#pandas-reads-data-in-a-variety-of-formats",
    "title": "3. Data I/O with Pandas",
    "section": "Pandas reads data in a variety of formats",
    "text": "Pandas reads data in a variety of formats\nExamples:\n\nText: CSV / Delimited\npd.read_csv(\"file.csv\", sep=\",\", header=0)\nSemi- Structured: JSON, HTML, XML\npd.read_json(\"file.json\", orient=\"records\")\nMicrosoft Excel\npd.read_excel(\"file.xlsx\", sheet_name=\"Sheet 1\")\nBig Data formats (ORC, Parquet, HDF5)\npd.read_parquet(\"file.parquet\")\nSQL Databases\n\nFor more details, see the Pandas IO documentation.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#pandas-can-read-from-almost-anywhere",
    "href": "04_data_wrangling/pandas-2.html#pandas-can-read-from-almost-anywhere",
    "title": "3. Data I/O with Pandas",
    "section": "Pandas can read from almost anywhere",
    "text": "Pandas can read from almost anywhere\n\nLocal files\npd_read_csv(\"./folder/file.csv\")\nFiles over the network using http / https\npd.read_csv(\"https://website/folder/file.csv\")\nFile-like: binary / text streams\n\n\nwith open('file.csv', 'r') as file:\n    data = file.read()\n    df = pd.read_csv(pd.compat.StringIO(data))  # text stream",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#reading-csv-delimited-text",
    "href": "04_data_wrangling/pandas-2.html#reading-csv-delimited-text",
    "title": "3. Data I/O with Pandas",
    "section": "Reading CSV / Delimited Text",
    "text": "Reading CSV / Delimited Text\nFor reading CSV files (or text files with other delimiters, such as tab), we use the read_csv() function.\n\nThis function is for processing text files one record per line with values separated by a delimiter (typically a ,, but can be any string).\nCommon named arguments:\n\nsep= the delimiter, default is a comma.\nheader= Which row, amongst those not skipped is the header\nnames= list of column names to use in the DataFrame\nskiprows= how many lines to skip before the data begins?\n\n\nSome examples of reading in the same data in different ways are below. In every case, the output is the same DataFrame:\n\nimport pandas as pd\n# To view the following files, see:\n# https://github.com/mafudge/datasets/tree/master/delimited\nlocation = \"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited\"\n\n# Header is first row, Comma-delimited\nstudents = pd.read_csv(f'{location}/students-header.csv') \n\n# No header in first row, Comma-delimited\nstudents = pd.read_csv(f'{location}/students-no-header.csv', header=None, names =['Name','Grade','Year'])\n\n# No header in first row, Pipe-delimited  \"|\"\nstudents = pd.read_csv(f'{location}/students-header.psv', sep=\"|\")\n\n# Header not in first row, header in 6th row, Comma-delimited\"\nstudents = pd.read_csv(f'{location}/students-header-blanks.csv', skiprows=5)\n\n# no header, data starts in 6th row, semicolon-delimited\"\nstudents = pd.read_csv(f'{location}/students-no-header-blanks.ssv', skiprows=5, header=None, sep=\";\", names =['Name','Grade','Year'])\n\nstudents\n\n\n\n\n\n\n\n\nName\nGrade\nYear\n\n\n\n\n0\nAbby\n7.0\nFreshman\n\n\n1\nBob\n9.0\nSophomore\n\n\n2\nChris\n10.0\nSenior\n\n\n3\nDave\n8.0\nFreshman\n\n\n4\nEllen\n7.0\nSophomore\n\n\n5\nFran\n10.0\nSenior\n\n\n6\nGreg\n8.0\nFreshman\n\n\n7\nHelen\nNaN\nSophomore\n\n\n8\nIris\n10.0\nSenior\n\n\n9\nJimmy\n8.0\nFreshman\n\n\n10\nKaren\n7.5\nFreshman\n\n\n11\nLynne\n10.0\nSophomore\n\n\n12\nMike\n10.0\nSophomore\n\n\n13\nNico\nNaN\nJunior\n\n\n14\nPete\n8.0\nFreshman\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.1\n\n\n\nRead this file into a Pandas DataFrame:\nhttps://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited/webtraffic.log\n\nWhat is the delimiter?\nIs there a header? Which row?\nDo you need to skip lines?\n\nDisplay only data where the time taken &gt; 500 (msec) and the sc-status is equal to 200.\nBonus: display the data in a Streamlit app.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport pandas as pd\n\nwt = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited/webtraffic.log\", skiprows=3, header=0, sep=\"\\s+\")\nwt.info() # colunmn info (to console only)\n\nwt_filter = (wt['sc-status'] == 200) & ( wt['time-taken'] &gt; 500)\nwt_slow_but_successful = wt[wt_filter]\nwt_slow_but_successful\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 489 entries, 0 to 488\nData columns (total 15 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   date             489 non-null    object\n 1   time             489 non-null    object\n 2   s-ip             489 non-null    object\n 3   cs-method        489 non-null    object\n 4   cs-uri-stem      489 non-null    object\n 5   cs-uri-query     489 non-null    object\n 6   s-port           489 non-null    int64 \n 7   cs-username      489 non-null    object\n 8   c-ip             489 non-null    object\n 9   cs(User-Agent)   489 non-null    object\n 10  cs(Referer)      489 non-null    object\n 11  sc-status        489 non-null    int64 \n 12  sc-substatus     489 non-null    int64 \n 13  sc-win32-status  489 non-null    int64 \n 14  time-taken       489 non-null    int64 \ndtypes: int64(5), object(10)\nmemory usage: 57.4+ KB\n\n\n&lt;&gt;:3: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:3: SyntaxWarning: invalid escape sequence '\\s'\n/tmp/ipykernel_11215/568368627.py:3: SyntaxWarning: invalid escape sequence '\\s'\n  wt = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited/webtraffic.log\", skiprows=3, header=0, sep=\"\\s+\")\n\n\n\n\n\n\n\n\n\ndate\ntime\ns-ip\ncs-method\ncs-uri-stem\ncs-uri-query\ns-port\ncs-username\nc-ip\ncs(User-Agent)\ncs(Referer)\nsc-status\nsc-substatus\nsc-win32-status\ntime-taken\n\n\n\n\n32\n2016-02-11\n17:16:17\n128.230.247.37\nGET\n/desktops\n-\n80\n-\n215.82.23.2\nMozilla/5.0+(Windows+NT+10.0;+WOW64;+rv:43.0)+...\nhttp://group0.ist722.ischool.syr.edu/\n200\n0\n0\n1144\n\n\n93\n2016-02-11\n17:16:59\n128.230.247.37\nGET\n/adidas-consortium-campus-80s-running-shoes\n-\n80\n-\n128.122.140.238\nMozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKi...\nhttp://group0.ist722.ischool.syr.edu/shoes\n200\n0\n0\n701\n\n\n105\n2016-02-11\n17:17:07\n128.230.247.37\nGET\n/htc-one-m8-android-l-50-lollipop\n-\n80\n-\n128.122.140.238\nMozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKi...\nhttp://group0.ist722.ischool.syr.edu/\n200\n0\n0\n613\n\n\n158\n2016-02-11\n18:03:08\n128.230.247.37\nGET\n/digital-downloads\n-\n80\n-\n74.111.6.173\nMozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKi...\nhttp://group0.ist722.ischool.syr.edu/register\n200\n0\n0\n572\n\n\n184\n2016-02-11\n18:03:58\n128.230.247.37\nGET\n/jewelry\n-\n80\n-\n74.111.6.173\nMozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKi...\nhttp://group0.ist722.ischool.syr.edu/cart\n200\n0\n0\n516\n\n\n281\n2016-02-11\n18:07:48\n128.230.247.37\nGET\n/electronics\n-\n80\n-\n172.189.252.8\nMozilla/5.0+(Macintosh;+Intel+Mac+OS+X+10_11_3...\nhttp://group0.ist722.ischool.syr.edu/\n200\n0\n0\n704\n\n\n293\n2016-02-11\n18:07:50\n128.230.247.37\nGET\n/apparel\n-\n80\n-\n172.189.252.8\nMozilla/5.0+(Macintosh;+Intel+Mac+OS+X+10_11_3...\nhttp://group0.ist722.ischool.syr.edu/electronics\n200\n0\n0\n645\n\n\n303\n2016-02-11\n18:07:53\n128.230.247.37\nGET\n/computers\n-\n80\n-\n172.189.252.8\nMozilla/5.0+(Macintosh;+Intel+Mac+OS+X+10_11_3...\nhttp://group0.ist722.ischool.syr.edu/apparel\n200\n0\n0\n703\n\n\n399\n2016-02-11\n19:55:38\n128.230.247.37\nGET\n/if-you-wait\n-\n80\n-\n8.37.70.99\nAddThis.com+(http://support.addthis.com/)\n-\n200\n0\n0\n539\n\n\n\n\n\n\n\nTo display in a Streamlit app, put the above in a script file (say, webtraffic.py) and add the following:\n\nimport streamlit as st\n\nst.title(\"Webtraffic Data\")\nst.dataframe(wt_slow_but_successful) # first 20 rows\n\nThen in a terminal run:\npython -m streamlit run webtraffic.py",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#reading-json-text",
    "href": "04_data_wrangling/pandas-2.html#reading-json-text",
    "title": "3. Data I/O with Pandas",
    "section": "Reading JSON Text",
    "text": "Reading JSON Text\nTo load JSON files as a Pandas DataFrame we use the read_json() function. Examples:\n\npd.read_json(\"file.json\", orient=\"columns\")\npd.read_json(\"file.json\", orient=\"records\", lines=True) &lt;== Line-oriented json\n\nOrientations: - split: dict like {index -&gt; [index]; columns -&gt; [columns]; data -&gt; [values]}\n\nrecords: list like [{column -&gt; value} …]\nindex: dict like {index -&gt; {column -&gt; value}}\ncolumns: dict like {column -&gt; {index -&gt; value}}\nvalues: just the values array\ntable: dict adhering to the JSON Table Schema https://specs.frictionlessdata.io/table-schema/#descriptor\n\nFor more on reading JSON files, see the Pandas Reading JSON guide.\nSome examples of reading in the same JSON data in different ways follows. In every case, the output is the same DataFrame:\n\n# https://github.com/mafudge/datasets/tree/master/json-formats to view the files\nlocation = \"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/json-formats\"\n\n# Row-oriented JSON [ { \"Name\": \"Alice\", \"Grade\": 12, \"Year\": 2021 }, { \"Name\": \"Bob\", \"Grade\": 11, \"Year\": 2022 } ]\nstudents = pd.read_json(f'{location}/students-records.json', orient='records')\n\n# line-oriented JSON { \"Name\": \"Alice\", \"Grade\": 12, \"Year\": 2021 }\\n { \"Name\": \"Bob\", \"Grade\": 11, \"Year\": 2022 }\\n\nstudents = pd.read_json(f'{location}/students-lines.json', orient='records', lines=True)\n\n# column-oriented JSON { \"Name\": [\"Alice\", \"Bob\"], \"Grade\": [12, 11], \"Year\": [2021, 2022] }\nstudents = pd.read_json(f'{location}/students-columns.json', orient='columns')\n\nstudents\n\n\n\n\n\n\n\n\nName\nGrade\nYear\n\n\n\n\n0\nAbby\n7.0\nFreshman\n\n\n1\nBob\n9.0\nSophomore\n\n\n2\nChris\n10.0\nSenior\n\n\n3\nDave\n8.0\nFreshman\n\n\n4\nEllen\n7.0\nSophomore\n\n\n5\nFran\n10.0\nSenior\n\n\n6\nGreg\n8.0\nFreshman\n\n\n7\nHelen\nNaN\nSophomore\n\n\n8\nIris\n10.0\nSenior\n\n\n9\nJimmy\n8.0\nFreshman\n\n\n10\nKaren\n7.5\nFreshman\n\n\n11\nLynne\n10.0\nSophomore\n\n\n12\nMike\n10.0\nSophomore\n\n\n13\nNico\nNaN\nJunior\n\n\n14\nPete\n8.0\nFreshman\n\n\n\n\n\n\n\n\nHandling Nested JSON\nThe read_json() method does not perform well on nested JSON structures. For example consider the following JSON file of customer orders:\nThe file orders.json:\n[\n    {\n        \"Customer\" : { \"FirstName\" : \"Abby\", \"LastName\" : \"Kuss\"}, \n        \"Items\" : [\n            { \"Name\" : \"T-Shirt\", \"Price\" : 10.0, \"Quantity\" : 3},\n            { \"Name\" : \"Jacket\", \"Price\" : 20.0, \"Quantity\" : 1}\n        ]\n    },\n    {\n        \"Customer\" : { \"FirstName\" : \"Bette\", \"LastName\" : \"Alott\"}, \n        \"Items\" : [\n            { \"Name\" : \"Shoes\", \"Price\" : 25.0, \"Quantity\" : 1}, \n            { \"Name\" : \"Jacket\", \"Price\" : 20.0, \"Quantity\" : 1}\n        ]\n    },\n    {\n        \"Customer\" : { \"FirstName\" : \"Chris\", \"LastName\" : \"Peanugget\"}, \n        \"Items\" : [\n            { \"Name\" : \"T-Shirt\", \"Price\" : 10.0, \"Quantity\" : 1}\n        ]\n    }\n]\nWhen we read this with read_json() we get the three orders but only two columns — one for the \"Customer\" key, and the other for the \"Items\" key:\n\norders = pd.read_json(\"https://raw.githubusercontent.com/mafudge/datasets/master/json-samples/orders.json\")\norders\n\n\n\n\n\n\n\n\nCustomer\nItems\n\n\n\n\n0\n{'FirstName': 'Abby', 'LastName': 'Kuss'}\n[{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity'...\n\n\n1\n{'FirstName': 'Bette', 'LastName': 'Alott'}\n[{'Name': 'Shoes', 'Price': 25.0, 'Quantity': ...\n\n\n2\n{'FirstName': 'Chris', 'LastName': 'Peanugget'}\n[{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity'...\n\n\n\n\n\n\n\nWhat we want is one row per item on the the order and the customer name to be in separate columns. The json_normalize() function can help here.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that json_normalize() does not take a file as input, but rather de-serialized json (i.e., a dict or list of dicts).\n\n\nAn example (note that we first need to load the file as JSON dict; for that, we’ll use the requests module to download the data):\n\n# first down load the data\nimport requests\nresponse = requests.get(\"https://raw.githubusercontent.com/mafudge/datasets/master/json-samples/orders.json\")\njson_data = response.json()  #de-serialize\nprint('Original JSON data:')\nprint(json_data)\n# now load into a DataFrame\norders = pd.json_normalize(json_data)\nprint(\"\\nLoaded DataFrame:\")\norders\n\nOriginal JSON data:\n[{'Customer': {'FirstName': 'Abby', 'LastName': 'Kuss'}, 'Items': [{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity': 3}, {'Name': 'Jacket', 'Price': 20.0, 'Quantity': 1}]}, {'Customer': {'FirstName': 'Bette', 'LastName': 'Alott'}, 'Items': [{'Name': 'Shoes', 'Price': 25.0, 'Quantity': 1}, {'Name': 'Jacket', 'Price': 20.0, 'Quantity': 1}]}, {'Customer': {'FirstName': 'Chris', 'LastName': 'Peanugget'}, 'Items': [{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity': 1}]}]\n\nLoaded DataFrame:\n\n\n\n\n\n\n\n\n\nItems\nCustomer.FirstName\nCustomer.LastName\n\n\n\n\n0\n[{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity'...\nAbby\nKuss\n\n\n1\n[{'Name': 'Shoes', 'Price': 25.0, 'Quantity': ...\nBette\nAlott\n\n\n2\n[{'Name': 'T-Shirt', 'Price': 10.0, 'Quantity'...\nChris\nPeanugget\n\n\n\n\n\n\n\nBetter but this only processed nested dict and not nested list. We still need to handle the list of Items. To accomplish this we :\n\nSet the record_path to be the nested list 'Items'. This tells json_normalize() to use that JSON key as the row level. So now we will have 5 rows (one for each item) instead of 3.\nThen we set the meta named argument to a list of each of the other values we wish to include, in this instance last name and first name.\n\nNote: The meta syntax is a bit weird. It’s a list of JSON paths (also represented as lists) to each item in the JSON. For example:\nThe meta Argument        ==&gt; Matches This in the JSON           ==&gt; And Displays As This Pandas Column\n[\"Customer\",\"FirstName\"] ==&gt; { \"Customer\" : { \"FirstName\": ...} ==&gt; Customer.Firstname\n\norders = pd.json_normalize(json_data, record_path=\"Items\", meta=[[\"Customer\",\"FirstName\"],[\"Customer\",\"LastName\"]])\norders\n\n\n\n\n\n\n\n\nName\nPrice\nQuantity\nCustomer.FirstName\nCustomer.LastName\n\n\n\n\n0\nT-Shirt\n10.0\n3\nAbby\nKuss\n\n\n1\nJacket\n20.0\n1\nAbby\nKuss\n\n\n2\nShoes\n25.0\n1\nBette\nAlott\n\n\n3\nJacket\n20.0\n1\nBette\nAlott\n\n\n4\nT-Shirt\n10.0\n1\nChris\nPeanugget\n\n\n\n\n\n\n\nYes it seems complicated, because conceptually it is a bit complicated. Let’s try another example, with some abstract values.\nIn the following example we want to generate a normalized table with 3 rows and 4 columns.\n\nThe rows are based on the \"A\" record_path, which has two sub-sets, A1 and A2. There are three sets of A data: (101, 102); (111, 112); and (201, 202).\nThe meta data are based on columns \"B\", and \"C1\"\n\n\njson_data = [\n    {\n        \"A\": [\n            {\"A1\": 101, \"A2\": 102},\n            {\"A1\": 111, \"A2\": 112}\n        ],\n        \"B\": 103,\n        \"C\": {\"C1\": 104}\n    },\n    {\n        \"A\": [\n            {\"A1\": 201, \"A2\": 202}\n        ],\n        \"B\": 203,\n        \"C\": {\"C1\": 204}\n    }\n]\n\ndf = pd.json_normalize(json_data, record_path=\"A\", meta=[\"B\", [\"C\", \"C1\"]])\ndf\n\n\n\n\n\n\n\n\nA1\nA2\nB\nC.C1\n\n\n\n\n0\n101\n102\n103\n104\n\n\n1\n111\n112\n103\n104\n\n\n2\n201\n202\n203\n204\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.2\n\n\n\nUse the json_normalize function to tabularize this JSON data:\nhttps://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/json-samples/employees.json\nThe final table should have these columns: dept, age, firstname, lastname.\nHint: read the file using the requests module, like in the above example.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests\nimport pandas as pd\n\nresponse = requests.get(\"https://raw.githubusercontent.com/mafudge/datasets/master/json-samples/employees.json\")\nemployees = response.json()\nemployees_df = pd.json_normalize(employees, record_path=[\"employees\"], meta=[\"dept\"])\nemployees_df\n\n\n\n\n\n\n\n\nfirstName\nlastName\nage\ndept\n\n\n\n\n0\nJohn\nDoe\n23\naccounting\n\n\n1\nMary\nSmith\n32\naccounting\n\n\n2\nSally\nGreen\n27\nsales\n\n\n3\nJim\nGalley\n41\nsales",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#reading-excel-files",
    "href": "04_data_wrangling/pandas-2.html#reading-excel-files",
    "title": "3. Data I/O with Pandas",
    "section": "Reading Excel files",
    "text": "Reading Excel files\nExcel files can be read using the read_excel() function. For example: pd.read_excel('file.xlsx', sheet_name=None)\n\n\n\n\n\n\nNote\n\n\n\nIn order to use the read_excel method, you need to additional install the optional Pandas dependency openpyxl. To do that using pip, run:\npip install openpyxl\n\n\nThis will read in all sheets as a dict, with the sheet names as the keys and the values as Pandas DataFrames representing the contents. An example using this with Streamlit:\n\nimport streamlit as st\nimport pandas as pd\n\nst.title(\"Excel Example - multiple sheets\")\n\ncontents = pd.read_excel(\"https://github.com/mafudge/datasets/raw/refs/heads/master/excel-examples/books_of_interest.xlsx\", sheet_name=None)\n\n# names of sheets in the excel file its a dictionary\nsheets = list(contents.keys()) \n\n# make tabs for each sheet\ntabs = st.tabs(sheets)\n\n#loop through each tab and write the contents of the sheet to the tab\nfor i in range(len(tabs)):\n    df = contents[sheets[i]]\n    tabs[i].dataframe(df)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#reading-html-tables",
    "href": "04_data_wrangling/pandas-2.html#reading-html-tables",
    "title": "3. Data I/O with Pandas",
    "section": "Reading HTML Tables",
    "text": "Reading HTML Tables\nYou can scrape an HTML table off a webpage using the read_html() function. This will return a list of all HTML tables on the page, with each table as a DataFrame.\n\n\n\n\n\n\nNote\n\n\n\nIn order to use the read_html method, you need to additional install the optional Pandas dependency lxml. To do that using pip, run:\npip install lxml \n\n\nFor example:\n\ncontents = pd.read_html(\"https://su-ist356-m003-fall-2025.github.io/course-home\")\nfor df in contents:\n    print(df)\n\n    Week         Dates                   Topic\n0      1    8/25, 8/29    Intro; CLI and Conda\n1      2           9/3       Python review - 1\n2      3     9/8, 9/10       Python review - 2\n3      4    9/15, 9/17                      UI\n4      5    9/22, 9/24      Data wrangling - 1\n5      6    9/29, 10/1      Data wrangling - 2\n6      7    10/6, 10/8      Data wrangling - 3\n7      8         10/15                  Exam 1\n8      9  10/20, 10/22            Web APIs - 1\n9     10  10/27, 10/29            Web APIs - 2\n10    11    11/3, 11/5        Web scraping - 1\n11    12  11/10, 11/12        Web scraping - 2\n12    13  11/17, 11/19  Data visualization - 1\n13    14    12/1, 12/3  Data visualization - 2\n14    15          12/8                  Exam 2\n15    16         12/15             Project due\n          Date                                         Unnamed: 1\n0    Mon. 8/25                                 First day of class\n1     Mon. 9/1                               Labor day - No class\n2    Mon. 9/15  Academic/Financial drop deadline; Religious ob...\n3   Mon. 10/13                              Fall break - No class\n4   Fri. 11/21                                Withdrawal deadline\n5  11/23-11/30                      Thanksgiving Break - No class\n6    Mon. 12/8                                  Last day of class\n\n\nAn example of turning this into a Streamlit app:\n\nimport streamlit as st\nimport pandas as pd\n\nst.title(\"HTML Example - multiple tables\")\n\nurl = \"https://su-ist356-m003-fall-2025.github.io/course-home\"\ncontents = pd.read_html(url)\n\n# There are 2 tables on this page, but we don't know this\ntables_count = len(contents)\n\nst.write(f\"Found {tables_count} tables on the page\")\n\n# make tabs for each HTML Table\ntab_names = [ f\"HTML Table {i}\" for i in range(tables_count)]\ntabs = st.tabs(tab_names)\n\n# for each tab, show its table\nfor i in range(len(tabs)):\n    df = contents[i]\n    tabs[i].dataframe(df)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#writing-dataframes",
    "href": "04_data_wrangling/pandas-2.html#writing-dataframes",
    "title": "3. Data I/O with Pandas",
    "section": "Writing Dataframes",
    "text": "Writing Dataframes\n\nOnce the data is in a pd.DataFrame is can be written out with one of the to() methods such as to_csv(), to_json(), to_parquet() etc.\nThis makes pandas a superior data conversion tool.\nIf you include a file, the to() method writes to the file, otherwise the binary contents are returned.\nhttps://pandas.pydata.org/pandas-docs/stable/reference/io.html\n\nAn example of converting the above Excel spreadsheet to CSV:\n\ncontents = pd.read_excel(\"https://github.com/mafudge/datasets/raw/refs/heads/master/excel-examples/books_of_interest.xlsx\", sheet_name=None)\nfor sheetname, df in contents.items():\n    # convert spaces to underscores for filenames\n    sheetname = sheetname.replace(' ', '_')\n    filename = f'books_of_interest-{sheetname}.csv'\n    df.to_csv(filename, header=True, index=False)\n\n\n\n\n\n\n\nCautionCode Challenge 3.3\n\n\n\nWrite a Streamlit app that will accept an Excel file via file uploader and then write out a record-oriented JSON file from the first tab in the excel file.\nThe program should display the contents of the dataframe and provide a download button for the converted the csv file. Hints: - To provide the ability to upload a file, see: st.file_uploader - To provide the ability to download a file, see: st.download_button\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests\nimport pandas as pd\nimport streamlit as st\n\nst.title(\"Excel to JSON\")\n\nuploaded_file = st.file_uploader(\"Upload an EXCEL file\", type=[\"xlsx\"])\n\nif uploaded_file:\n    df = pd.read_excel(uploaded_file.getvalue())\n    st.dataframe(df)\n    json_file = df.to_json(orient=\"records\", index=False)\n    json_filename = uploaded_file.name.replace('.xlsx', '.json')\n    download = st.download_button(f\"Download {json_filename}\", data=json_file, file_name=json_filename)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-2.html#challenge-3-2-3",
    "href": "04_data_wrangling/pandas-2.html#challenge-3-2-3",
    "title": "3. Data I/O with Pandas",
    "section": "Challenge 3-2-3",
    "text": "Challenge 3-2-3\n\nExcel to JSON",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "3. Data I/O with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/numpy_basics.html",
    "href": "04_data_wrangling/numpy_basics.html",
    "title": "1. Introduction to NumPy",
    "section": "",
    "text": "Data manipulation in Python is nearly synonymous with NumPy array manipulation: even tools like Pandas are built around the NumPy array. Numpy arrays can be thought of as mathematical vectors and behave correspondingly; contrary to Python lists, which are a container that stores arbitrary objects.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "1. Introduction to NumPy"
    ]
  },
  {
    "objectID": "04_data_wrangling/numpy_basics.html#python-list-recap",
    "href": "04_data_wrangling/numpy_basics.html#python-list-recap",
    "title": "1. Introduction to NumPy",
    "section": "Python list recap",
    "text": "Python list recap\nWe saw previously that arrays allow us to store a series of values in a single variable. For example:\n\nlistnumbers = [1, 2, 3]\nprint(listnumbers)\n\n[1, 2, 3]\n\n\nNotably, in Python, a list can contain objects that are not of the same data type. For example:\n\nlist1 = [1, \"string\", {'a':1}, [[1, 3], set()]]  # arbitrary objects\nprint(list1)\n\n[1, 'string', {'a': 1}, [[1, 3], set()]]\n\n\nAdding two lists concatenates them, there is no mathematical operation: adding two containers that contain arbitrary objects means “combining” them.\n\nlistadd = list1 + listnumbers\nprint(listadd)\n\n[1, 'string', {'a': 1}, [[1, 3], set()], 1, 2, 3]\n\n\nBeing able to store multiple data types in a single list can be convenient. However, Python lists can be cumbersome to work with when doing mathematical operations, and for more complex and multi-dimensional data. They are also relatively slow to process. For example, say we wanted to add two large lists of values together. We would need to do the following:\n\n# create large list a\na = range(1000000)\nb = range(1000000)\nadded_list = [ai+bi for ai, bi in zip(a, b)]\n\nLet’s time how long it took to do that last line. For that, we can use the %timeit magic command in a jupyter notebook:\n\n%timeit added_list = [ai+bi for ai, bi in zip(a, b)]\n\n94.3 ms ± 819 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nWhile this may seem fast in human time, it’s quite slow computationally wise. If you had multiple operations like this it would quickly add up. As we will see below, it’s possible to do array operations like this much more quickly using NumPy.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "1. Introduction to NumPy"
    ]
  },
  {
    "objectID": "04_data_wrangling/numpy_basics.html#numpy-arrays",
    "href": "04_data_wrangling/numpy_basics.html#numpy-arrays",
    "title": "1. Introduction to NumPy",
    "section": "NumPy arrays",
    "text": "NumPy arrays\nFor much faster, easier manipulation of numerical arrays, we use NumPy. What is NumPy? A good summary is provided by Claude AI:\n\nNumPy is a fundamental Python library for scientific computing that provides support for large, multi-dimensional arrays and matrices. It offers a comprehensive collection of mathematical functions to operate on these arrays efficiently, with operations implemented in C for high performance. NumPy serves as the foundation for most other scientific Python libraries like pandas, scikit-learn, and matplotlib, making it essential for data science, machine learning, and numerical analysis workflows.\n\nLet’s see how to do some basic array operations with numpy. First, if you have not done so, you’ll need to install numpy into your conda environment. To do so, in a terminal, activate your conda environment, then either run:\npip install numpy\nor\nconda install -y -c conda-forge numpy\nWe can now import numpy:\n\nimport numpy as np\n\n\nNumPy array creation\nThere are multiple ways to create an array using numpy. Some examples:\n\n# from a Python list:\narrnumbers = np.array(listnumbers)\nprint(\"arrnumbers:\", arrnumbers)\n\n# manually creating it:\narrnumbers2 = np.array([5, 3, 42])\nprint(\"arrnumbers2:\", arrnumbers2)\n\n# from a range of values (compare to range above):\narrnumbers3 = np.arange(1000000)\nprint(\"arrnumbers3:\", arrnumbers3)\n\n# an array of values linearlly spaced between two endpoints:\narrlinspace = np.linspace(0, 10, 5)  # 5 values equally spaced between 0 and 10\nprint(\"arrlinspace:\", arrlinspace)\n\n# an array of zeros:\narrzeros = np.zeros(4)\nprint(\"arrzeros:\", arrzeros)\n\n# an array of ones:\narrones = np.ones(4)\nprint(\"arrones:\", arrones)\n\n# an empty array (values will be whatever is in memory at the time):\narrempty = np.empty(4)\nprint(\"arrempty:\", arrempty)\n\narrnumbers: [1 2 3]\narrnumbers2: [ 5  3 42]\narrnumbers3: [     0      1      2 ... 999997 999998 999999]\narrlinspace: [ 0.   2.5  5.   7.5 10. ]\narrzeros: [0. 0. 0. 0.]\narrones: [1. 1. 1. 1.]\narrempty: [1. 1. 1. 1.]\n\n\nNumPy arrays can have multiple dimensions:\n\narr2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr2d)\n\n# we can get the number of dimensions with .ndim:\nprint(\"arr2d.ndim:\", arr2d.ndim)\n\n# or the shape with .shape:\nprint(\"arr2d.shape:\", arr2d.shape) # returns the number of rows and columns\n\n[[1 2 3]\n [4 5 6]]\narr2d.ndim: 2\narr2d.shape: (2, 3)\n\n\nMany array constructor functions take a shape argument to create a mult-dimensional array:\n\nzeros2d = np.zeros((3, 4))  # 3 rows, 4 columns\nprint(\"zeros2d:\\n\", zeros2d)\n\nones3d = np.ones((2, 3, 4))  # 2 blocks, 3 rows, 4 columns\nprint(\"ones3d:\\n\", ones3d)\n\nzeros2d:\n [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\nones3d:\n [[[1. 1. 1. 1.]\n  [1. 1. 1. 1.]\n  [1. 1. 1. 1.]]\n\n [[1. 1. 1. 1.]\n  [1. 1. 1. 1.]\n  [1. 1. 1. 1.]]]\n\n\nOr, we can reshape a current array:\n\narrnumbers2d = arrnumbers3.reshape(1000, 1000)  # 1000 rows, 1000 columns\nprint(\"arrnumbers3.shape:\", arrnumbers3.shape)\nprint(\"arrnumbers3:\\n\", arrnumbers3)\n\nprint(\"arrnumbers2d.shape:\", arrnumbers2d.shape)\nprint(\"arrnumbers2d:\\n\", arrnumbers2d)\n\narrnumbers3.shape: (1000000,)\narrnumbers3:\n [     0      1      2 ... 999997 999998 999999]\narrnumbers2d.shape: (1000, 1000)\narrnumbers2d:\n [[     0      1      2 ...    997    998    999]\n [  1000   1001   1002 ...   1997   1998   1999]\n [  2000   2001   2002 ...   2997   2998   2999]\n ...\n [997000 997001 997002 ... 997997 997998 997999]\n [998000 998001 998002 ... 998997 998998 998999]\n [999000 999001 999002 ... 999997 999998 999999]]\n\n\n\n\nArray slicing\nSimilar to Python lists, we can access elements of a list using braces and indices. The syntax is:\narr[start:end:step]\nSome examples:\n\n# print a single element in arrnumbers:\nprint(arrnumbers[0]) # first element\nprint(arrnumbers[1]) # second element\n\n1\n2\n\n\n\n# print a range of elements in arrnumbers:\nprint('arrnumbers[0:2]:', arrnumbers[0:2])\n\n# equivalently:\nprint('arrnumbers[:2]:', arrnumbers[:2])  # start is 0 by default\n\n# or to print from an index to the end:\nprint('arrnumbers[1:]:', arrnumbers[1:])  # goes to the end by default\n\n# or to print all numbers:\nprint('arrnumbers[:]:', arrnumbers[:])  # start and end are default\n\n# print every second element:\nprint('arrnumbers[::2]:', arrnumbers[::2])\n\narrnumbers[0:2]: [1 2]\narrnumbers[:2]: [1 2]\narrnumbers[1:]: [2 3]\narrnumbers[:]: [1 2 3]\narrnumbers[::2]: [1 3]\n\n\nNegative indices can be used to slice starting from the end, and to reverse order. For example:\n\nprint('arrnumbers[-2:]', arrnumbers[-2:])  # print the last two elements\nprint('arrnumbers[::-1]', arrnumbers[::-1])  # print all elements in reverse order\n\narrnumbers[-2:] [2 3]\narrnumbers[::-1] [3 2 1]\n\n\nFor multi-dimensional arrays, the same rules apply, you just separate the indexing for each dimension by commas. For example:\n\nprint(\"arr2d:\\n\", arr2d)\nprint(\"arr2d[0, 0]:\", arr2d[0, 0])  # first row, first column\nprint(\"arr2d[:, 0]:\", arr2d[:, 0])  # all rows, first column\nprint(\"arr2d[0, :]:\", arr2d[0, :])  # first row, all columns\nprint(\"arr2d[0:2, 1:3]:\\n\", arr2d[0:2, 1:3])  # first two rows, columns 1 and 2\n\narr2d:\n [[1 2 3]\n [4 5 6]]\narr2d[0, 0]: 1\narr2d[:, 0]: [1 4]\narr2d[0, :]: [1 2 3]\narr2d[0:2, 1:3]:\n [[2 3]\n [5 6]]\n\n\n\n\nSubarrays as no-copy views\nOne important–and extremely useful–thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies. Consider our two-dimensional array from before:\n\nprint(arr2d)\n\n[[1 2 3]\n [4 5 6]]\n\n\nLet’s extract a \\(2 \\times 2\\) subarray from this:\n\narr2d_sub = arr2d[:2, :2]\nprint(arr2d_sub)\n\n[[1 2]\n [4 5]]\n\n\nNow if we modify this subarray, we’ll see that the original array is changed! Observe:\n\narr2d_sub[0, 0] = 99\nprint(arr2d_sub)\n\n[[99  2]\n [ 4  5]]\n\n\n\nprint(arr2d)\n\n[[99  2  3]\n [ 4  5  6]]\n\n\nThis default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.\n\n\nCreating copies of arrays\nDespite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method:\n\narr2d_sub_copy = arr2d[:2, :2].copy()\nprint(arr2d_sub_copy)\n\n[[99  2]\n [ 4  5]]\n\n\nIf we now modify this subarray, the original array is not touched:\n\narr2d_sub_copy[0, 0] = 42\nprint(arr2d_sub_copy)\n\n[[42  2]\n [ 4  5]]\n\n\n\nprint(arr2d)\n\n[[99  2  3]\n [ 4  5  6]]\n\n\n\n\nBoolean slicing\nYou can use boolean expressions to retrieve certain values in an array. For example:\n\n# print all the values in arrnumbers2 greater than 4:\nprint(\"arrnumbers:\\n\", arrnumbers2)\nprint(\"arrnumbers &gt; 5:\\n\", arrnumbers2[arrnumbers2 &gt; 4])  # boolean array\n\narrnumbers:\n [ 5  3 42]\narrnumbers &gt; 5:\n [ 5 42]\n\n\nWhat’s actually happening here is you’re first creating a boolean array. This is an array in which each element is either True or False. In this case, arrnumbers2 &gt; 4 is creating an array indicating which indices in arrnumbers2 are greater than 4. Passing the boolean array as an index then pulls out those values. We can see this if we break it into two steps:\n\nmask = arrnumbers2 &gt; 4\nprint(\"mask:\\n\", mask)\nprint(\"arrnumbers2[mask]:\\n\", arrnumbers2[mask])\n\nmask:\n [ True False  True]\narrnumbers2[mask]:\n [ 5 42]",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "1. Introduction to NumPy"
    ]
  },
  {
    "objectID": "04_data_wrangling/numpy_basics.html#data-types",
    "href": "04_data_wrangling/numpy_basics.html#data-types",
    "title": "1. Introduction to NumPy",
    "section": "Data types",
    "text": "Data types\nA key difference between NumPy arrays and Python arrays is that the data in a NumPy array must all be of the same type. You can get the data type of the values in an array using .dtype. For example:\n\nprint('arrnumbers:', arrnumbers)\nprint('arrnumbers.dtype:', arrnumbers.dtype)\n\narrnumbers: [1 2 3]\narrnumbers.dtype: int64\n\n\n\nprint('arrones:', arrones)\nprint('arrones.dtype:', arrones.dtype)\n\narrones: [1. 1. 1. 1.]\narrones.dtype: float64\n\n\n\nprint('mask:', mask)\nprint('mask.dtype:', mask.dtype)\n\nmask: [ True False  True]\nmask.dtype: bool\n\n\nIf you try to create an array with different data types, numpy will automatically cast them to all be the same. For example:\n\nmixed = np.array([1, 2.0, 3, 4.8, True, False])  # ints, floats, bools\nprint(\"mixed:\", mixed)\nprint(\"mixed.dtype:\", mixed.dtype)  # everything cast to float (note that True -&gt; 1, False -&gt; 0)\n\nmixed: [1.  2.  3.  4.8 1.  0. ]\nmixed.dtype: float64\n\n\nYou can cast an array to a different type using .astype. This will create a copy of the array with values cast to the type you specified. For example:\n\nmixedint = mixed.astype(int)  # cast to int\nprint(\"mixedint:\", mixedint)\nprint(\"mixedint.dtype:\", mixedint.dtype)  # everything cast to int\n\nmixedint: [1 2 3 4 1 0]\nmixedint.dtype: int64",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "1. Introduction to NumPy"
    ]
  },
  {
    "objectID": "04_data_wrangling/numpy_basics.html#array-operations",
    "href": "04_data_wrangling/numpy_basics.html#array-operations",
    "title": "1. Introduction to NumPy",
    "section": "Array operations",
    "text": "Array operations\nOne of the most useful aspects about NumPy arrays is they allow you to perform mathematical operations on the all the elements in the list using the same syntax you would for single variables. For example, we can add all the values in one array to another by doing:\n\na = np.arange(1000000)\nb = np.arange(1000000)\nc = a + b  # add element-wise\nprint(\"c:\", c)\n\nc: [      0       2       4 ... 1999994 1999996 1999998]\n\n\nCompare that to the way we had to add two Python lists together above. Note that if a and b were Python lists a+b concatenates them together (i.e., appends the values of b on to the end of a) where as if a and b are Numpy arrays, the values are added together element-wise.\nAside from being easier to write, NumPy array operations are also much faster than Python operations. Let’s time how long it took to create c:\n\n%timeit c = a + b\n\n381 μs ± 20.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nCompare to what we got when we did the same thing with Python lists above. It’s about 100 times faster!\n\nMore advanced math operations\nNumpy comes with a large number of math functions built-in, which we can run on NumPy arrays. For example:\n\n# take the sine of every element in a:\nprint(np.sin(a))\n\n# sum up all the values in a:\nprint(np.sum(a))\n\n# take the average of all the values in a:\nprint(np.mean(a))\n\n[ 0.          0.84147098  0.90929743 ...  0.21429647 -0.70613761\n -0.97735203]\n499999500000\n499999.5\n\n\nSome operations can also be executed as methods on the array. For example:\n\n# sum up all the values in a:\nprint(a.sum())  \n\n# take the average of all the values in a:\nprint(a.mean())\n\n499999500000\n499999.5\n\n\nNote that this doesn’t work with the sine function, however:\n\na.sin()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 a.sin()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'sin'\n\n\n\nIn VS Code, you can see all the operations you can call as methods of the array by typing the array name + .; e.g., a.. That will show a drop-down list that you can cycle through.\n\n\n\n\n\n\nCautionCode Challenge 1.1\n\n\n\nLet’s illustrate the speed and simplicity of NumPy vs native Python lists.\n\nCreate a Python array that has 100,000 values equally spaced between 0 and 2*pi (pi = 3.141592653589793).\nCalculate the average of the cosine of every value in the Python array. For the cos function, you will need to import the math module.\nRepeat steps 1 and 2, but using purely NumPy arrays and functions. You should be able to do step 2 in a single line of code. Note that NumPy has an in-built pi value (np.pi).\nTime how long it takes the computer to do Step 2. Compare how long that takes when you use NumPy. When doing the comparison, just time the math operation step, not the array creation. Which is faster? Hint: for timing the Python version, you’ll need to use %%timeit rather than %timeit, as the Python version will require multiple lines of code. Put all the lines in a single cell in your notebook, and put %%timeit at the top to time the entire cell, rather than just a single line.\nWhich is computationally faster? By what factor?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nTo create the Python array:\n\n\nxsize = 100000\nx = [xi*2*np.pi/xsize for xi in range(xsize)]\n\n\nEvaluation:\n\n\nimport math\ny = [math.cos(xi) for xi in x]\nsum(y)/len(y)\n\n-3.8771274778262895e-17\n\n\n\nSame, with numpy:\n\n\nimport numpy as np\n# create the array\nx = np.linspace(0, 2*np.pi, xsize)\n# take the average of the cosine of the values\nnp.cos(x).mean()\n\nnp.float64(9.99999999994543e-06)\n\n\n\nPython timing, in a Jupyter cell:\n\n\n%%timeit\ny = [math.cos(xi) for xi in x]\nsum(y)/len(y)\n\nNumpy:\n\n%timeit np.cos(x).mean()\n\nYou should get that the numpy version is faster, by a factor of 10-100.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "1. Introduction to NumPy"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html",
    "href": "03_ui/ui-2.html",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "",
    "text": "In this tutorial we will learn basic usage of https://docs.streamlit.io/get-started/fundamentals/main-concepts.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#what-is-streamlit",
    "href": "03_ui/ui-2.html#what-is-streamlit",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "What is Streamlit?",
    "text": "What is Streamlit?\nStreamlit markets itself as a User Interface library for building simple web applications from Python scripts.\nBuild:\n\nInteractive dashboard with tables, charts and graphs\nAccepting user input for a data pipeline\nChat applications\nand more!\n\nThe best part of streamlit is the application runs in a browser and you don’t need to learn front-end technologies like HTML, CSS and Javascript",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#how-does-it-work",
    "href": "03_ui/ui-2.html#how-does-it-work",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "How does it work?",
    "text": "How does it work?\nTo illustrate how streamlit works, here is a simple Python script that uses the Streamlit library:\n\nimport streamlit as st\n\nst.title(\"Saying Hello.\")\nname = st.text_input(\"And you are?\")\n\nif name:\n    st.write(f\"Hello, {name}!\")\n\nTo test this:\n\nCreate a file called hello-ui.py using VS Code (Click “File -&gt; New File”) and paste the above code into it.\nSave the file (File -&gt; Save, or CTRL/CMD + S).\nOpen a terminal (in VS Code, Terminal -&gt; New Terminal). If your ist356 environment is not active, activate it by running conda activate ist356.\nNow run:\npython -m streamlit run hello-ui.py\nWhen you run the script, Streamlit launches a webserver on your computer to run your app; your default web browser will automatically open with the simple website Streamlit created.\nIf you edit the hello-ui.py file, streamlit will automatically detect the changes when the changes are saved. If you re-do the interaction (in this case, adding a name and hitting Enter), the new code will be executed. Try it! Add a waving emoji by adding :wave: before the Hello in the st.write.\nThe app will continue to run in the browser. You can stop it by first hitting CTRL+C in the terminal, then closing the browser tab.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#interactions",
    "href": "03_ui/ui-2.html#interactions",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "Interactions",
    "text": "Interactions\nStreamlit supports both linear and event-driven interactions.\n\nLinear style\nWith linear interactions the code runs from top-down each time the input changes.\nThe linear pattern is the simpler pattern.\nThe most effective way to use of this pattern is:\n\nsetup widgets, saving their state in variables\nthen check interations through the variables with if\n\nAn example:\n\nimport streamlit as st\n\nst.title(\"Streamlit Interaction: linear\")\n\n# setup\nname = st.text_input(\"Who are you?\")\nhi_clicked = st.button('Say Hi!')\nclear_clicked = st.button('Clear')\n\n# interactions\nif hi_clicked:\n    if name:\n        st.success(f\"Hello, {name}\", icon=\"👍\")\n    else:\n        st.error(f\"I can't say hello, if you don't tell me your name!\", icon=\"💣\")\n\nif clear_clicked:\n    name = None \n\n\n\nEvent-driven style\nWith event-driven interactions, you write a function to handle the event. This is similar to how most other UI libraries work.\n\n\n\n\n\n\nWarning\n\n\n\nThe event-driven pattern is more complex, and might have unexpected behaviors due to streamlit’s processing order!\n\n\nThe most effective use of this pattern is to:\n\ncreate handler functions with def\nsetup interactions, using the function on the event\n\nExample:\n\nimport streamlit as st\n\n\ndef hi_click():\n    if name:\n        st.success(f\"Hello, {name}\", icon=\"👍\")\n    else:\n        st.error(f\"I can't say hello, if you don't tell me your name!\", icon=\"💣\")\n\n\ndef clear_click():\n    name = None \n\n# setup\nst.title(\"Streamlit Interaction: event-driven\")\nname = st.text_input(\"Who are you?\")\nst.button('Say Hi!', on_click=hi_click)\nst.button('Clear', on_click=clear_click)\n\n\n\n\n\n\n\nCautionCode Challenge 2.1\n\n\n\nWrite a streamlit app that takes as input a length and width of a rectangle, and outputs the permieter [2 x (L+W)] and area [(L x W)] of that rectangle. Add a “calculate” button and a “clear” button.\nHint: use st.number_input() for numbers.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\n\nst.title('Area and permieter')\nlength = st.number_input(\"Enter Length:\")\nwidth = st.number_input(\"Enter Width:\")\nbtn_clicked = st.button('Calculate!')\n\nif btn_clicked:\n    area = length * width\n    perm = 2 * (length + width)\n    st.write(f\"Area: {area}\")\n    st.write(f\"Perimeter: {perm}\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#session-state-helping-streamlit-remember-values",
    "href": "03_ui/ui-2.html#session-state-helping-streamlit-remember-values",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "Session State: Helping Streamlit Remember values",
    "text": "Session State: Helping Streamlit Remember values\nst.session_state is a global key/value store for data that need to persist between streamlit runs.\nAny data dependent on a previous interaction would be a use case for this.\nThe pattern using session state is:\n- initialize the session state\n- create the widgets\n- check the interactions that change the state\n- display the widgets that update the state\nThe session state is necessary to store persistent data. For example, the following will not work:\nWrong way:\n\nimport streamlit as st\n\n# Streamlit is always running, so only do this will not do what you think\n\ncount = 0\n\n# widget setup\nst.title('Counter Example: Wrong')\nst.write(\"variables that change based on previous runs will not work as expected \")\nst.write(\"this is because streamlit runs all this code with each interaction\")\nincr_clicked = st.button('increment counter', type='primary')\nreset_clicked = st.button('reset counter', type='secondary')\n\n# interactions\nif reset_clicked:\n    count = 0\nelif incr_clicked:\n    count = count + 1\n    \n# display session state, after interations\nst.write(f'Button clicked {count} times')\n\nWhy not? (Try it!)\nHowever, this will work:\nRight way:\n\nimport streamlit as st\n\n# Streamlit is always running, so only do this when count is not in session_state\n\n# initialize\nif 'count' not in st.session_state:\n    st.session_state.count = 0\n\n# widget setup\nst.title('Counter Example: Session State')\nst.write(\"variables that change based on previous runs need session state\")\nst.write(\"`st.session_state` preserves the values of the variable between runs\")\nincr_clicked = st.button('increment counter', type='primary')\nreset_clicked = st.button('reset counter', type='secondary')\n\n# interactions\nif reset_clicked:\n    st.session_state.count = 0\nelif incr_clicked:\n    st.session_state.count = st.session_state.count + 1\n    \n# display session state, after interations\nst.write(f'Button clicked {st.session_state.count} times')\n\n\n\n\n\n\n\nCautionCode Challenge 2.2\n\n\n\nOrder total and history:\n\nWrite a streamlit app to input an amount.\nCreate an “add to total” button to accumulate the amount in the total.\nCreate a “clear” button to reset the session vars.\nDisplay the total and the history of each item entered.\n\nHint: you’ll need to manage a list for history!\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\n\n# initialize\nif 'total' not in st.session_state:\n    st.session_state.total = 0.0\nif 'history' not in st.session_state:\n    st.session_state.history = []\n\nst.title('Order Total and History')\namount = st.number_input(\"Amount:\")\nbtn_add = st.button('Add to Total')\nbtn_clear = st.button('Clear')\n\nif btn_add:\n    st.session_state.history.append(amount)\n    st.session_state.total = sum(st.session_state.history)\n    st.write(f\"TOTAL: {st.session_state.total}\")\n    st.write(\"HISTORY:\")\n    for h in st.session_state.history:\n        st.write(h)\n\nif btn_clear:\n    st.session_state.history = []\n    st.session_state.total = 0.0",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#exploring-streamlit-input-widgets",
    "href": "03_ui/ui-2.html#exploring-streamlit-input-widgets",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "Exploring Streamlit input widgets",
    "text": "Exploring Streamlit input widgets\nStreamlit offers a number of different widgets for user input, beyond the text input highlighted above. The following code highlights them:\n\nimport streamlit as st\nfrom  datetime import datetime \nst.title('Streamlit Input Widgets!')\n\nst.markdown(\"## Text Inputs\")\ntxt = st.text_input('Enter your name:', value='John Doe')\nst.text(f\"OUTPUT: {txt}, type: {type(txt)}\")\npw = st.text_input('Enter your password:', type=\"password\")\nst.text(f\"OUTPUT: {pw}, type: {type(pw)}\")\ntxta = st.text_area('Leave a comment:', value='Type here...')\nst.text(f\"OUTPUT: {txta}, type: {type(txta)}\")\nst.divider()\n\nst.markdown(\"## Binary Widgets\")\nchk = st.checkbox('I agree to the terms and conditions', value=False)\nst.text(f\"OUTPUT: {chk}, type: {type(chk)}\")\ntog = st.toggle('Enable notifications', value=False)\nst.text(f\"OUTPUT: {tog}, type: {type(tog)}\")\nst.divider()\n\nst.markdown(\"## Date / Time Widgets\")\ndt =st.date_input('Select a date:')\nst.text(f\"OUTPUT: {dt}, type: {type(dt)}\")\ntm = st.time_input('Select a time:')\nst.text(f\"OUTPUT: {tm}, type: {type(tm)}\")\nst.divider()\n\nst.markdown(\"## Number Widgtets\")\nnumi = st.number_input('Enter Hourly Wage:', value=7.25, max_value=20.0, min_value=5.25, step=0.25)\nst.text(f\"OUTPUT: {numi}, type: {type(numi)}\")\nnums = st.slider('Pick a number between 1 and 20:', min_value=1, max_value=20, value=10, step=1)\nst.text(f\"OUTPUT: {nums}, type: {type(nums)}\")\nst.divider()\n\nst.markdown(\"## Selection Widgets\")\nselbox = st.selectbox('Choose one shipping method:', ['Jiffy Express', ' You Pee Es', 'FedUp Express'])\nst.text(f\"OUTPUT: {selbox}, type: {type(selbox)}\")\nmulselbox = st.multiselect('Select all your favorite colors:', ['Red', 'Green', 'Blue', 'Yellow', 'White'])\nst.text(f\"OUTPUT: {mulselbox}, type: {type(mulselbox)}\")\nselslider = st.select_slider('Rate us:', options=['1=Poor','2=ok','3=good','4=great','5=excellent'], value = '3=good')\nst.text(f\"OUTPUT: {selslider}, type: {type(selslider)}\")\nradio = st.radio('Rate us:', ['1=Poor','2=ok','3=good','4=great','5=excellent'], index=2, horizontal=True)\nst.text(f\"OUTPUT: {radio}, type: {type(radio)}\")\nst.divider()\n\n\nst.markdown(\"## 'Other' Widgets\")\nfeed = st.feedback('faces')\nst.text(f\"OUTPUT: {feed}, type: {type(feed)}\")\ncolor = st.color_picker('Pick a color:', value='#00f900')\nst.text(f\"OUTPUT: {color}, type: {type(color)}\")\nfile = st.file_uploader('Upload a file:')\nst.text(f\"OUTPUT: {file}, type: {type(file)}\")\npic = st.camera_input('Take a selfie:')\nst.text(f\"OUTPUT: {pic}, type: {type(pic)}\")\nst.divider()",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#exploring-steamlit-output-widgets",
    "href": "03_ui/ui-2.html#exploring-steamlit-output-widgets",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "Exploring Steamlit output widgets",
    "text": "Exploring Steamlit output widgets\nLikewise, Streamlit offers a number of different widgets for displaying different types of output. Here’s a sampling:\n\nimport streamlit as st\n\nst.title('Streamlit Output Widgets!')\n\nst.markdown(\"## Text Output\")\nst.text(\"Plain text.\\nObeys newlines.\")\n\nst.markdown(\"## Markdown Output\")\nst.markdown('''\n### Heading 3\n- this\n- is a\n- list\n            \nLearn markdown here: [https://www.markdownguide.org/getting-started/](https://www.markdownguide.org/getting-started/)\n''')\n\nst.markdown(\"## Code Output\")\nst.code('''\nname = input(\"Enter your name:\")\nprint(f\"Hello, {name}\")\n''', language=\"python\", line_numbers=True)\n\nst.markdown(\"## Image Output\")\nst.image(\"https://ist256.com/images/logo.png\",caption=\"IST256 logo\")\n\nst.markdown(\"## Metric / Card Ouput\")\nst.metric(label=\"Temperature\", value=\"70 °F\", delta=\"1.2 °F\")\nst.metric(label=\"Mike Fudge\", value=\"B+\", delta=\"-5 pts\")\n\nst.markdown(\"## Video Output\")\nst.video(\"https://youtu.be/soVItkifdms?si=eNNbRXnAg4efcJGi\")\n\nst.markdown(\"## Audio Output\")\nst.audio(\"https://file-examples.com/storage/fe6993554766e3161a375a5/2017/11/file_example_MP3_700KB.mp3\")\n\nst.markdown(\"## Toast Output\")\nif st.button(\"Click to show toast\"):\n    st.toast(\"Congrats! You clicked it!\", icon=\":material/thumb_up:\")\n\nst.markdown(\"## Column Layouts\")\ncol1, col2, col3 = st.columns(3)\ncol1.markdown(\"Hello\")\ncol2.text(\"There\")\ncol2.text(\"Mike\")\ncol3.warning(\"Warning!\")\ncol3.error(\"Error!\")\ncol3.success(\"Success!\")\n\nst.markdown(\"## Tab Layouts\")\ncol1, col2, col3 = st.tabs([\"Tab A\",\"Tab B\",\"Tab C\"])\ncol1.markdown(\"Hello\")\ncol2.text(\"There\")\ncol2.text(\"Mike\")\ncol3.warning(\"Warning!\")\ncol3.error(\"Error!\")\ncol3.success(\"Success!\")\n\nst.markdown(\"## Expander Output\")\nwith st.expander(\"See a map\"):\n    st.write('Here is a map for you!')\n    st.map(latitude=76,longitude=-43, zoom=13)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#file-uploads",
    "href": "03_ui/ui-2.html#file-uploads",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "File Uploads",
    "text": "File Uploads\nWe can use st.file_uploader() to allow users to upload files. The uploaded files return a Python “file-like” that can be used in a variety of applications with little additional processing.\nAn example:\n\nimport streamlit as st\nfrom io import StringIO # required to convert binary to text\n\n\nst.title(\"File Upload Example\")\nst.markdown('''\nThis example demonstrates how to process and uploaded file. \n            \n- The first example can process and file-like (image, video, data for a dataframe, etc)\n- The second example shows how to process text explicitly.\n            \n''')\nbin_file_data = st.file_uploader(\"Upload an image/photo file\", type=[\"png\", \"jpeg\", \"jpg\", \"gif\"])\ntext_file_data = st.file_uploader(\"Upload a text file\", type=[\"txt\", \"csv\", \"md\"])\n\nif bin_file_data:\n    st.markdown(f\"### {bin_file_data.name}\")\n    st.image(bin_file_data)\n\nif text_file_data:\n    st.markdown(f\"### {text_file_data.name}\")\n    binary_contents = text_file_data.getvalue()\n    # Convert binary to text\n    text_contents = StringIO(binary_contents.decode(\"utf-8\")).read() \n    st.text(text_contents)\n\nprint(text_file_data)\n\n\n\n\n\n\n\nCautionCode Challenge 2.3\n\n\n\nOrder file processing:\n\nWrite a Streamlit app that takes as input a text file with one line per order. Samples are provided in the data folder, but each line should have the amount of the order.\nOutput the number of orders and the total amount of all orders.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\n\nfrom io import StringIO # required to convert binary to text\n\nst.title(\"Order File Processing\")\ntext_file_data = st.file_uploader(\"Upload the order file\", type=[\"txt\"])\n\nif text_file_data:\n    binary_contents = text_file_data.getvalue()\n    # Convert binary to text\n    text_contents = StringIO(binary_contents.decode(\"utf-8\")).read() \n    total = 0\n    count = 0\n    for line in text_contents.split(\"\\n\"):\n        try:\n            order = float(line)\n            total = total + order\n            count = count + 1\n        except ValueError:\n            continue \n    st.info(f\"Number of orders: {count}\", icon=\"➕\")\n    st.info(f\"Total amount: ${total:.2f}\", icon=\"💵\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/ui-2.html#image-processing-with-the-camera",
    "href": "03_ui/ui-2.html#image-processing-with-the-camera",
    "title": "2. Creating simple web apps with Streamlit",
    "section": "Image Processing with the camera",
    "text": "Image Processing with the camera\nWe can use st.camera_input() to get images from our webcams.\nFrom there its easy to load into popular image processing libraries.\nAn example:\n\nimport streamlit as st\nfrom PIL import Image\n\n\nst.title(\"Camera Example\")\nst.markdown('''\n    Let's take a picture with the camera and conver the image to greyscale with PIL\n\n    Learn More about PIL: https://pillow.readthedocs.io/en/stable/index.html\n''')\npic_data = st.camera_input(\"Take a pic!\")\n\nif pic_data:\n    img = Image.open(pic_data)\n    grey_img = img.convert(\"L\")\n    st.image(grey_img)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "2. Creating simple web apps with Streamlit"
    ]
  },
  {
    "objectID": "03_ui/index.html",
    "href": "03_ui/index.html",
    "title": "3. UI",
    "section": "",
    "text": "In this unit we will learn about some useful Python modules for creating dynamic user interface (UI) to your code.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI"
    ]
  },
  {
    "objectID": "03_ui/index.html#tutorials",
    "href": "03_ui/index.html#tutorials",
    "title": "3. UI",
    "section": "Tutorials",
    "text": "Tutorials\n\nInteraction in Jupyter using ipywidgets\nCreating simple web apps with Streamlit",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI"
    ]
  },
  {
    "objectID": "03_ui/index.html#lectures",
    "href": "03_ui/index.html#lectures",
    "title": "3. UI",
    "section": "Lectures",
    "text": "Lectures\n\nWednesday, 9/17/2025\nWalkthrough of how to run pytest in VS Code; Jupyter Widgets and Streamlit",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI"
    ]
  },
  {
    "objectID": "02_python/python-3.html",
    "href": "02_python/python-3.html",
    "title": "3. Functions, documentation, strings, files",
    "section": "",
    "text": "A Function is a named sequence of statements which accomplish a task. They promote modularity, making our code less complex, easier to understand and encourage code-reuse.\nWhen you “run” a defined function it’s known as a function call. Functions are designed to be written once, but called many times.\nWe’ve seen functions before:\n\n\n# We call functions all the time\n# input(). random.randint(), and int() are all functions!\nimport random\nx = input(\"Enter Name: \")  \ny = random.randint(1, 10)  #random is the module, randint() is the function\nz = int(\"9\")\nprint(x, y, z)\n\n\n\n\nFunctions are like their own little programs. They take input, which we call the function arguments (or parameters) and give us back output that we refer to as return values.\nINPUT      ==&gt; PROCESS     ==&gt; OUTPUT\nFunction   ==&gt; Function    ==&gt; Function \nArguments      Definition       Return\nWe use the def keyword to define a function.\n\n# Example Function\ndef area_of_triangle(base, height): # &lt;== INPUTs\n    area = 0.5 * base * height\n    return area # &lt;== OUTPUT\n\n# Function call: using your function\na = area_of_triangle(10, 5)\nprint(a)\n\n25.0\n\n\nWhen you call a function you can name your arguments. This allows you to override the order of the arguments.\n\n# These are the same order as defined\narea1 = area_of_triangle(base = 10, height = 5)\n# Different order than defined\narea2 = area_of_triangle(height = 5, base = 10)\nprint(area1, area2)\n\n25.0 25.0\n\n\n\n\n\ndef division_and_modulo(dividend, divisor):\n    quotient = dividend // divisor # int division\n    remainder = dividend % divisor # modulo\n    return quotient, remainder\n\nq, r = division_and_modulo(10, 3)\nprint(f\"10 divided by 3 is {q}, with a remainder of {r}\")\n\n10 divided by 3 is 3, with a remainder of 1\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.1\n\n\n\nWrite a function called average which takes a list of numbers as input then outputs the average of the numbers (sum / count)\nCall your function with an arbitrary list of numbers you create.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\ndef average(list_of_numbers):\n    total = 0\n    count = 0\n    for n in list_of_numbers:\n        total += n\n        count += 1\n    return total/count\n\nnums = [10, 15, 10, 5]\navg = average(nums)\nprint(f\"Average of {nums} is {avg}\")\n\nAverage of [10, 15, 10, 5] is 10.0\n\n\n\n\n\n\n\n\n\n\n\nTypes can be added to the def statement to help the caller understand what type of data the function expects. These are known as type hints\nIn this example the expected arguments are float and the return is float\n\ndef area_of_triangle(base: float, height: float) -&gt; float: \n    area = 0.5 * base *height\n    return area\n\nYou can see type hints in action by calling the function\nRun the cell above to create the function.\nIn the code below, start a left paren ( to see the type hints\n\narea_of_triangle\n\n&lt;function __main__.area_of_triangle(base: float, height: float) -&gt; float&gt;\n\n\n\n\n\nA Docstring is a multi-line comment which explains what the function does to the function caller.\nThe same function with type hints and docstring:\n\ndef area_of_triangle(base: float, height: float) -&gt; float: \n    '''\n    Calculates the area of a triangle given base and height\n    returns the area defined as 1/2 the base times height\n    '''\n    area = 0.5 * base *height\n    return area\n\nYou can see doc strings in action by calling the function\nRun the cell above to create the function. In the code below, start a left paren ( to see the doc string\n\narea_of_triangle\n\n&lt;function __main__.area_of_triangle(base: float, height: float) -&gt; float&gt;\n\n\nYou can also use ? or help() to see the docstring and type hints:\n\nhelp(area_of_triangle)\n\nHelp on function area_of_triangle in module __main__:\n\narea_of_triangle(base: float, height: float) -&gt; float\n    Calculates the area of a triangle given base and height\n    returns the area defined as 1/2 the base times height",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#what-are-functions",
    "href": "02_python/python-3.html#what-are-functions",
    "title": "3. Functions, documentation, strings, files",
    "section": "",
    "text": "A Function is a named sequence of statements which accomplish a task. They promote modularity, making our code less complex, easier to understand and encourage code-reuse.\nWhen you “run” a defined function it’s known as a function call. Functions are designed to be written once, but called many times.\nWe’ve seen functions before:\n\n\n# We call functions all the time\n# input(). random.randint(), and int() are all functions!\nimport random\nx = input(\"Enter Name: \")  \ny = random.randint(1, 10)  #random is the module, randint() is the function\nz = int(\"9\")\nprint(x, y, z)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#function-definitions",
    "href": "02_python/python-3.html#function-definitions",
    "title": "3. Functions, documentation, strings, files",
    "section": "",
    "text": "Functions are like their own little programs. They take input, which we call the function arguments (or parameters) and give us back output that we refer to as return values.\nINPUT      ==&gt; PROCESS     ==&gt; OUTPUT\nFunction   ==&gt; Function    ==&gt; Function \nArguments      Definition       Return\nWe use the def keyword to define a function.\n\n# Example Function\ndef area_of_triangle(base, height): # &lt;== INPUTs\n    area = 0.5 * base * height\n    return area # &lt;== OUTPUT\n\n# Function call: using your function\na = area_of_triangle(10, 5)\nprint(a)\n\n25.0\n\n\nWhen you call a function you can name your arguments. This allows you to override the order of the arguments.\n\n# These are the same order as defined\narea1 = area_of_triangle(base = 10, height = 5)\n# Different order than defined\narea2 = area_of_triangle(height = 5, base = 10)\nprint(area1, area2)\n\n25.0 25.0\n\n\n\n\n\ndef division_and_modulo(dividend, divisor):\n    quotient = dividend // divisor # int division\n    remainder = dividend % divisor # modulo\n    return quotient, remainder\n\nq, r = division_and_modulo(10, 3)\nprint(f\"10 divided by 3 is {q}, with a remainder of {r}\")\n\n10 divided by 3 is 3, with a remainder of 1\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.1\n\n\n\nWrite a function called average which takes a list of numbers as input then outputs the average of the numbers (sum / count)\nCall your function with an arbitrary list of numbers you create.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\ndef average(list_of_numbers):\n    total = 0\n    count = 0\n    for n in list_of_numbers:\n        total += n\n        count += 1\n    return total/count\n\nnums = [10, 15, 10, 5]\navg = average(nums)\nprint(f\"Average of {nums} is {avg}\")\n\nAverage of [10, 15, 10, 5] is 10.0",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#type-hints",
    "href": "02_python/python-3.html#type-hints",
    "title": "3. Functions, documentation, strings, files",
    "section": "",
    "text": "Types can be added to the def statement to help the caller understand what type of data the function expects. These are known as type hints\nIn this example the expected arguments are float and the return is float\n\ndef area_of_triangle(base: float, height: float) -&gt; float: \n    area = 0.5 * base *height\n    return area\n\nYou can see type hints in action by calling the function\nRun the cell above to create the function.\nIn the code below, start a left paren ( to see the type hints\n\narea_of_triangle\n\n&lt;function __main__.area_of_triangle(base: float, height: float) -&gt; float&gt;",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#docstrings",
    "href": "02_python/python-3.html#docstrings",
    "title": "3. Functions, documentation, strings, files",
    "section": "",
    "text": "A Docstring is a multi-line comment which explains what the function does to the function caller.\nThe same function with type hints and docstring:\n\ndef area_of_triangle(base: float, height: float) -&gt; float: \n    '''\n    Calculates the area of a triangle given base and height\n    returns the area defined as 1/2 the base times height\n    '''\n    area = 0.5 * base *height\n    return area\n\nYou can see doc strings in action by calling the function\nRun the cell above to create the function. In the code below, start a left paren ( to see the doc string\n\narea_of_triangle\n\n&lt;function __main__.area_of_triangle(base: float, height: float) -&gt; float&gt;\n\n\nYou can also use ? or help() to see the docstring and type hints:\n\nhelp(area_of_triangle)\n\nHelp on function area_of_triangle in module __main__:\n\narea_of_triangle(base: float, height: float) -&gt; float\n    Calculates the area of a triangle given base and height\n    returns the area defined as 1/2 the base times height",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#strings-are-sequence-types",
    "href": "02_python/python-3.html#strings-are-sequence-types",
    "title": "3. Functions, documentation, strings, files",
    "section": "Strings are sequence types",
    "text": "Strings are sequence types\nYou can use slice notation like with lists.\nThese are zero based.\nvar[start:stop]\nTakes stop - start characters from var starting at position start\n\nx = \"fudge\"\nprint(x[0:2]) # fu\nprint(x[2:5]) # dge\nprint(x[:4]) # fudg\nprint(x[:]) # fudge\nprint(x[:-1]) # fudg\n\nfu\ndge\nfudg\nfudge\nfudg",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#string-methods",
    "href": "02_python/python-3.html#string-methods",
    "title": "3. Functions, documentation, strings, files",
    "section": "String Methods",
    "text": "String Methods\nhttps://docs.python.org/3/library/stdtypes.html#text-sequence-type-str\nMethod functions attach to the string x.strip()\nCommon methods\nstrip()\nupper()\nlower()\nfind()\ncount()\nsplit()\njoin()\nreplace()\n\n# Samples\ns = \"this is a test\"\nprint(s.count(\"is\")) # 2\nprint(s.count(\"t\"))  # 3\nprint(s.upper()[:4]) # TEST\nprint(s.find(\" a \")) # 7\nprint(s.find(\"this\")) # 9\nprint(\"   x   \".strip()) # x\nprint(s.replace(\"this\", \"that\")) # that is a test\n\n2\n3\nTHIS\n7\n0\nx\nthat is a test\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.2\n\n\n\nWrite a function called cleanup which takes a string as input and returns a “cleaned string” meaning:\n\nremove any ? , . or !\nstrip off the whitespace from the ends\nreturn text in lower case\n\nWrite code to call your function and test it\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\ndef cleanup(text: str) -&gt; str:\n    for ch in \"!?,.\":\n        if ch in text:\n            text = text.replace(ch, \"\")\n    return text.lower().strip()\n\ntext = \"  THis! Is. , a tEST? \"\ncleaned = cleanup(text)\nprint(cleaned)\n\nthis is  a test",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#string-tokenization-and-parsing",
    "href": "02_python/python-3.html#string-tokenization-and-parsing",
    "title": "3. Functions, documentation, strings, files",
    "section": "String Tokenization and Parsing",
    "text": "String Tokenization and Parsing\n\nTokenization is the process of breaking up a string into words, phrases, or symbols.\n\nTokenize a sentence into words.\n\"mike is here\" becomes the iterable ['mike', 'is', 'here']\n\nParsing is the process of extracting meaning from a string.\n\nParse text to a numerical value or date.\nint('45') becomes 45\n\n\n\n# tokenize with split()\n# parse with int(), or float()\n\ntext = \"30 40 90 10\"\ntokens = text.split()\nnumbers = [int(t) for t in tokens]\ntotal = sum(numbers)\nprint(total)\n\n170\n\n\n\n# What you split on is called the delimiter:\ntext = \"name, age, phone, gpa\"\nitems = [ x.upper().strip() for x in text.split(',') ]\nprint(items)\n\n['NAME', 'AGE', 'PHONE', 'GPA']",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#files-persistence",
    "href": "02_python/python-3.html#files-persistence",
    "title": "3. Functions, documentation, strings, files",
    "section": "Files == Persistence",
    "text": "Files == Persistence\n\nFiles add a Persistence Layer to our computing environment where we can store our data after the program completes.\nThink: Saving a game’s progress or saving your work!\nWhen our program Stores data, we open the file for writing.\nWhen our program Reads data, we open the file for reading.\nTo read or write a file we must first open it, which gives us a special variable called a file handle.\nWe then use the file handle to read or write from the file.\nThe read() function reads from the write() function writes to the file through the file handle.\n\n\nReading from a file\n\nfilename = \"data/sample.txt\"\nprint(\"=== All at once ===\")\nwith open(filename, 'r') as handle:\n    contents = handle.read()\n    print(contents)\n\nprint(\"=== A Line at a time ===\")\ni = 1\nwith open(filename, 'r') as handle:\n    for line in handle.readlines():\n        print(i, line.strip())\n        i += 1\n\n=== All at once ===\nThis\nIs\nA\nSample\n=== A Line at a time ===\n1 This\n2 Is\n3 A\n4 Sample\n\n\n\n\nWriting to a file\n\nfilename = \"data/demo.txt\"\nprint(\"=== Create file and write to it ===\")\nwith open(filename, \"w\") as f:\n    f.write(\"message!\\n\")\n\nprint(\"=== Append (add to end) of existing file ===\")\nwith open(filename, \"a\") as f:\n    f.write(\"message # 2!\\n\")\n\n=== Create file and write to it ===\n=== Append (add to end) of existing file ===\n\n\n\n%%bash\n# switch to bash interpreter\ncat data/demo.txt\n\nmessage!\nmessage # 2!\n\n\n\n\nHandling missing files\n\n# Try / Except to handle FileNotFound\ntry:\n    file = 'data/data.txt'\n    with open(file,'r') as f:\n        print( f.read() )\nexcept FileNotFoundError:\n    print(f\"{file} was not found!\")\n\ndata/data.txt was not found!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-3.html#json-and-python-dictionaries",
    "href": "02_python/python-3.html#json-and-python-dictionaries",
    "title": "3. Functions, documentation, strings, files",
    "section": "JSON and Python Dictionaries",
    "text": "JSON and Python Dictionaries\n\nJSON (JavaScript Object Notation) is a standard, human-readable data format. It’s a popular format for data on the web.\nJSON can be easily converted to lists of dictionaries using Python’s json module.\nTransferring a JSON string to Python is known as de-serializing.\nTransferring Python to a JSON string is known as serializing.\nThis is easy to do in Python but challenging to do in most other languages.\n\n\nSerialization\n\n# Serialize a python object as json\nimport json\ngrades = { 'CHE101' : [100, 80, 70], 'IST195' : [100, 80, 100] }\nwith open(\"data/grades.json\", \"w\") as f:\n    json.dump(grades, f, indent=4) # write grades to file as JSON\n\n\n%%bash\n# switch to bash interpreter\ncat data/grades.json\n\n{\n    \"CHE101\": [\n        100,\n        80,\n        70\n    ],\n    \"IST195\": [\n        100,\n        80,\n        100\n    ]\n}\n\n\n\n\nDeserialization\n\n# de-serialize some json\nfile = \"data/stocks.json\"\nwith open(file, \"r\") as f:\n    stocks = json.load(f)\n    \n# stocks is a python object\n# Deserialized from text!\nfor stock in stocks:\n    print(stock['symbol'])\n\nAAPL\nAMZN\nFB\nGOOG\nIBM\nMSFT\nNET\nNFLX\nTSLA\nTWTR\n\n\n\n\n\n\n\n\nCautionCode Challenge 3.3\n\n\n\nwrite a program to read in a string of students and gpas in one input statement like this:\nmike 3.4, noel 3.2, obby 3.5, peta 3.4\nand write out JSON like this:\n[\n    { \"name\" : \"mike\", \"gpa\" : 3.4 },\n    { \"name\" : \"noel\", \"gpa\" : 3.2 },\n    { \"name\" : \"obby\", \"gpa\" : 3.5 },\n    { \"name\" : \"peta\", \"gpa\" : 3.4 }\n]\nSuggested approach:\n\ninput text\nsplit on “,” from the text\nfor each student:\n\nsplit the student into name and gpa\nparse the gpa so its a float\nadd the name and gpa to the list as a dictionary\n\nwrite the list to students.json as JSON\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport json \ntext = input(\"Enter names and grades: \")\nstudents = []\nfor student in text.split(\",\"):\n    name, gpa = student.strip().split()\n    gpa = float(gpa)\n    students.append({ \"name\": name, \"gpa\": gpa })\nwith open (\"students.json\", \"w\") as f:\n    json.dump(students, f)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "3. Functions, documentation, strings, files"
    ]
  },
  {
    "objectID": "02_python/python-1.html",
    "href": "02_python/python-1.html",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "",
    "text": "Identify the problem inputs (requirements)\nIdentify the problem outputs (results)\nWrite an algorithm to transform inputs to outputs.\nIf you don’t know how to do a step… research it!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#input-process-output",
    "href": "02_python/python-1.html#input-process-output",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "",
    "text": "Identify the problem inputs (requirements)\nIdentify the problem outputs (results)\nWrite an algorithm to transform inputs to outputs.\nIf you don’t know how to do a step… research it!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#python-input-and-output",
    "href": "02_python/python-1.html#python-input-and-output",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Python input and output",
    "text": "Python input and output\nprint() does output\ninput() does input, returns input so you must assign it to a variable\n\nx = input(\"Enter something: \")\n\n\nprint(x)\n\n\n\n\n\n\n\nCautionCode Challenge 1.1\n\n\n\nWrite a program to input your first name and last name then output your last name, first name.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfirst = input(\"Enter your first name: \")\nlast = input(\"Enter your last name: \")\nprint(last, \",\", first)\n# could also do (see next section):\nprint(f\"{last}, {first}\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#f-strings",
    "href": "02_python/python-1.html#f-strings",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "F-Strings",
    "text": "F-Strings\n\nF-Strings are Python’s answer to string interpolation.\nThis replaces the variable name with its value within a string.\nCalled an F-string because the f tells Python to interpolate the string.\n\n\nname = 'George'\nprint(\"{name} was curious.\")\nprint(f\"{name} was curious.\")\n\n{name} was curious.\nGeorge was curious.\n\n\n\n\n\n\n\n\nNoteCheck Yourself 1\n\n\n\nWhich is an example of a properly used string literal?\nA. print(welcome)\nB. print(\"welcome\")\nC. print \"welcome\"\nD. print welcome\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nB\nNote: In Python 2.7 and earlier the correct answer would be C. The syntax for the print function was changed in Python 3. Python 2 is no longer supported.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#variables",
    "href": "02_python/python-1.html#variables",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Variables",
    "text": "Variables\n\nVariables are named areas of computer memory for storing data.\nThe name can be anything but should make symbolic sense to the programmer.\nWe write to the variable’s memory location with the assignment statement (=)\nWe read from the variable by calling its name.\nVariable names must begin with a letter or _ and must only contain letters, numbers or _.\n\n\nVariables are of a Specific Type\n\n\n\n\nType\n\n\nPurpose\n\n\nExamples\n\n\n\n\n\n\nint\n\n\nNumeric type for integers only\n\n\n45, -10\n\n\n\n\nfloat\n\n\nNumeric type floating point numbers\n\n\n45, -10\n\n\n\n\nbool\n\n\nTrue or False values\n\n\nTrue, False\n\n\n\n\nstr\n\n\nCharacters and text\n\n\n“A”, ‘Mike’\n\n\n\n\n\n\nType Detection and Conversion\n\n\n\n\nPython Function\n\n\nWhat It Does\n\n\nExample of Use\n\n\n\n\n\n\ntype(n)\n\n\nReturns the current type of n\n\n\ntype(13) == int\n\n\n\n\nint(n)\n\n\nConverts n to type int\n\n\nint(“45”) == 45\n\n\n\n\nfloat(n)\n\n\nConverts n to type float\n\n\nfloat(45) == 45.0\n\n\n\n\nstr(n)\n\n\nConverts n to type str\n\n\nstr(4.0) == ‘4.0’",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#programmatic-expressions",
    "href": "02_python/python-1.html#programmatic-expressions",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Programmatic Expressions",
    "text": "Programmatic Expressions\nProgrammatic Expressions contain operators and operands. They evaluate to a value, preserving type:\n\nprint(2 + 2)\nprint(2.0 + 2)\nprint(\"sh\" + 'ip')\nprint('hi' + 2) # error\n\n4\n4.0\nship\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 4\n      2 print(2.0 + 2)\n      3 print(\"sh\" + 'ip')\n----&gt; 4 print('hi' + 2) # error\n\nTypeError: can only concatenate str (not \"int\") to str",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#arithmetic-operators",
    "href": "02_python/python-1.html#arithmetic-operators",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\n\nOperator\n\n\nWhat it Does\n\n\nExample of Use\n\n\n\n\n\n\n+\n\n\nAddition or string concenation\n\n\n3 + 4 == 7\n\n\n\n\n-\n\n\nSubtraction\n\n\n4 - 3 == 1\n\n\n\n\n*\n\n\nMultiplication\n\n\n3 * 4 == 12\n\n\n\n\n/\n\n\nDivision\n\n\n4 / 3 == 1.33333\n\n\n\n\n//\n\n\nIntger division (quotent)\n\n\n13 // 3 == 4\n\n\n\n\n%\n\n\nModulo (remainder)\n\n\n13 % 3 == 1\n\n\n\n\n( )\n\n\nForce an order of operations\n\n\n2 * (3 + 4) == 14\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 1.2: Program to divide up the check among diners in a party\n\n\n\nWrite a program that takes as input the amount of a restaurant check, tip %, and number of diners.\nThe program should output the total amount with tip, and the amount each diner owes.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nbill = float(input(\"Enter the total amount of the bill $\"))\ntip = int(input(\"What % would you like to tip, eg. 20 == 20%? \"))\ntip_pct = tip/100\ndiners = int(input(\"How many diners? \"))\ntotal = bill + bill*tip_pct\nshare = total / diners\nprint(\"Total Bill, with Tip: \", total)\nprint(f\"Even share among {diners} diners is {share:.2f}\")\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCheck Yourself 2\n\n\n\nWhat is the value of str(314) ?\nA. 314\nB. \"314\"\nC. int\nD. '34.0'\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nB\n\n\n\n\n\n\n\n\n\n\n\nNoteCheck Yourself 3\n\n\n\nWhat is the value of type(314.0) ?\nA. 314\nB. float\nC. int\nD. '314.0'\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nB\n\n\n\n\n\n\n\n\n\n\n\nNoteCheck Yourself 4\n\n\n\nWhat is the output of the following python code?\n\na = 10\nb = 2\nc = 1 + (a/b)\nprint(c)\n\nA. 6\nB. 5.5\nC. 6.0\nD. 5\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nC",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#program-flow-control-with-if",
    "href": "02_python/python-1.html#program-flow-control-with-if",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Program Flow Control with IF",
    "text": "Program Flow Control with IF\n\nThe IF statement is used to branch your code based on a Boolean expression.\n\n\nif boolean-expression:\n    statements-when-true\nelse:\n    statemrnts-when-false",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#pythons-relational-operators",
    "href": "02_python/python-1.html#pythons-relational-operators",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Python’s Relational Operators",
    "text": "Python’s Relational Operators\n\n\n\n\nOperator\n\n\nWhat it does\n\n\nExamples\n\n\n\n\n\n\n &gt; \n\n\nGreater than\n\n\n4&gt;2 (True)\n\n\n\n\n &lt; \n\n\nLess than\n\n\n4&lt;2 (False)\n\n\n\n\n == \n\n\nEqual To\n\n\n4==2 (False)\n\n\n\n\n != \n\n\nNot Equal To\n\n\n4!=2 (True)\n\n\n\n\n &gt;= \n\n\nGreater Than or Equal To\n\n\n4&gt;=2 (True)\n\n\n\n &lt;= \n\n\nLess Than or Equal To\n\n\n4&lt;=2 (True)\n\n\n\n\nExpressions consisting of relational operators evaluate to a Boolean value\n\n\n\n\n\n\nCautionCode Challenge 1.3: Presssure sensor that determines whether to open a door\n\n\n\nWrite code that simulates a pressure sensor that opens a door when the pressure is larger than 10; otherwise, it closes the door.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nreading = float(input(\"Sensor Reading: \"))\n\nif reading &gt; 10:\n    status = \"Opening\"\nelse:\n    status = \"Closing\"\n\nprint(f\"{status} the door\")\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCheck Yourself 5: Relational operators\n\n\n\nOn Which line number is the Boolean expression True?\n\nx = 15      # 1\ny = 20      # 2\nz = 2       # 3\nx &gt; y       # 4\nz*x &lt;= y    # 5\ny &gt;= x-z    # 6\nz*10 == x   # 7\n\nA. 4\nB. 5\nC. 6\nD. 7\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nC",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#pythons-logical-operators",
    "href": "02_python/python-1.html#pythons-logical-operators",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Python’s Logical Operators",
    "text": "Python’s Logical Operators\n\n\n\n\nOperator\n\n\nWhat it does\n\n\nExamples\n\n\n\n\n\n\n and \n\n\nTrue only when both are True\n\n\n4&gt;2 and 4&lt;5 (True)\n\n\n\n\n or \n\n\nFalse only when both are False\n\n\n4&lt;2 or 4==4 (True)\n\n\n\n\n not \n\n\nNegation(Opposite)\n\n\nnot 4==2 (True)\n\n\n\n\n in \n\n\nSet operator\n\n\n4 in [2,4,7] (True)\n\n\n\n\n\n\n\n\n\n\nNoteCheck Yourself 6: Logical Operators\n\n\n\nIn the following code, which line evaluates to True?\n\nraining = False              # 1\nsnowing = True               # 2\nage = 45                     # 3\nage &lt; 18 and raining         # 4\nage &gt;= 18 and not snowing    # 5\nnot snowing or not raining   # 6\nage == 45 and not snowing    # 7\n\nA. 4\nB. 5\nC. 6\nD. 7\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nC",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "02_python/python-1.html#multiple-decisions-if-ladder",
    "href": "02_python/python-1.html#multiple-decisions-if-ladder",
    "title": "1. Input, output, variables, types, conditionals",
    "section": "Multiple Decisions: IF ladder",
    "text": "Multiple Decisions: IF ladder\nUse elif to make more than one decision in your if statement. Only one code block within the ladder is executed.\n\nif boolean-expression1:\n    statements-when-exp1-true\nelif boolean-expression2:\n    statements-when-exp2-true\nelif boolean-expression3:\n    statements-when-exp3-true\nelse:\n    statements-none-are-true\n\n\n\n#Elif versus multiple ifs...\n# One decision or multiple decisions. \n\nx = int(input(\"enter an integer\"))\n\n# one decision\nif x&gt;10:\n    print(\"A:bigger than 10\")\nelif x&gt;20:\n    print(\"A:bigger than 20\")    \n\n    # Multiple decisions\nif x&gt;10:\n    print(\"B:bigger than 10\")\nif x&gt;20:\n    print(\"B:bigger than 20\")\n\n\n\n\n\n\n\nNoteCheck Yourself 7: IF statement\n\n\n\nAssuming values x = 25 and y = 6, what will be printed when the following code is run?\n\nif x &gt; 20:\n    if y == 4:\n        print(\"One\")\n    elif y &gt; 4:\n        print(\"Two\")\n    else:\n        print(\"Three\")\nelse:\n    print(\"Four\")\n\nA. One\nB. Two\nC. Three\nD. Four\nVote now at https://PollEv.com/ist356m3\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nB\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 1.4: Number to letter grade\n\n\n\n\nLetter grades in a college class are computed as follows:\n\n95 and above is an A\n75 and above, but below 95 is a B\n50 and above, but below 75 is a C\nbelow 50 is F\n\nWrite a program to input the number grade and calculate the letter grade\nThe program should also print an error message if the provided number grade is out of range (i.e., &gt; 120 or &lt; 0).\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nnumber_grade = int(input(\"Enter your numerical grade: 0 - 120\"))\nletter_grade = \"unknown\"\n\nif number_grade &gt;= 0 and number_grade &lt;= 120:\n\n    if number_grade &gt;= 95:\n        letter_grade = \"A\"\n    elif number_grade &gt;= 75:\n        letter_grade = \"B\"\n    elif number_grade &gt;= 50:\n        letter_grade = \"C\"\n    else:\n        letter_grade = \"F\"\n\n    print(f\"For {number_grade} points the letter grade is {letter_grade}\")\n\nelse:\n    print(f\"Number grade of {number_grade} is out of range!\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "1. Input, output, variables, types, conditionals"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html",
    "href": "01_cli_conda/shell-intro.html",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "",
    "text": "What is a command shell and why would I use one?\n\n\nExplain how the shell relates to the keyboard, the screen, the operating system, and users’ programs.\nExplain when and why command-line interfaces should be used instead of graphical interfaces.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#command-note-sheet",
    "href": "01_cli_conda/shell-intro.html#command-note-sheet",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Command note sheet",
    "text": "Command note sheet\nThe following Word document has a table of many of the commands we cover in this tutorial (and others!):\n\nBash Commands cheat sheet (Click to download)\n\nYou can use this to keep notes of the commands you learn in this tutorial by filling out the “What it does / Meaning” column. When you’re done, you will have your own cheat sheet of bash commands for future reference.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#the-prompt",
    "href": "01_cli_conda/shell-intro.html#the-prompt",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "The prompt",
    "text": "The prompt\nWhen the shell is first opened, you are presented with a prompt, indicating that the shell is waiting for input.\n$\nThe shell typically uses $ as the prompt, but may use a different symbol. In the examples for this lesson, we’ll show the prompt as $. Most importantly, do not type the prompt when typing commands. Only type the command that follows the prompt. This rule applies both in these lessons and in lessons from other sources. Also note that after you type a command, you have to press the Enter key to execute it.\nThe prompt is followed by a text cursor, a character that indicates the position where your typing will appear. The cursor is usually a flashing or solid block, but it can also be an underscore or a pipe. You may have seen it in a text editor program, for example.\nNote that your prompt might look a little different. In particular, most popular shell environments by default put your user name and the host name before the $. Such a prompt might look like, e.g.:\ncdcapano@localhost $\nThe prompt might even include more than this. Do not worry if your prompt is not just a short $. This lesson does not depend on this additional information and it should also not get in your way. The only important item to focus on is the $ character itself and we will see later why.\n\n\n\n\n\n\nNoneOPTIONAL: Change your prompt to $\n\n\n\nIf you’d like to simplify your prompt for the purposes of this tutorial, copy and paste the code below into your terminal:\nPS1='$ '\n\n\nSo let’s try our first command, ls, which is short for listing. This command will list the contents of the current directory:\n$ ls\nDesktop     Downloads   Movies      Pictures\nDocuments   Library     Music       Public\n\n\n\n\n\n\nTipCommand not found\n\n\n\nIf the shell can’t find a program whose name is the command you typed, it will print an error message such as:\n$ ks\nks: command not found\nThis might happen if the command was mis-typed or if the program corresponding to that command is not installed.\n\n\n\nA Typical Problem\nYou are a marine biologist who has just returned from a six-month survey of the North Pacific Gyre, where you have sampled gelatinous marine life in the Great Pacific Garbage Patch. You have 1520 samples that you’ve run through an assay machine to measure the relative abundance of 300 proteins. You need to run these 1520 files through an imaginary program called goostats.sh. In addition to this huge task, you have to write up results by the end of the month, so your paper can appear in a special issue of Aquatic Goo Letters.\nIf you choose to run goostats.sh by hand using a GUI, you’ll have to select and open a file 1520 times. If goostats.sh takes 30 seconds to run each file, the whole process will take more than 12 hours. With the shell, you can instead assign your computer this mundane task while you focuses her attention on writing your paper.\nThe next few lessons will explore the ways you can achieve this. More specifically, the lessons explain how you can use a command shell to run the goostats.sh program, using loops to automate the repetitive steps of entering file names, so that your computer can work while you write your paper.\nAs a bonus, once you have put a processing pipeline together, you will be able to use it again whenever you collect more data.\nIn order to achieve your task, you need to know how to:\n\nnavigate to a file/directory\ncreate a file/directory\ncheck the length of a file\nchain commands together\nretrieve a set of files\niterate over files\nrun a shell script containing your pipeline\n\nDownload files:\nYou need to download some files to follow this lesson.\n\nDownload shell-lesson-data.zip and move the file to your Desktop.\nUnzip/extract the file. Let your instructor know if you need help with this step. You should end up with a new folder called shell-lesson-data on your Desktop.\n\n\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\nA shell is a program whose primary purpose is to read commands and run other programs.\nThis lesson uses Bash, the default shell in many implementations of Unix.\nPrograms can be run in Bash by entering commands at the command-line prompt.\nThe shell’s main advantages are its high action-to-keystroke ratio, its support for automating repetitive tasks, and its capacity to access networked machines.\nA significant challenge when using the shell can be knowing what commands need to be run and how to run them.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#where-am-i",
    "href": "01_cli_conda/shell-intro.html#where-am-i",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Where am I?",
    "text": "Where am I?\nFirst, let’s find out where we are by running a command called pwd (which stands for ‘print working directory’). Directories are like places — at any time while we are using the shell, we are in exactly one place called our current working directory. Commands mostly read and write files in the current working directory, i.e. ‘here’, so knowing where you are before running a command is important. pwd shows you where you are:\n$ pwd\n/Users/cdcapano\nThe computer’s response is /Users/cdcapano, which is my home directory.\n\nThe home directory\nThe home directory will look different on different operating systems.\n\nLinux: /home/cdcapano\nWindows: C:\\Users\\cdcapano (This may differ based on Windows versions)\n\nIf pwd returns something else, you may need to navigate to your home directory using the command cd.\nLet’s look at how a filesystem is organized:\n\nThe filesystem looks like an upside-down tree. We refer to the topmost directory as the “root directory”. It contains everything else. It is referred to with the / symbol.\nIn this illustration, we have several other directories stemming off of the root. The bin directory often refers to a location where programs are stored. tmp is a common directory on filesystems as a place to hold files that don’t need to be kept long-term. You may recognize the Users directory from the output we had above when typing pwd. We know the path of our home directory, where we currently are, is /Users/cdcapano.\n\n\n\n\n\n\nTipSlashes\n\n\n\nThere are two meanings for the / character. When it appears at the front of a path, it means “root”, when it appears within a path, it acts as a separator.\n\n\n\nThe Users folder may have more than one directory in it. For example, the image below depicts two users, cdcapano and aturing. The home directory just as the home directory for cdcapano is /Users/cdcapano, the home directory for aturing would be /Users/aturing. Typically, when you open a new command prompt, you will start in your home directory by default",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#listing-directory-contents",
    "href": "01_cli_conda/shell-intro.html#listing-directory-contents",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Listing directory contents",
    "text": "Listing directory contents\nWe often want to know what a directory contains. To do this, we use the “listing” command ls:\n$ ls\nwhich might return something that looks like this (results will vary):\nApplications  Documents  Library  Music    Public\nDesktop       Downloads  Movies   Pictures  \nls prints the names of the files and directories in your current location. Typing ls -F will give specific notation below based on the\n\nA trailing / indicates that this is a directory\n@ indicates a link\n* indicates an executable\n\nDepending on your shell’s default settings, the shell might also use colors to indicate whether each entry is a file or directory.\n\n\n\n\n\n\nTipClearing your terminal\n\n\n\nIf your screen gets too cluttered, you can clear your terminal using the clear command. You can still access previous commands using ↑↑ and ↓↓ to move line-by-line, or by scrolling in your terminal.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#getting-help",
    "href": "01_cli_conda/shell-intro.html#getting-help",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Getting help",
    "text": "Getting help\nMost bash commands have help menus that tell the user about how the command is used and what options are available for tailoring the function. There are two common ways to get help with commands, which can differ based on the source of the command (built-in or externally installed), and the operating system.\n\nWe can pass a --help option to any command (on linux and Git Bash).\n$ ls --help\nWe can read the manual on a function using the man command (linux and OSX)\n$ man ls\nTo navigate through the man pages, you may use ↑↑ and ↓↓ to move line-by-line, or try bb and SpacebarSpacebar to skip up and down by a full page. To search for a character or word in the man pages, use // followed by the character or word you are searching for. Sometimes a search will result in multiple hits. If so, you can move between hits using NN (for moving forward) and Shift+NShift+N (for moving backward). To **quit** the man pages, press qq.\n\n\n\n\n\n\n\nTipBash Built-in commands\n\n\n\nSome commands are built in to the Bash shell, rather than existing as separate programs on the filesystem. One example is the cd “change directory” command. If you get a message like No manual entry for cd, try help cd instead. The help command is how you get usage information for Bash built-ins.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#command-options",
    "href": "01_cli_conda/shell-intro.html#command-options",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Command options",
    "text": "Command options\nWe’ve seen that commands can have many options associated with them, which can control how the command functions. These are often notated by short and long forms that are typically equal in their meaning, but have conventional use in different scenarios.\nWhen options exist as both short and long options:\n\nUse the short option when typing commands directly into the shell to minimize keystrokes and get your task done faster.\nUse the long option in scripts to provide clarity. It will be read many times and typed once.\n\n\n\n\n\n\n\nTipUnsupported commandline options\n\n\n\nIf you try to use an option that is not supported, ls and other commands will usually print an error message similar to:\n$ ls -j\nMac zsh output:\nls: invalid option -- j\nusage: ls [-@ABCFGHILOPRSTUWXabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]\nother Unix output:\nls: invalid option -- 'j'\nTry 'ls --help' for more information.\n\n\n\nChallenge Questions\nExplore the ls command options to answer the following questions:\n\n\n\n\n\n\nCautionQuestion 1: Listing in long, human-readable format\n\n\n\nWhich options for ls produces output in long format that is human readable? (i.e. displaying something like 5.3K instead of 5369)\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nthe options -l and -h can be used with ls to make the output long and human readable. You can use them in two ways:\n\nls -l -h\nor string them together ls -lh\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 2: Listing in Reverse Chronological Order\n\n\n\nBy default, ls lists the contents of a directory in alphabetical order by name. The command ls -t lists items by time of last change instead of alphabetically. The command ls -r lists the contents of a directory in reverse order. Which file is displayed last when you combine the -t and -r options? Hint: You may need to use the -l option to see the last changed dates.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe most recently changed file is listed last when using -rt. This can be very useful for finding your most recent edits or checking to see if a new output file was written.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#exploring-other-directories",
    "href": "01_cli_conda/shell-intro.html#exploring-other-directories",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Exploring other directories",
    "text": "Exploring other directories\nNot only can we use ls on the current working directory, but we can use it to list the contents of a different directory. Let’s take a look at our Desktop directory by running ls -F Desktop, i.e., the command ls with the -F option and the argument Desktop. The argument Desktop tells ls that we want a listing of something other than our current working directory:\n$ ls -F Desktop\nYou should see your newly created directory “workshop”\nshell-lesson-data/\nNote that if a directory named Desktop does not exist in your current working directory, this command will return an error. Typically, a Desktop directory exists in your home directory, which we assume is the current working directory of your bash shell.\nYour output should be a list of all the files and sub-directories in your Desktop directory, including the shell-lesson-data directory you downloaded at the setup for this lesson. (On most systems, the contents of the Desktop directory in the shell will show up as icons in a graphical user interface behind all the open windows. See if this is the case for you.)\nOrganizing things hierarchically helps us keep track of our work. While it’s possible to put hundreds of files in our home directory just as it’s possible to pile hundreds of printed papers on our desk, it’s much easier to find things when they’ve been organized into sensibly-named subdirectories.\nNow that we know the shell-lesson-data directory is located in our Desktop directory, we can do two things.\nFirst, using the same strategy as before, we can look at its contents by passing a directory name to ls:\n$ ls -F Desktop/shell-lesson-data\nexercise-data/  north-pacific-gyre/\nSecond, we can actually change our location to a different directory, so we are no longer located in our home directory.\nThe command to change locations is cd followed by a directory name to change our working directory. cd stands for ‘change directory’, which is a bit misleading. The command doesn’t change the directory; it changes the shell’s current working directory. In other words it changes the shell’s settings for what directory we are in. The cd command is akin to double-clicking a folder in a graphical interface to get into that folder.\nLet’s say we want to move into the exercise-data directory we saw above. We can use the following series of commands to get there:\n$ cd Desktop\n$ cd shell-lesson-data\n$ cd exercise-data\nThese commands will move us from our home directory into our Desktop directory, then into the shell-lesson-data directory, then into the exercise-data directory. You will notice that cd doesn’t print anything. This is normal. Many shell commands will not output anything to the screen when successfully executed. But if we run pwd after it, we can see that we are now in /Users/cdcapano/Desktop/shell-lesson-data/exercise-data.\nIf we run ls -F without arguments now, it lists the contents of /Users/cdcapano/Desktop/shell-lesson-data/exercise-data, because that’s where we now are:\n$ pwd\n/Users/cdcapano/Desktop/shell-lesson-data/exercise-data\n$ ls -F\nalkanes/  animal-counts/  creatures/  numbers.txt  writing/\nWe now know how to go down the directory tree (i.e. how to go into a subdirectory), but how do we go up (i.e. how do we leave a directory and go into its parent directory)? We might try the following:\n$ cd shell-lesson-data\n-bash: cd: shell-lesson-data: No such file or directory\nBut we get an error! Why is this?\nWith our methods so far, cd can only see sub-directories inside your current directory. There are different ways to see directories above your current location; we’ll start with the simplest.\nThere is a shortcut in the shell to move up one directory level. It works as follows:\n$ cd ..\n.. is a special directory name meaning “the directory containing this one”, or more succinctly, the parent of the current directory. Sure enough, if we run pwd after running cd .., we’re back in /Users/cdcapano/Desktop/shell-lesson-data:\n$ pwd\n/Users/cdcapano/Desktop/shell-lesson-data\nThe special directory .. doesn’t usually show up when we run ls. If we want to display it, we can add the -a option to ls -F:\n$ ls -F -a\n./  ../  exercise-data/  north-pacific-gyre/\n-a stands for ‘show all’ (including hidden files); it forces ls to show us file and directory names that begin with ., such as .. (which, if we’re in /Users/cdcapano, refers to the /Users directory). As you can see, it also displays another special directory that’s just called ., which means ‘the current working directory’. It may seem redundant to have a name for it, but we’ll see some uses for it soon.\nNote that in most command line tools, multiple options can be combined with a single - and no spaces between the options; ls -F -a is equivalent to ls -Fa.\n\n\n\n\n\n\nNoteOther Hidden Files\n\n\n\nIn addition to the hidden directories .. and ., you may also see a file called .bash_profile. This file usually contains shell configuration settings. You may also see other files and directories beginning with .. These are usually files and directories that are used to configure different programs on your computer. The prefix . is used to prevent these configuration files from cluttering the terminal when a standard ls command is used.\n\n\nThese three commands are the basic commands for navigating the filesystem on your computer: pwd, ls, and cd. Let’s explore some variations on those commands. What happens if you type cd on its own, without giving a directory?\n$ cd\nHow can you check what happened? pwd gives us the answer!\n$ pwd\n/Users/cdcapano\nIt turns out that cd without an argument will return you to your home directory, which is great if you’ve got lost in your own filesystem.\nLet’s try returning to the exercise-data directory from before. Last time, we used three commands, but we can actually string together the list of directories to move to exercise-data in one step:\n$ cd Desktop/shell-lesson-data/exercise-data\nCheck that we’ve moved to the right place by running pwd and ls -F.\nIf we want to move up one level from the data directory, we could use cd ... But there is another way to move to any directory, regardless of your current location.\nSo far, when specifying directory names, or even a directory path (as above), we have been using relative paths. When you use a relative path with a command like ls or cd, it tries to find that location from where we are, rather than from the root of the file system.\nHowever, it is possible to specify the absolute path to a directory by including its entire path from the root directory, which is indicated by a leading slash. The leading / tells the computer to follow the path from the root of the file system, so it always refers to exactly one directory, no matter where we are when we run the command.\nThis allows us to move to our shell-lesson-data directory from anywhere on the filesystem (including from inside exercise-data). To find the absolute path we’re looking for, we can use pwd and then extract the piece we need to move to shell-lesson-data.\n$ pwd\n/Users/cdcapano/Desktop/shell-lesson-data/exercise-data\n$ cd /Users/cdcapano/Desktop/shell-lesson-data\nRun pwd and ls -F to ensure that we’re in the directory we expect.\n\n\n\n\n\n\nTipTwo more shortcuts\n\n\n\nThe shell interprets a tilde (~) character at the start of a path to mean “the current user’s home directory”. For example, if your home directory is /Users/cdcapano, then ~/data is equivalent to /Users/cdcapano/data. This only works if it is the first character in the path; here/there/~/elsewhere is not here/there/Users/cdcapano/elsewhere.\nAnother shortcut is the - (dash) character. cd will translate - into the previous directory I was in, which is faster than having to remember, then type, the full path. This is a very efficient way of moving back and forth between two directories – i.e. if you execute cd - twice, you end up back in the starting directory.\nThe difference between cd .. and cd - is that the former brings you up, while the latter brings you back.\n\nTry it!\nFirst navigate to ~/Desktop/shell-lesson-data (you should already be there).\n$ cd ~/Desktop/shell-lesson-data\nThen cd into the exercise-data/creatures directory\n$ cd exercise-data/creatures\nNow if you run\n$ cd -\nyou’ll see you’re back in ~/Desktop/shell-lesson-data. Run cd - again and you’re back in ~/Desktop/shell-lesson-data/exercise-data/creatures\n\n\n\n\nChallenge Questions\n\n\n\n\n\n\nCautionQuestion 3: Absolute vs Relative Paths\n\n\n\nStarting from /Users/cdcapano/data, which of the following commands could cdcapano use to navigate to her home directory, which is /Users/cdcapano?\n\ncd .\ncd /\ncd /home/cdcapano\ncd ../..\ncd ~\ncd home\ncd ~/data/..\ncd\ncd ..\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nNo: . stands for the current directory.\nNo: / stands for the root directory.\nNo: cdcapano’s home directory is /Users/cdcapano.\nNo: this command goes up two levels, i.e. ends in /Users.\nYes: ~ stands for the user’s home directory, in this case /Users/cdcapano.\nNo: this command would navigate into a directory home in the current directory if it exists.\nYes: unnecessarily complicated, but correct.\nYes: shortcut to go back to the user’s home directory.\nYes: goes up one level.\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 4: Relative Path Resolution\n\n\n\nUsing the filesystem diagram below, if pwd displays /Users/thing, what will ls -F ../backup display?\n\n\n../backup: No such file or directory\n2012-12-01 2013-01-08 2013-01-27\n2012-12-01/ 2013-01-08/ 2013-01-27/\noriginal/ pnas_final/ pnas_sub/\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nNo: there is a directory backup in /Users.\nNo: this is the content of Users/thing/backup, but with .., we asked for one level further up.\nNo: see previous explanation.\nYes: ../backup/ refers to /Users/backup/.\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 5: ls reading comprehension\n\n\n\nUsing the filesystem diagram below, if pwd displays /Users/backup, and -r tells ls to display things in reverse order, what command(s) will result in the following output:\npnas_sub/ pnas_final/ original/\n\n\nls pwd\nls -r -F\nls -r -F /Users/backup\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nNo: pwd is not the name of a directory.\nYes: ls without directory argument lists files and directories in the current directory.\nYes: uses the absolute path explicitly.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#general-syntax-of-a-shell-command",
    "href": "01_cli_conda/shell-intro.html#general-syntax-of-a-shell-command",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "General Syntax of a Shell Command",
    "text": "General Syntax of a Shell Command\nWe have now encountered commands, options, and arguments, but it is perhaps useful to formalise some terminology.\nConsider the command below as a general example of a command, which we will dissect into its component parts:\n$ ls -F /\n\nls is the command, with an option -F and an argument /. We’ve already encountered options which either start with a single dash (-), known as short options, or two dashes (--), known as long options. Options change the behavior of a command and arguments tell the command what to operate on (e.g. files and directories). Sometimes options and arguments are referred to as parameters. A command can be called with more than one option and more than one argument, but a command doesn’t always require an argument or an option.\nYou might sometimes see options being referred to as switches or flags, especially for options that take no argument. In this lesson we will stick with using the term option.\nEach part is separated by spaces. If you omit the space between ls and -F the shell will look for a command called ls-F, which doesn’t exist. Also, capitalization can be important. For example, ls -s will display the size of files and directories alongside the names, while ls -S will sort the files and directories by size, as shown below:\n$ cd ~/Desktop/shell-lesson-data\n$ ls -s exercise-data\ntotal 28\n 4 animal-counts   4 creatures  12 numbers.txt   4 alkanes   4 writing\nNote that the sizes returned by ls -s are in blocks. As these are defined differently for different operating systems, you may not obtain the same figures as in the example.\n$ ls -S exercise-data\nanimal-counts  creatures  alkanes  writing  numbers.txt\nPutting all that together, our command ls -F / above gives us a listing of files and directories in the root directory /. An example of the output you might get from the above command is given below:\n$ ls -F /\nApplications/         System/\nLibrary/              Users/\nNetwork/              Volumes/\n\nYour Task: Organizing Files\nKnowing this much about files and directories, You are ready to organize the files that the protein assay machine will create.\nYou create a directory called north-pacific-gyre (to remind yourself where the data came from), which will contain the data files from the assay machine and your data processing scripts.\nEach of your physical samples is labelled according to your lab’s convention with a unique ten-character ID, such as ‘NENE01729A’. This ID is what you used in your collection log to record the location, time, depth, and other characteristics of the sample, so you decide to use it within the filename of each data file. Since the output of the assay machine is plain text, you will call your files NENE01729A.txt, NENE01812A.txt, and so on. All 1520 files will go into the same directory.\nNow in your current directory shell-lesson-data, you can see what files you have using the command:\n$ ls north-pacific-gyre/\nThis command is a lot to type, but you can let the shell do most of the work through what is called tab completion. If you types:\n$ ls nor\nand then presses Tab (the tab key on her keyboard), the shell automatically completes the directory name for you:\n$ ls north-pacific-gyre/\nPressing Tab again does nothing, since there are multiple possibilities; pressing Tab twice brings up a list of all the files.\nIf you then presses G and then presses Tab again, the shell will append ‘goo’ since all files that start with ‘g’ share the first three characters ‘goo’.\n$ ls north-pacific-gyre/goo\nTo see all of those files, you can press Tab twice more.\nls north-pacific-gyre/goo\ngoodiff.sh   goostats.sh\nThis is called tab completion, and we will see it in many other tools as we go on.\n\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\nThe file system is responsible for managing information on the disk.\nInformation is stored in files, which are stored in directories (folders).\nDirectories can also store other directories, which then form a directory tree.\npwd prints the user’s current working directory.\nls [path] prints a listing of a specific file or directory; ls on its own lists the current working directory.\ncd [path] changes the current working directory.\nMost commands take options that begin with a single -.\nDirectory names in a path are separated with / on Unix, but \\ on Windows.\n/ on its own is the root directory of the whole file system.\nAn absolute path specifies a location from the root of the file system.\nA relative path specifies a location starting from the current location.\n. on its own means ‘the current directory’; .. means ‘the directory above the current one’.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#creating-directories-and-files",
    "href": "01_cli_conda/shell-intro.html#creating-directories-and-files",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Creating Directories and Files",
    "text": "Creating Directories and Files\nWe now know how to explore files and directories, but how do we create them in the first place?\nIn this episode we will learn about creating and moving files and directories, using the exercise-data/writing directory as an example.\n\nStep one: see where we are and what we already have\nWe should still be in the shell-lesson-data directory on the Desktop, which we can check using:\n$ pwd\n/Users/cdcapano/Desktop/shell-lesson-data\nNext we’ll move to the exercise-data/writing directory and see what it contains:\n$ cd exercise-data/writing/\n$ ls -F\nhaiku.txt  LittleWomen.txt\n\n\nCreate a directory\nLet’s create a new directory called thesis using the command mkdir thesis (which has no output):\n$ mkdir thesis\nAs you might guess from its name, mkdir means ‘make directory’. Since thesis is a relative path (i.e., does not have a leading slash, like /what/ever/thesis), the new directory is created in the current working directory:\n$ ls -F\nhaiku.txt  LittleWomen.txt  thesis/\nSince we’ve just created the thesis directory, there’s nothing in it yet:\n$ ls -F thesis\nNote that mkdir is not limited to creating single directories one at a time. The -p option allows mkdir to create a directory with nested subdirectories in a single operation:\n$ mkdir -p ../project/data ../project/results\nThe -R option to the ls command will list all nested subdirectories within a directory. Let’s use ls -FR to recursively list the new directory hierarchy we just created in the project directory:\n$ ls -FR ../project\n../project/:\ndata/  results/\n\n../project/data:\n\n../project/results:\n\n\n\n\n\n\nNoteTwo ways of doing the same thing\n\n\n\nUsing the shell to create a directory is no different than using a file explorer. If you open the current directory using your operating system’s graphical file explorer, the thesis directory will appear there too. While the shell and the file explorer are two different ways of interacting with the files, the files and directories themselves are the same.\n\n\n\n\n\n\n\n\nNoteGood names for files and directories\n\n\n\nComplicated names of files and directories can make your life painful when working on the command line. Here we provide a few useful tips for the names of your files and directories.\n\nDon’t use spaces.\n\nSpaces can make a name more meaningful, but since spaces are used to separate arguments on the command line it is better to avoid them in names of files and directories. You can use - or _ instead (e.g. north-pacific-gyre/ rather than north pacific gyre/). To test this out, try typing mkdir north pacific gyre and see what directory (or directories!) are made when you check with ls -F.\n\nDon’t begin the name with - (dash).\n\nCommands treat names starting with - as options.\n\nStick with letters, numbers, . (period or ‘full stop’), - (dash) and _ (underscore).\n\nMany other characters have special meanings on the command line. We will learn about some of these during this lesson. There are special characters that can cause your command to not work as expected and can even result in data loss.\nIf you need to refer to names of files or directories that have spaces or other special characters, you should surround the name in single quotes ('').\n\n\n\n\nCreate a text file\nLet’s change our working directory to thesis using cd, then run a text editor called Nano to create a file called draft.txt:\n$ cd thesis\n$ nano draft.txt\n\n\n\n\n\n\nNoteWhich editor?\n\n\n\nWhen we say, ‘nano is a text editor’ we really do mean ‘text’. It can only work with plain character data, not tables, images, or any other human-friendly media. We use it in examples because it is one of the least complex text editors. However, because of this trait, it may not be powerful enough or flexible enough for the work you need to do after this workshop. On Unix systems (such as Linux and macOS), many programmers use Emacs or Vim (both of which require more time to learn), or a graphical editor such as Gedit or VScode. On Windows, you may wish to use Notepad++. Windows also has a built-in editor called notepad that can be run from the command line in the same way as nano for the purposes of this lesson.\nNo matter what editor you use, you will need to know where it searches for and saves files. If you start it from the shell, it will (probably) use your current working directory as its default location. If you use your computer’s start menu, it may want to save files in your Desktop or Documents directory instead. You can change this by navigating to another directory the first time you ‘Save As…’\n\n\nLet’s type a few lines of text:\n\nOnce we’re happy with our text, we can press Ctrl+O (press the Ctrl or Control key and, while holding it down, press the O key) to write our data to disk. We will be asked to provide a name for the file that will contain our text. Press Return to accept the suggested default of draft.txt.\nOnce our file is saved, we can use Ctrl+X to quit the editor and return to the shell.\n\n\n\n\n\n\nTipControl, Ctrl, or ^ Key\n\n\n\nThe Control key is also called the ‘Ctrl’ key. There are various ways in which using the Control key may be described. For example, you may see an instruction to press the Control key and, while holding it down, press the X key, described as any of:\n\nControl-X\nControl+X\nCtrl-X\nCtrl+X\n^X\nC-x\n\nIn nano, along the bottom of the screen you’ll see ^G Get Help ^O WriteOut. This means that you can use Control-G to get help and Control-O to save your file.\n\n\nnano doesn’t leave any output on the screen after it exits, but ls now shows that we have created a file called draft.txt:\n$ ls\ndraft.txt\n\n\n\n\n\n\nCautionChallenge: Creating Files a Different Way\n\n\n\nWe have seen how to create text files using the nano editor. Now, try the following command:\n$ touch my_file.txt\n\nWhat did the touch command do? When you look at your current directory using the GUI file explorer, does the file show up?\nUse ls -l to inspect the files. How large is my_file.txt?\nWhen might you want to create a file this way?\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nThe touch command generates a new file called my_file.txt in your current directory. You can observe this newly generated file by typing ls at the command line prompt. my_file.txt can also be viewed in your GUI file explorer.\nWhen you inspect the file with ls -l, note that the size of my_file.txt is 0 bytes. In other words, it contains no data. If you open my_file.txt using your text editor it is blank.\nSome programs do not generate output files themselves, but instead require that empty files have already been generated. When the program is run, it searches for an existing file to populate with its output. The touch command allows you to efficiently generate a blank text file to be used by such programs.\n\n\n\n\nTo avoid confusion later on, we suggest removing the file you’ve just created before proceeding with the rest of the episode, otherwise future outputs may vary from those given in the lesson. To do this, use the following command:\n$ rm my_file.txt\n\n\n\nWhat’s in a name?\nYou may have noticed that all of your files are named ‘something dot something’, and in this part of the lesson, we always used the extension .txt. This is just a convention; we can call a file mythesis or almost anything else we want. However, most people use two-part names most of the time to help them (and their programs) tell different kinds of files apart. The second part of such a name is called the filename extension and indicates what type of data the file holds: .txt signals a plain text file, .pdf indicates a PDF document, .cfg is a configuration file full of parameters for some program or other, .png is a PNG image, and so on.\nThis is just a convention, albeit an important one. Files merely contain bytes; it’s up to us and our programs to interpret those bytes according to the rules for plain text files, PDF documents, configuration files, images, and so on.\nNaming a PNG image of a whale as whale.mp3 doesn’t somehow magically turn it into a recording of whale song, though it might cause the operating system to associate the file with a music player program. In this case, if someone double-clicked whale.mp3 in a file explorer program, the music player will automatically (and erroneously) attempt to open the whale.mp3 file.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#moving-files-and-directories",
    "href": "01_cli_conda/shell-intro.html#moving-files-and-directories",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Moving Files and Directories",
    "text": "Moving Files and Directories\nReturning to the shell-lesson-data/exercise-data/writing directory,\n$ cd ~/Desktop/shell-lesson-data/exercise-data/writing\nIn our thesis directory we have a file draft.txt which isn’t a particularly informative name, so let’s change the file’s name using mv, which is short for ‘move’:\n$ mv thesis/draft.txt thesis/quotes.txt\nThe first argument tells mv what we’re ‘moving’, while the second is where it’s to go. In this case, we’re moving thesis/draft.txt to thesis/quotes.txt, which has the same effect as renaming the file. Sure enough, ls shows us that thesis now contains one file called quotes.txt:\n$ ls thesis\nquotes.txt\nOne must be careful when specifying the target file name, since mv will silently overwrite any existing file with the same name, which could lead to data loss. By default, mv will not ask for confirmation before overwriting files. However, an additional option, mv -i (or mv --interactive), will cause mv to request such confirmation.\nNote that mv also works on directories.\nLet’s move quotes.txt into the current working directory. We use mv once again, but this time we’ll use just the name of a directory as the second argument to tell mv that we want to keep the filename but put the file somewhere new. (This is why the command is called ‘move’.) In this case, the directory name we use is the special directory name . that we mentioned earlier.\n$ mv thesis/quotes.txt .\nThe effect is to move the file from the directory it was in to the current working directory. ls now shows us that thesis is empty:\n$ ls thesis\n$\nAlternatively, we can confirm the file quotes.txt is no longer present in the thesis directory by explicitly trying to list it:\n$ ls thesis/quotes.txt\nls: cannot access 'thesis/quotes.txt': No such file or directory\nls with a filename or directory as an argument only lists the requested file or directory. If the file given as the argument doesn’t exist, the shell returns an error as we saw above. We can use this to see that quotes.txt is now present in our current directory:\n$ ls quotes.txt\nquotes.txt\n\n\n\n\n\n\nCautionChallenge: Moving files to a new folder\n\n\n\nAfter running the following commands, Jamie realizes that she put the files sucrose.dat and maltose.dat into the wrong folder. The files should have been placed in the raw folder.\n$ ls -F\n analyzed/ raw/\n$ ls -F analyzed\nfructose.dat glucose.dat maltose.dat sucrose.dat\n$ cd analyzed\nFill in the blanks to move these files to the raw/ folder (i.e. the one she forgot to put them in)\n$ mv sucrose.dat maltose.dat ____/____\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n$ mv sucrose.dat maltose.dat ../raw\nRecall that .. refers to the parent directory (i.e. one above the current directory) and that . refers to the current directory.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#copying-files-and-directories",
    "href": "01_cli_conda/shell-intro.html#copying-files-and-directories",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Copying Files and Directories",
    "text": "Copying Files and Directories\nThe cp command works very much like mv, except it copies a file instead of moving it. We can check that it did the right thing using ls with two paths as arguments — like most Unix commands, ls can be given multiple paths at once:\n$ cp quotes.txt thesis/quotations.txt\n$ ls quotes.txt thesis/quotations.txt\nquotes.txt   thesis/quotations.txt\nWe can also copy a directory and all its contents by using the recursive option -r, e.g. to back up a directory:\n$ cp -r thesis thesis_backup\nWe can check the result by listing the contents of both the thesis and thesis_backup directory:\n$ ls thesis thesis_backup\nthesis:\nquotations.txt\n\nthesis_backup:\nquotations.txt\nIt is important to include the -r flag. If you want to copy a directory and you omit this option you will see a message that the directory has been omitted because -r not specified.\n$ cp thesis thesis_backup\ncp: -r not specified; omitting directory 'thesis'\n\n\n\n\n\n\nCautionChallenge: Renaming files\n\n\n\nSuppose that you created a plain-text file in your current directory to contain a list of the statistical tests you will need to do to analyze your data, and named it statstics.txt\nAfter creating and saving this file you realize you misspelled the filename! You want to correct the mistake, which of the following commands could you use to do so?\n\ncp statstics.txt statistics.txt\nmv statstics.txt statistics.txt\nmv statstics.txt .\ncp statstics.txt .\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nNo. While this would create a file with the correct name, the incorrectly named file still exists in the directory and would need to be deleted.\nYes, this would work to rename the file.\nNo, the period(.) indicates where to move the file, but does not provide a new file name; identical file names cannot be created.\nNo, the period(.) indicates where to copy the file, but does not provide a new file name; identical file names cannot be created.\n\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Moving and copying\n\n\n\nWhat is the output of the closing ls command in the sequence shown below?\n$ pwd\n/Users/jamie/data\n$ ls\nproteins.dat\n$ mkdir recombined\n$ mv proteins.dat recombined/\n$ cp recombined/proteins.dat ../proteins-saved.dat\n$ ls\n\nproteins-saved.dat recombined\nrecombined\nproteins.dat recombined\nproteins-saved.dat\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nWe start in the /Users/jamie/data directory, and create a new folder called recombined. The second line moves (mv) the file proteins.dat to the new folder (recombined). The third line makes a copy of the file we just moved. The tricky part here is where the file was copied to. Recall that .. means ‘go up a level’, so the copied file is now in /Users/jamie. Notice that .. is interpreted with respect to the current working directory, not with respect to the location of the file being copied. So, the only thing that will show using ls (in /Users/jamie/data) is the recombined folder.\n\nNo, see explanation above. proteins-saved.dat is located at /Users/jamie\nYes\nNo, see explanation above. proteins.dat is located at /Users/jamie/data/recombined\nNo, see explanation above. proteins-saved.dat is located at /Users/jamie",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#removing-files-and-directories",
    "href": "01_cli_conda/shell-intro.html#removing-files-and-directories",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Removing files and directories",
    "text": "Removing files and directories\nReturning to the shell-lesson-data/exercise-data/writing directory, let’s tidy up this directory by removing the quotes.txt file we created. The Unix command we’ll use for this is rm (short for ‘remove’):\n$ rm quotes.txt\nWe can confirm the file has gone using ls:\n$ ls quotes.txt\nls: cannot access 'quotes.txt': No such file or directory\n\n\n\n\n\n\nImportantDeleting is forever\n\n\n\nThe Unix shell doesn’t have a trash bin that we can recover deleted files from (though most graphical interfaces to Unix do). Instead, when we delete files, they are unlinked from the file system so that their storage space on disk can be recycled. Tools for finding and recovering deleted files do exist, but there’s no guarantee they’ll work in any particular situation, since the computer may recycle the file’s disk space right away.\n\n\n\n\n\n\n\n\nCautionChallenge: Using rm Safely\n\n\n\nWhat happens when we execute rm -i thesis_backup/quotations.txt? Why would we want this protection when using rm?\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nrm: remove regular file 'thesis_backup/quotations.txt'? y\nThe -i option will prompt before (every) removal (use Y to confirm deletion or N to keep the file). The Unix shell doesn’t have a trash bin, so all the files removed will disappear forever. By using the -i option, we have the chance to check that we are deleting only the files that we want to remove.\n\n\n\n\n\nIf we try to remove the thesis directory using rm thesis, we get an error message:\n$ rm thesis\nrm: cannot remove 'thesis': Is a directory\nThis happens because rm by default only works on files, not directories.\nrm can remove a directory and all its contents if we use the recursive option -r, and it will do so without any confirmation prompts:\n$ rm -r thesis\nGiven that there is no way to retrieve files deleted using the shell, rm -r should be used with great caution (you might consider adding the interactive option rm -r -i).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#operations-with-multiple-filenames",
    "href": "01_cli_conda/shell-intro.html#operations-with-multiple-filenames",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Operations with Multiple Filenames",
    "text": "Operations with Multiple Filenames\nOftentimes one needs to copy or move several files at once. This can be done by providing a list of individual filenames, or specifying a naming pattern using wildcards. Wildcards are special characters that can be used to represent unknown characters or sets of characters when navigating the Unix file system.\n\n\n\n\n\n\nCautionChallenge: Copy with Multiple Filenames\n\n\n\nFor this exercise, you can test the commands in the shell-lesson-data/exercise-data directory.\nIn the example below, what does cp do when given several filenames and a directory name?\n$ mkdir backup\n$ cp creatures/minotaur.dat creatures/unicorn.dat backup/\nIn the example below, what does cp do when given three or more file names?\n$ cd creatures\n$ ls -F\nbasilisk.dat  minotaur.dat  unicorn.dat\n$ cp minotaur.dat unicorn.dat basilisk.dat\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nIf given more than one file name followed by a directory name (i.e. the destination directory must be the last argument), cp copies the files to the named directory.\nIf given three file names, cp throws an error such as the one below, because it is expecting a directory name as the last argument.\ncp: target 'basilisk.dat' is not a directory",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#using-wildcards-for-accessing-multiple-files-at-once",
    "href": "01_cli_conda/shell-intro.html#using-wildcards-for-accessing-multiple-files-at-once",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Using Wildcards for Accessing Multiple Files at Once",
    "text": "Using Wildcards for Accessing Multiple Files at Once\n* is a wildcard, which represents zero or more other characters. Let’s consider the shell-lesson-data/exercise-data/alkanes directory: *.pdb represents ethane.pdb, propane.pdb, and every file that ends with ‘.pdb’. On the other hand, p*.pdb only represents pentane.pdb and propane.pdb, because the ‘p’ at the front can only represent filenames that begin with the letter ‘p’.\n? is also a wildcard, but it represents exactly one character. So ?ethane.pdb could represent methane.pdb whereas *ethane.pdb represents both ethane.pdb and methane.pdb.\nWildcards can be used in combination with each other. For example, ???ane.pdb indicates three characters followed by ane.pdb, giving cubane.pdb  ethane.pdb  octane.pdb.\nWhen the shell sees a wildcard, it expands the wildcard to create a list of matching filenames before running the preceding command. As an exception, if a wildcard expression does not match any file, Bash will pass the expression as an argument to the command as it is. For example, typing ls *.pdf in the alkanes directory (which contains only files with names ending with .pdb) results in an error message that there is no file called *.pdf. However, generally commands like wc and ls see the lists of file names matching these expressions, but not the wildcards themselves. It is the shell, not the other programs, that expands the wildcards.\n\n\n\n\n\n\nCautionChallenge: List filenames matching a pattern\n\n\n\nWhen run in the alkanes directory, which ls command(s) will produce this output?\nethane.pdb   methane.pdb\n\nls *t*ane.pdb\nls *t?ne.*\nls *t??ne.pdb\nls ethane.*\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe solution is 3.\n1. shows all files whose names contain zero or more characters (*) followed by the letter t, then zero or more characters (*) followed by ane.pdb. This gives ethane.pdb  methane.pdb  octane.pdb  pentane.pdb.\n2. shows all files whose names start with zero or more characters (*) followed by the letter t, then a single character (?), then ne. followed by zero or more characters (*). This will give us octane.pdb and pentane.pdb but doesn’t match anything which ends in thane.pdb.\n3. fixes the problems of option 2 by matching two characters (??) between t and ne. This is the solution.\n4. only shows files starting with ethane..\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: More on wildcards\n\n\n\nSam has a directory containing calibration data, datasets, and descriptions of the datasets:\n.\n├── 2015-10-23-calibration.txt\n├── 2015-10-23-dataset1.txt\n├── 2015-10-23-dataset2.txt\n├── 2015-10-23-dataset_overview.txt\n├── 2015-10-26-calibration.txt\n├── 2015-10-26-dataset1.txt\n├── 2015-10-26-dataset2.txt\n├── 2015-10-26-dataset_overview.txt\n├── 2015-11-23-calibration.txt\n├── 2015-11-23-dataset1.txt\n├── 2015-11-23-dataset2.txt\n├── 2015-11-23-dataset_overview.txt\n├── backup\n│   ├── calibration\n│   └── datasets\n└── send_to_bob\n    ├── all_datasets_created_on_a_23rd\n    └── all_november_files\nBefore heading off to another field trip, she wants to back up her data and send some datasets to her colleague Bob. Sam uses the following commands to get the job done:\n$ cp *dataset* backup/datasets\n$ cp ____calibration____ backup/calibration\n$ cp 2015-____-____ send_to_bob/all_november_files/\n$ cp ____ send_to_bob/all_datasets_created_on_a_23rd/\nHelp Sam by filling in the blanks.\nThe resulting directory structure should look like this\n.\n├── 2015-10-23-calibration.txt\n├── 2015-10-23-dataset1.txt\n├── 2015-10-23-dataset2.txt\n├── 2015-10-23-dataset_overview.txt\n├── 2015-10-26-calibration.txt\n├── 2015-10-26-dataset1.txt\n├── 2015-10-26-dataset2.txt\n├── 2015-10-26-dataset_overview.txt\n├── 2015-11-23-calibration.txt\n├── 2015-11-23-dataset1.txt\n├── 2015-11-23-dataset2.txt\n├── 2015-11-23-dataset_overview.txt\n├── backup\n│   ├── calibration\n│   │   ├── 2015-10-23-calibration.txt\n│   │   ├── 2015-10-26-calibration.txt\n│   │   └── 2015-11-23-calibration.txt\n│   └── datasets\n│       ├── 2015-10-23-dataset1.txt\n│       ├── 2015-10-23-dataset2.txt\n│       ├── 2015-10-23-dataset_overview.txt\n│       ├── 2015-10-26-dataset1.txt\n│       ├── 2015-10-26-dataset2.txt\n│       ├── 2015-10-26-dataset_overview.txt\n│       ├── 2015-11-23-dataset1.txt\n│       ├── 2015-11-23-dataset2.txt\n│       └── 2015-11-23-dataset_overview.txt\n└── send_to_bob\n    ├── all_datasets_created_on_a_23rd\n    │   ├── 2015-10-23-dataset1.txt\n    │   ├── 2015-10-23-dataset2.txt\n    │   ├── 2015-10-23-dataset_overview.txt\n    │   ├── 2015-11-23-dataset1.txt\n    │   ├── 2015-11-23-dataset2.txt\n    │   └── 2015-11-23-dataset_overview.txt\n    └── all_november_files\n        ├── 2015-11-23-calibration.txt\n        ├── 2015-11-23-dataset1.txt\n        ├── 2015-11-23-dataset2.txt\n        └── 2015-11-23-dataset_overview.txt\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n$ cp *calibration.txt backup/calibration\n$ cp 2015-11-* send_to_bob/all_november_files/\n$ cp *-23-dataset* send_to_bob/all_datasets_created_on_a_23rd/\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Organizing directories and files\n\n\n\nJamie is working on a project, and she sees that her files aren’t very well organized:\n$ ls -F\nanalyzed/  fructose.dat    raw/   sucrose.dat\nThe fructose.dat and sucrose.dat files contain output from her data analysis. What command(s) covered in this lesson does she need to run so that the commands below will produce the output shown?\n$ ls -F\nanalyzed/   raw/\n$ ls analyzed\nfructose.dat    sucrose.dat\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nmv *.dat analyzed\nJamie needs to move her files fructose.dat and sucrose.dat to the analyzed directory. The shell will expand *.dat to match all .dat files in the current directory. The mv command then moves the list of .dat files to the ‘analyzed’ directory.\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Reproduce a folder structure\n\n\n\nYou’re starting a new experiment and would like to duplicate the directory structure from your previous experiment so you can add new data.\nAssume that the previous experiment is in a folder called 2016-05-18, which contains a data folder that in turn contains folders named raw and processed that contain data files. The goal is to copy the folder structure of the 2016-05-18 folder into a folder called 2016-05-20 so that your final directory structure looks like this:\n2016-05-20/\n└── data\n   ├── processed\n   └── raw\nWhich of the following set of commands would achieve this objective? What would the other commands do?\n$ mkdir 2016-05-20\n$ mkdir 2016-05-20/data\n$ mkdir 2016-05-20/data/processed\n$ mkdir 2016-05-20/data/raw\n$ mkdir 2016-05-20\n$ cd 2016-05-20\n$ mkdir data\n$ cd data\n$ mkdir raw processed\n$ mkdir 2016-05-20/data/raw\n$ mkdir 2016-05-20/data/processed\n$ mkdir -p 2016-05-20/data/raw\n$ mkdir -p 2016-05-20/data/processed\n$ mkdir 2016-05-20\n$ cd 2016-05-20\n$ mkdir data\n$ mkdir raw processed\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe first two sets of commands achieve this objective. The first set uses relative paths to create the top-level directory before the subdirectories.\nThe third set of commands will give an error because the default behavior of mkdir won’t create a subdirectory of a non-existent directory: the intermediate level folders must be created first.\nThe fourth set of commands achieve this objective. Remember, the -p option, followed by a path of one or more directories, will cause mkdir to create any intermediate subdirectories as required.\nThe final set of commands generates the ‘raw’ and ‘processed’ directories at the same level as the ‘data’ directory.\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\ncp [old] [new] copies a file.\nmkdir [path] creates a new directory.\nmv [old] [new] moves (renames) a file or directory.\nrm [path] removes (deletes) a file.\n* matches zero or more characters in a filename, so *.txt matches all files ending in .txt.\n? matches any single character in a filename, so ?.txt matches a.txt but not any.txt.\nUse of the Control key may be described in many ways, including Ctrl-X, Control-X, and ^X.\nThe shell does not have a trash bin: once something is deleted, it’s really gone.\nMost files’ names are something.extension. The extension isn’t required, and doesn’t guarantee anything, but is normally used to indicate the type of data in the file.\nDepending on the type of work you do, you may need a more powerful text editor than Nano.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#capturing-output-from-commands",
    "href": "01_cli_conda/shell-intro.html#capturing-output-from-commands",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Capturing output from commands",
    "text": "Capturing output from commands\n\nOutput page by page with less\nWe’ll continue to use cat in this lesson, for convenience and consistency, but it has the disadvantage that it always dumps the whole file onto your screen. More useful in practice is the command less (e.g. less lengths.txt). This displays a screenful of the file, and then stops. You can go forward one screenful by pressing the spacebar, or back one by pressing b. Press q to quit.\nWhich of these files contains the fewest lines? It’s an easy question to answer when there are only six files, but what if there were 6000? Our first step toward a solution is to run the command:\n$ wc -l *.pdb &gt; lengths.txt\nThe greater than symbol, &gt;, tells the shell to redirect the command’s output to a file instead of printing it to the screen. This command prints no screen output, because everything that wc would have printed has gone into the file lengths.txt instead. If the file doesn’t exist prior to issuing the command, the shell will create the file. If the file exists already, it will be silently overwritten, which may lead to data loss. Thus, redirect commands require caution.\nls lengths.txt confirms that the file exists:\n$ ls lengths.txt\nlengths.txt\nWe can now send the content of lengths.txt to the screen using cat lengths.txt. The cat command gets its name from ‘concatenate’ i.e. join together, and it prints the contents of files one after another. There’s only one file in this case, so cat just shows us what it contains:\n$ cat lengths.txt\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\n\n\nFiltering output\nNext we’ll use the sort command to sort the contents of the lengths.txt file. But first we’ll do an exercise to learn a little about the sort command:\n\n\n\n\n\n\nCautionChallenge: What does sort -n do?\n\n\n\nThe file shell-lesson-data/exercise-data/numbers.txt contains the following lines:\n10\n2\n19\n22\n6\nIf we run sort on this file, the output is:\n10\n19\n2\n22\n6\nIf we run sort -n on the same file, we get this instead:\n2\n6\n10\n19\n22\nExplain why -n has this effect.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe -n option specifies a numerical rather than an alphanumerical sort.\n\n\n\n\n\nWe will also use the -n option to specify that the sort is numerical instead of alphanumerical. This does not change the file; instead, it sends the sorted result to the screen:\n$ sort -n lengths.txt\n  9  methane.pdb\n 12  ethane.pdb\n 15  propane.pdb\n 20  cubane.pdb\n 21  pentane.pdb\n 30  octane.pdb\n107  total\nWe can put the sorted list of lines in another temporary file called sorted-lengths.txt by putting &gt; sorted-lengths.txt after the command, just as we used &gt; lengths.txt to put the output of wc into lengths.txt. Once we’ve done that, we can run another command called head to get the first few lines in sorted-lengths.txt:\n$ sort -n lengths.txt &gt; sorted-lengths.txt\n$ head -n 1 sorted-lengths.txt\n  9  methane.pdb\nUsing -n 1 with head tells it that we only want the first line of the file; -n 20 would get the first 20, and so on. Since sorted-lengths.txt contains the lengths of our files ordered from least to greatest, the output of head must be the file with the fewest lines.\n\n\n\n\n\n\nImportantRedirecting to the same file\n\n\n\nIt’s a very bad idea to try redirecting the output of a command that operates on a file to the same file. For example:\n$ sort -n lengths.txt &gt; lengths.txt\nDoing something like this may give you incorrect results and/or delete the contents of lengths.txt.\n\n\n\n\n\n\n\n\nCautionChallenge: What Does &gt;&gt; Mean?\n\n\n\nWe have seen the use of &gt;, but there is a similar operator &gt;&gt; which works slightly differently. We’ll learn about the differences between these two operators by printing some strings. We can use the echo command to print strings e.g.\n$ echo The echo command prints text\nThe echo command prints text\nNow test the commands below to reveal the difference between the two operators:\n$ echo hello &gt; testfile01.txt\nand:\n$ echo hello &gt;&gt; testfile02.txt\nHint: Try executing each command twice in a row and then examining the output files.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nIn the first example with &gt;, the string ‘hello’ is written to testfile01.txt, but the file gets overwritten each time we run the command.\nWe see from the second example that the &gt;&gt; operator also writes ‘hello’ to a file (in this case testfile02.txt), but appends the string to the file if it already exists (i.e. when we run it for the second time).\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: appending data\n\n\n\nWe have already met the head command, which prints lines from the start of a file. tail is similar, but prints lines from the end of a file instead.\nConsider the file shell-lesson-data/exercise-data/animal-counts/animals.csv. After these commands, select the answer that corresponds to the file animals-subset.csv:\n$ head -n 3 animals.csv &gt; animals-subset.csv\n$ tail -n 2 animals.csv &gt;&gt; animals-subset.csv\n\nThe first three lines of animals.csv\nThe last two lines of animals.csv\nThe first three lines and the last two lines of animals.csv\nThe second and third lines of animals.csv\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nOption 3 is correct. For option 1 to be correct we would only run the head command. For option 2 to be correct we would only run the tail command. For option 4 to be correct we would have to pipe the output of head into tail -n 2 by doing head -n 3 animals.csv | tail -n 2 &gt; animals-subset.csv",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#passing-output-to-another-command",
    "href": "01_cli_conda/shell-intro.html#passing-output-to-another-command",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Passing output to another command",
    "text": "Passing output to another command\nIn our example of finding the file with the fewest lines, we are using two intermediate files lengths.txt and sorted-lengths.txt to store output. This is a confusing way to work because even once you understand what wc, sort, and head do, those intermediate files make it hard to follow what’s going on. We can make it easier to understand by running sort and head together:\n$ sort -n lengths.txt | head -n 1\n  9  methane.pdb\nThe vertical bar, |, between the two commands is called a pipe. It tells the shell that we want to use the output of the command on the left as the input to the command on the right.\nThis has removed the need for the sorted-lengths.txt file.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#combining-multiple-commands",
    "href": "01_cli_conda/shell-intro.html#combining-multiple-commands",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Combining multiple commands",
    "text": "Combining multiple commands\nNothing prevents us from chaining pipes consecutively. We can for example send the output of wc directly to sort, and then send the resulting output to head. This removes the need for any intermediate files.\nWe’ll start by using a pipe to send the output of wc to sort:\n$ wc -l *.pdb | sort -n\n   9 methane.pdb\n  12 ethane.pdb\n  15 propane.pdb\n  20 cubane.pdb\n  21 pentane.pdb\n  30 octane.pdb\n 107 total\nWe can then send that output through another pipe, to head, so that the full pipeline becomes:\n$ wc -l *.pdb | sort -n | head -n 1\n   9  methane.pdb\nThis is exactly like a mathematician nesting functions like log(3x) and saying ‘the log of three times x’. In our case, the algorithm is ‘head of sort of line count of *.pdb’.\nThe redirection and pipes used in the last few commands are illustrated below:\n\n\n\n\n\n\n\nCautionChallenge: Piping commands together\n\n\n\nIn our current directory, we want to find the 3 files which have the least number of lines. Which command listed below would work?\n\nwc -l * &gt; sort -n &gt; head -n 3\nwc -l * | sort -n | head -n 1-3\nwc -l * | head -n 3 | sort -n\nwc -l * | sort -n | head -n 3\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nOption 4 is the solution. The pipe character | is used to connect the output from one command to the input of another. &gt; is used to redirect standard output to a file. Try it in the shell-lesson-data/exercise-data/alkanes directory!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#tools-designed-to-work-together",
    "href": "01_cli_conda/shell-intro.html#tools-designed-to-work-together",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Tools designed to work together",
    "text": "Tools designed to work together\nThis idea of linking programs together is why Unix has been so successful. Instead of creating enormous programs that try to do many different things, Unix programmers focus on creating lots of simple tools that each do one job well, and that work well with each other. This programming model is called ‘pipes and filters’. We’ve already seen pipes; a filter is a program like wc or sort that transforms a stream of input into a stream of output. Almost all of the standard Unix tools can work this way. Unless told to do otherwise, they read from standard input, do something with what they’ve read, and write to standard output.\nThe key is that any program that reads lines of text from standard input and writes lines of text to standard output can be combined with every other program that behaves this way as well. You can and should write your programs this way so that you and other people can put those programs into pipes to multiply their power.\n\n\n\n\n\n\nCautionChallenge: Pipe Reading Comprehension\n\n\n\nA file called animals.csv (in the shell-lesson-data/exercise-data/animal-counts folder) contains the following data:\n2012-11-05,deer,5\n2012-11-05,rabbit,22\n2012-11-05,raccoon,7\n2012-11-06,rabbit,19\n2012-11-06,deer,2\n2012-11-06,fox,4\n2012-11-07,rabbit,16\n2012-11-07,bear,1\nWhat text passes through each of the pipes and the final redirect in the pipeline below? Note, the sort -r command sorts in reverse order.\n$ cat animals.csv | head -n 5 | tail -n 3 | sort -r &gt; final.txt\nHint: build the pipeline up one command at a time to test your understanding\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe head command extracts the first 5 lines from animals.csv. Then, the last 3 lines are extracted from the previous 5 by using the tail command. With the sort -r command those 3 lines are sorted in reverse order. Finally, the output is redirected to a file: final.txt. The content of this file can be checked by executing cat final.txt. The file should contain the following lines:\n2012-11-06,rabbit,19\n2012-11-06,deer,2\n2012-11-05,raccoon,7\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Pipe Construction\n\n\n\nFor the file animals.csv from the previous exercise, consider the following command:\n$ cut -d , -f 2 animals.csv\nThe cut command is used to remove or ‘cut out’ certain sections of each line in the file, and cut expects the lines to be separated into columns by a Tab character. A character used in this way is called a delimiter. In the example above we use the -d option to specify the comma as our delimiter character. We have also used the -f option to specify that we want to extract the second field (column). This gives the following output:\ndeer\nrabbit\nraccoon\nrabbit\ndeer\nfox\nrabbit\nbear\nThe uniq command filters out adjacent matching lines in a file. How could you extend this pipeline (using uniq and another command) to find out what animals the file contains (without any duplicates in their names)?\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n$ cut -d , -f 2 animals.csv | sort | uniq\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Which Pipe?\n\n\n\nThe file animals.csv contains 8 lines of data formatted as follows:\n2012-11-05,deer,5\n2012-11-05,rabbit,22\n2012-11-05,raccoon,7\n2012-11-06,rabbit,19\n...\nThe uniq command has a -c option which gives a count of the number of times a line occurs in its input. Assuming your current directory is shell-lesson-data/exercise-data/animal-counts, what command would you use to produce a table that shows the total count of each type of animal in the file?\n\nsort animals.csv | uniq -c\nsort -t, -k2,2 animals.csv | uniq -c\ncut -d, -f 2 animals.csv | uniq -c\ncut -d, -f 2 animals.csv | sort | uniq -c\ncut -d, -f 2 animals.csv | sort | uniq -c | wc -l\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nOption 4. is the correct answer. If you have difficulty understanding why, try running the commands, or sub-sections of the pipelines (make sure you are in the shell-lesson-data/exercise-data/animal-counts directory).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#your-pipeline-checking-files",
    "href": "01_cli_conda/shell-intro.html#your-pipeline-checking-files",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Your Pipeline: Checking Files",
    "text": "Your Pipeline: Checking Files\nYou have run your samples through the assay machines and created 17 files in the north-pacific-gyre directory described earlier. As a quick check, starting from the shell-lesson-data directory, you type:\n$ cd north-pacific-gyre\n$ wc -l *.txt\nThe output is 18 lines that look like this:\n300 NENE01729A.txt\n300 NENE01729B.txt\n300 NENE01736A.txt\n300 NENE01751A.txt\n300 NENE01751B.txt\n300 NENE01812A.txt\n... ...\nNow you type this:\n$ wc -l *.txt | sort -n | head -n 5\n 240 NENE02018B.txt\n 300 NENE01729A.txt\n 300 NENE01729B.txt\n 300 NENE01736A.txt\n 300 NENE01751A.txt\nWhoops: one of the files is 60 lines shorter than the others. When you go back and check it, you see that you did that assay at 8:00 on a Monday morning — someone was probably in using the machine on the weekend, and you forgot to reset it. Before re-running that sample, you check to see if any files have too much data:\n$ wc -l *.txt | sort -n | tail -n 5\n 300 NENE02040B.txt\n 300 NENE02040Z.txt\n 300 NENE02043A.txt\n 300 NENE02043B.txt\n5040 total\nThose numbers look good — but what’s that ‘Z’ doing there in the third-to-last line? All of your samples should be marked ‘A’ or ‘B’; by convention, your lab uses ‘Z’ to indicate samples with missing information. To find others like it, you do this:\n$ ls *Z.txt\nNENE01971Z.txt    NENE02040Z.txt\nSure enough, when you check the log on your laptop, there’s no depth recorded for either of those samples. Since it’s too late to get the information any other way, you must exclude those two files from your analysis. You could delete them using rm, but there are actually some analyses you might do later where depth doesn’t matter, so instead, you’ll have to be careful later on to select files using the wildcard expressions NENE*A.txt NENE*B.txt.\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\nwc counts lines, words, and characters in its inputs.\ncat displays the contents of its inputs.\nsort sorts its inputs.\nhead displays the first 10 lines of its input by default without additional arguments.\ntail displays the last 10 lines of its input by default without additional arguments.\ncommand &gt; [file] redirects a command’s output to a file (overwriting any existing content).\ncommand &gt;&gt; [file] appends a command’s output to a file.\n[first] | [second] is a pipeline: the output of the first command is used as the input to the second.\nThe best way to use the shell is to use pipes to combine simple single-purpose programs (filters).’ll have to be careful later on to select files using the wildcard expressions NENE*A.txt NENE*B.txt.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#your-pipeline-processing-files",
    "href": "01_cli_conda/shell-intro.html#your-pipeline-processing-files",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Your pipeline: Processing files",
    "text": "Your pipeline: Processing files\nYou are now ready to process your data files using goostats.sh — a shell script written by your supervisor. This calculates some statistics from a protein sample file and takes two arguments:\n\nan input file (containing the raw data)\nan output file (to store the calculated statistics)\n\nSince you are still learning how to use the shell, you decides to build up the required commands in stages. Your first step is to make sure that you can select the right input files — remember, these are ones whose names end in ‘A’ or ‘B’, rather than ‘Z’. Moving to the north-pacific-gyre directory, you type:\n$ cd\n$ cd Desktop/shell-lesson-data/north-pacific-gyre\n$ for datafile in NENE*A.txt NENE*B.txt\n&gt; do\n&gt;     echo $datafile\n&gt; done\nNENE01729A.txt\nNENE01736A.txt\nNENE01751A.txt\n\n...\nNENE02040B.txt\nNENE02043B.txt\nYour next step is to decide what to call the files that the goostats.sh analysis program will create. Prefixing each input file’s name with ‘stats’ seems simple, so you modify your loop to do that:\n$ for datafile in NENE*A.txt NENE*B.txt\n&gt; do\n&gt;     echo $datafile stats-$datafile\n&gt; done\nNENE01729A.txt stats-NENE01729A.txt\nNENE01736A.txt stats-NENE01729A.txt\nNENE01751A.txt stats-NENE01729A.txt\n...\nNENE02040B.txt stats-NENE02040B.txt\nNENE02043B.txt stats-NENE02043B.txt\nYou haven’t actually run goostats.sh yet, but now you’re sure you can select the right files and generate the right output filenames.\nTyping in commands over and over again is becoming tedious, though, and you are worried about making mistakes, so instead of re-entering your loop, you press ↑. In response, the shell re-displays the whole loop on one line (using semi-colons to separate the pieces):\n$ for datafile in NENE*A.txt NENE*B.txt; do echo $datafile stats-$datafile; done\nUsing the ←, you navigate to the echo command and change it to bash goostats.sh:\n$ for datafile in NENE*A.txt NENE*B.txt; do bash goostats.sh $datafile stats-$datafile; done\nWhen you press Enter, the shell runs the modified command. However, nothing appears to happen — there is no output. After a moment, you realize that since your script doesn’t print anything to the screen any longer, you has no idea whether it is running, much less how quickly. You kill the running command by typing Ctrl+C, uses ↑ to repeat the command, and edit it to read:\n$ for datafile in NENE*A.txt NENE*B.txt; do echo $datafile;\nbash goostats.sh $datafile stats-$datafile; done\n\n\n\n\n\n\nTipBeginning and end\n\n\n\nWe can move to the beginning of a line in the shell by typing Ctrl+A and to the end using Ctrl+E.\n\n\nWhen you run your program now, it produces one line of output every five seconds or so:\nNENE01729A.txt\nNENE01736A.txt\nNENE01751A.txt\n...\n1518 times 5 seconds, divided by 60, tells her that her script will take about two hours to run. As a final check, you open another terminal window, go into north-pacific-gyre, and uses cat stats-NENE01729B.txt to examine one of the output files. It looks good, so you decides to get some coffee and catch up on your reading.\n\n\n\n\n\n\nTipThose who know history can choose to repeat it\n\n\n\nAnother way to repeat previous work is to use the history command to get a list of the last few hundred commands that have been executed, and then to use !123 (where ‘123’ is replaced by the command number) to repeat one of those commands. For example, if you types this:\n$ history | tail -n 5\n456  for datafile in NENE*A.txt NENE*B.txt; do   echo $datafile stats-$datafile; done\n457  for datafile in NENE*A.txt NENE*B.txt; do echo $datafile stats-$datafile; done\n458  for datafile in NENE*A.txt NENE*B.txt; do bash goostats.sh $datafile stats-$datafile; done\n459  for datafile in NENE*A.txt NENE*B.txt; do echo $datafile; bash goostats.sh $datafile\nstats-$datafile; done\n460  history | tail -n 5\nthen you can re-run goostats.sh on the files simply by typing !459.\n\n\n\n\n\n\n\n\nTipMore history commands\n\n\n\nThere are a number of other shortcut commands for getting at the history.\n\nCtrl+R enters a history search mode ‘reverse-i-search’ and finds the most recent command in your history that matches the text you enter next. Press Ctrl+R one or more additional times to search for earlier matches. You can then use the left and right arrow keys to choose that line and edit it then hit Return to run the command.\n!! retrieves the immediately preceding command (you may or may not find this more convenient than using ↑)\n!$ retrieves the last word of the last command. That’s useful more often than you might expect: after bash goostats.sh NENE01729B.txt stats-NENE01729B.txt, you can type less !$ to look at the file stats-NENE01729B.txt, which is quicker than doing ↑ and editing the command-line.\n\n\n\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\nA for loop repeats commands once for every thing in a list.\nEvery for loop needs a variable to refer to the thing it is currently operating on.\nUse $name to expand a variable (i.e., get its value). ${name} can also be used.\nDo not use spaces, quotes, or wildcard characters such as ‘*’ or ‘?’ in filenames, as it complicates variable expansion.\nGive files consistent names that are easy to match with wildcard patterns to make it easy to select them for looping.\nUse the up-arrow key to scroll up through previous commands to edit and repeat them.\nUse Ctrl+R to search through the previously entered commands.\nUse history to display recent commands, and ![number] to repeat a command by number.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/shell-intro.html#your-pipeline-creating-a-script",
    "href": "01_cli_conda/shell-intro.html#your-pipeline-creating-a-script",
    "title": "Introduction to Command Line Interface and the Unix Shell",
    "section": "Your pipeline: Creating a script",
    "text": "Your pipeline: Creating a script\nYour supervisor insisted that all your analytics must be reproducible. The easiest way to capture all the steps is in a script.\nFirst we return to your project directory:\n$ cd ../../north-pacific-gyre/\nYou create a file using nano …\n$ nano do-stats.sh\n…which contains the following:\n# Calculate stats for data files.\nfor datafile in \"$@\"\ndo\n    echo $datafile\n    bash goostats.sh $datafile stats-$datafile\ndone\nYou save this in a file called do-stats.sh so that you can now re-do the first stage of your analysis by typing:\n$ bash do-stats.sh NENE*A.txt NENE*B.txt\nYou can also do this:\n$ bash do-stats.sh NENE*A.txt NENE*B.txt | wc -l\nso that the output is just the number of files processed rather than the names of the files that were processed.\nOne thing to note about your script is that it lets the person running it decide what files to process. You could have written it as:\n# Calculate stats for Site A and Site B data files.\nfor datafile in NENE*A.txt NENE*B.txt\ndo\n    echo $datafile\n    bash goostats.sh $datafile stats-$datafile\ndone\nThe advantage is that this always selects the right files: you doesn’t have to remember to exclude the ‘Z’ files. The disadvantage is that it always selects just those files — you can’t run it on all files (including the ‘Z’ files), or on the ‘G’ or ‘H’ files your colleagues in Antarctica are producing, without editing the script. If you wanted to be more adventurous, you could modify your script to check for command-line arguments, and use NENE*A.txt NENE*B.txt if none were provided. Of course, this introduces another tradeoff between flexibility and complexity.\n\n\n\n\n\n\nCautionChallenge: Variables in shell scripts\n\n\n\nIn the alkanes directory, imagine you have a shell script called script.sh containing the following commands:\nhead -n $2 $1\ntail -n $3 $1\nWhile you are in the alkanes directory, you type the following command:\n$ bash script.sh '*.pdb' 1 1\nWhich of the following outputs would you expect to see?\n\nAll of the lines between the first and the last lines of each file ending in .pdb in the alkanes directory\nThe first and the last line of each file ending in .pdb in the alkanes directory\nThe first and the last line of each file in the alkanes directory\nAn error because of the quotes around *.pdb\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe correct answer is 2.\nThe special variables $1, $2 and $3 represent the command line arguments given to the script, such that the commands run are:\n$ head -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb\n$ tail -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb\nThe shell does not expand '*.pdb' because it is enclosed by quote marks. As such, the first argument to the script is '*.pdb' which gets expanded within the script by head and tail\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Find the longest file with a given extension\n\n\n\nWrite a shell script called longest.sh that takes the name of a directory and a filename extension as its arguments, and prints out the name of the file with the most lines in that directory with that extension. For example:\n$ bash longest.sh shell-lesson-data/exercise-data/alkanes pdb\nwould print the name of the .pdb file in shell-lesson-data/exercise-data/alkanes that has the most lines.\nFeel free to test your script on another directory e.g.\n$ bash longest.sh shell-lesson-data/exercise-data/writing txt\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n# Shell script which takes two arguments:\n#    1. a directory name\n#    2. a file extension\n# and prints the name of the file in that directory\n# with the most lines which matches the file extension.\n\nwc -l $1/*.$2 | sort -n | tail -n 2 | head -n 1\nThe first part of the pipeline, wc -l $1/*.$2 | sort -n, counts the lines in each file and sorts them numerically (largest last). When there’s more than one file, wc also outputs a final summary line, giving the total number of lines across all files. We use tail -n 2 | head -n 1 to throw away this last line.\nWith wc -l $1/*.$2 | sort -n | tail -n 1 we’ll see the final summary line: we can build our pipeline up in pieces to be sure we understand the output.\n\n\n\n\n\n\n\n\n\n\n\nCautionChallenge: Script reading comprehension\n\n\n\nFor this question, consider the shell-lesson-data/exercise-data/alkanes directory once again. This contains a number of .pdb files in addition to any other files you may have created. Explain what each of the following three scripts would do when run as bash script1.sh *.pdb, bash script2.sh *.pdb, and bash script3.sh *.pdb respectively.\n# Script 1\necho *.*\n# Script 2\nfor filename in $1 $2 $3\ndo\n    cat $filename\ndone\n# Script 3\necho $@.pdb\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nIn each case, the shell expands the wildcard in *.pdb before passing the resulting list of file names as arguments to the script.\nScript 1 would print out a list of all files containing a dot in their name. The arguments passed to the script are not actually used anywhere in the script.\nScript 2 would print the contents of the first 3 files with a .pdb file extension. $1, $2, and $3 refer to the first, second, and third argument respectively.\nScript 3 would print all the arguments to the script (i.e. all the .pdb files), followed by .pdb. $@ refers to all the arguments given to a shell script.\ncubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb propane.pdb.pdb\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Points\n\n\n\n\nSave commands in files (usually called shell scripts) for re-use.\nbash [filename] runs the commands saved in a file.\n$@ refers to all of a shell script’s command-line arguments.\n$1, $2, etc., refer to the first command-line argument, the second command-line argument, etc.\nPlace variables in quotes if the values might have spaces in them.\nLetting users decide what files to process is more flexible and more consistent with built-in Unix commands.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Introduction to Command Line Interface and the Unix Shell"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html",
    "href": "01_cli_conda/conda.html",
    "title": "Bash variables, environments, and conda",
    "section": "",
    "text": "Note: This task requires that you have conda installed already. Follow the instructions here to do that if you have not done so already.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#the-export-command",
    "href": "01_cli_conda/conda.html#the-export-command",
    "title": "Bash variables, environments, and conda",
    "section": "The export Command",
    "text": "The export Command\nIn Bash, variables normally exist only in the shell where they were defined.\nThe export command marks a variable so that it is passed to child processes (programs or scripts started from the current shell).\n\nExample:\nMYVAR=\"Hello\"\nbash -c 'echo $MYVAR'\n# (No output — MYVAR was not exported)\n\nexport MYVAR=\"Hello\"\nbash -c 'echo $MYVAR'\n# Output: Hello\nIn the first case, the new Bash process doesn’t know about MYVAR.\nIn the second case, export makes it available to the child process.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#the-source-command",
    "href": "01_cli_conda/conda.html#the-source-command",
    "title": "Bash variables, environments, and conda",
    "section": "The source Command",
    "text": "The source Command\nWhen you run a Bash script normally, it executes in a subshell (a separate process).\nAny variables set inside that script will only exist in that subshell and disappear when the script finishes.\nThe source command runs a script in the current shell.\nThis means variables or changes made by the script persist after it finishes.\n\nExample: Difference between source and export\ntest.sh:\nMYVAR=\"Hello\"\nCase 1 — Running normally:\nbash test.sh\necho $MYVAR\n# (No output — MYVAR is not set in current shell)\nCase 2 — Using source:\nsource test.sh\necho $MYVAR\n# Output: Hello\nCase 3 — Using export in a script:\n# test_export.sh\nexport MYVAR=\"Hello\"\nbash test_export.sh\necho $MYVAR\n# (No output — export makes it available to child processes, not the parent shell)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#how-environments-work",
    "href": "01_cli_conda/conda.html#how-environments-work",
    "title": "Bash variables, environments, and conda",
    "section": "How environments work",
    "text": "How environments work\n\nEach shell session has its own environment.\nWhen you start a new process (run a command or script), it inherits a copy of your current environment.\nVariables you define with export become part of the environment and are visible to child processes.\nVariables defined without export are shell variables — they exist only in the current shell and are not passed to child processes.\n\nYou can view all your current environment variables by running:\nenv",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#the-path-variable",
    "href": "01_cli_conda/conda.html#the-path-variable",
    "title": "Bash variables, environments, and conda",
    "section": "The PATH Variable",
    "text": "The PATH Variable\nThe PATH variable controls where Bash searches for executables. It points to a : separated list of absolute paths. See what yours is set to by running:\necho $PATH\nWhen you execute any program in the shell, bash cycles through the list of paths specified in PATH until it finds an executable file that matches the name.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#create-environment",
    "href": "01_cli_conda/conda.html#create-environment",
    "title": "Bash variables, environments, and conda",
    "section": "Create Environment",
    "text": "Create Environment\nconda create --name py311 python=3.11\nThis creates a new environment called py311 with Python version 3.11 installed.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#activatedeactivate-environments",
    "href": "01_cli_conda/conda.html#activatedeactivate-environments",
    "title": "Bash variables, environments, and conda",
    "section": "Activate/Deactivate Environments",
    "text": "Activate/Deactivate Environments\nconda activate py311\nActivating an environment changes your shell’s environment variables (especially PATH) so that commands use the software installed in that environment. You’ll see the environment name in parentheses at the start of your shell prompt.\nconda deactivate\nDeactivating an environment restores your shell to its previous state, removing the environment-specific entries from PATH and other variables.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#install-packages",
    "href": "01_cli_conda/conda.html#install-packages",
    "title": "Bash variables, environments, and conda",
    "section": "Install Packages",
    "text": "Install Packages\nconda install numpy\nInstalls the package numpy into the currently active environment.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#channels",
    "href": "01_cli_conda/conda.html#channels",
    "title": "Bash variables, environments, and conda",
    "section": "Channels",
    "text": "Channels\nA channel is a location (usually online) where Conda looks for packages. When you install or search for a package, Conda queries one or more channels.\n\nDefault channel: If you installed Conda via Miniforge, the default channel is conda-forge, a large community-maintained repository.\nSpecialized channels: Some fields use dedicated channels, such as bioconda for bioinformatics software.\n\nTo install from a specific channel:\nconda install -c bioconda package_name\nHere, -c bioconda tells Conda to search the bioconda channel for the package.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#challenge-questions",
    "href": "01_cli_conda/conda.html#challenge-questions",
    "title": "Bash variables, environments, and conda",
    "section": "Challenge questions",
    "text": "Challenge questions\n\nCreate a conda environment called conda-test.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nconda create -n conda-test\n\n\n\nActivate your conda-test environment and install numpy into it.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nconda activate conda-test\nconda install numpy\n\n\n\nPrint your $PATH; you should see that the first couple of paths point to a conda directory with the name of your environment (conda-test) in it.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\necho $PATH\n\n\n\nCreate a python script called check_versions.py that contains the following code:\n#!/usr/bin/env python\nimport platform\nimport numpy\nprint(\"Python version: \", platform.python_version())\nnpvers = numpy.__version__\nprint(\"Numpy version: \", npvers)\nChange the permissions on check_versions.py to make it executable.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nRun:\nnano check_versions.py\nType/paste in the code. Then type Ctrl+O to save, then Ctrl+X to exit.\nRun:\nchmod +x check_versions.py\n\n\n\n\nSee what version of Python and numpy you installed by running check_versions.py script:\n./check_versions.py\nThis will print out the current time followed by the versions of Python and numpy you are using.\nSuppose that you need to use an older version of Python and numpy for a project. Create another conda environment called conda-test2 and install Python version 3.9 and numpy 1.24.1 into it.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nconda create -n conda-test2 python=3.9 numpy=1.24.1\n\n\n\nActivate conda-test2 and run check versions to verify that you have the desired versions.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nconda activate conda-test2\n./check_versions.py\n\n\n\nCreate an environment called biotest and install gofasta in it using the bioconda channel. (You can specify the channel for conda create and/or conda install using the -c flag, just as we did the in search command above.)\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nconda create -n biotest -c bioconda gofasta\n\n\n\nNow activate the bioconda channel and test that gofasta is installed by running:\nconda activate biotest\ngofasta --help\nYou should get a help message giving information about gofasta.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#removing-environments",
    "href": "01_cli_conda/conda.html#removing-environments",
    "title": "Bash variables, environments, and conda",
    "section": "Removing environments",
    "text": "Removing environments\nLets remove some of the environments we created to save space. You can see all the environments you have installed by running:\nconda info --envs\nLet’s uninstall the conda-test environment. First, if you are not in the base environment, go to it by running conda deactivate. This will take you to your last active environment. Keep deactivating until you see (base) on the left of the prompt. Alternatively, just type conda activate (with no environment name) to go directly to the base environment. Now let’s remove the conda-test environment by running:\nconda remove --all -n conda-test\nNow run conda info --envs again to check that the environment is installed. Repeat for any other environments you’d like to remove.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#bash-customization",
    "href": "01_cli_conda/conda.html#bash-customization",
    "title": "Bash variables, environments, and conda",
    "section": "Bash Customization",
    "text": "Bash Customization\nCustomizing your Bash shell can improve your workflow and make frequently used commands easier to run.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#example-enabling-color-output-in-ls",
    "href": "01_cli_conda/conda.html#example-enabling-color-output-in-ls",
    "title": "Bash variables, environments, and conda",
    "section": "Example: Enabling Color Output in ls",
    "text": "Example: Enabling Color Output in ls\nOn many systems, the output of ls returns text that is the same color, regardless of if it’s a file, folder, or something else. However, if you run:\nls -G\nThen the output text will be colored depending on what item is; e.g., directory names will be in blue and files in white.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#aliases",
    "href": "01_cli_conda/conda.html#aliases",
    "title": "Bash variables, environments, and conda",
    "section": "Aliases",
    "text": "Aliases\nAliases map a short command to a longer one. For example, if we always want the output of ls to be colored, we can alias ls -G with ls with:\nalias ls=\"ls -hG\"",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#customization-with-.bash_profile-and-.bashrc",
    "href": "01_cli_conda/conda.html#customization-with-.bash_profile-and-.bashrc",
    "title": "Bash variables, environments, and conda",
    "section": "Customization with .bash_profile and .bashrc",
    "text": "Customization with .bash_profile and .bashrc\nYou may find an alias like the above so useful that you want it to run everytime you open a terminal. There are files that are run automatically when Bash starts that allow you to do just that. They are:\n\n~/.bash_profile\nRuns for login shells. Good for commands that should only run once when you log in (e.g., setting environment variables).\n~/.bashrc\nRuns for interactive non-login shells. Good for commands you want every interactive shell to run (e.g., aliases, functions).\n\nOn many systems, .bash_profile will call .bashrc so settings apply in both contexts.\n\nTip: If unsure where to put something, put it in .bashrc and make sure .bash_profile sources .bashrc.\n\n\n\n\n\n\n\nNoteUsing Zsh?\n\n\n\nIf your default shell is zsh (common on macOS Catalina and later), the equivalent files are: - ~/.zprofile – similar to .bash_profile - ~/.zshrc – similar to .bashrc\nConfiguration changes should go into ~/.zshrc for most use cases.\n\n\nYou can put any bash command you like in these scripts to further customize your shell. For example, you can configure your shell so that pressing the up arrow searches backward through your command history for commands starting with the text you have typed, and the down arrow searches forward with:\n\nfor bash (add this to your ~/.bashrc):\nbind '\"\\e[A\": history-search-backward'\nbind '\"\\e[B\": history-search-forward'\nfor Zsh (add this to your ~/.zshrc):\nbindkey \"^[[A\" history-search-backward\nbindkey \"^[[B\" history-search-forward",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#redirection-and-append",
    "href": "01_cli_conda/conda.html#redirection-and-append",
    "title": "Bash variables, environments, and conda",
    "section": "Redirection and Append",
    "text": "Redirection and Append\nRedirect output to a file:\necho \"Hello\" &gt; file.txt\nOverwrites file.txt with “Hello”.\nAppend output to a file:\necho \"World\" &gt;&gt; file.txt\nAdds “World” to the end of file.txt.\n\n\n\n\n\n\nCautionChallenge:\n\n\n\nCreate a file called input.txt that contains the text “To be, or not to be,” on the first line. Then add “that is the question.” on the second line.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\necho \"To be, or not to be,\" &gt; input.txt\necho \"that is the question.\" &gt;&gt; input.txt",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#text-manipulation-with-sed",
    "href": "01_cli_conda/conda.html#text-manipulation-with-sed",
    "title": "Bash variables, environments, and conda",
    "section": "Text Manipulation with sed",
    "text": "Text Manipulation with sed\nIt frequently happens that you need to search for and replace text in a file. The program sed is used for that. For example, let’s modify the text in the input.txt you created above, and replace the word “be” with “use AI”:\nsed 's/ be/ use AI/g' input.txt\nWe can write results to a new file:\nsed 's/ be/ use AI/g' input.txt &gt; output.txt",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#file-permissions-and-ownership",
    "href": "01_cli_conda/conda.html#file-permissions-and-ownership",
    "title": "Bash variables, environments, and conda",
    "section": "File Permissions and Ownership",
    "text": "File Permissions and Ownership\n\nViewing Permissions\nls -l\nExample output:\n-rw-r--r--  1 user  staff   1234 Aug 14 12:00 file.txt\n\nFirst character: file type (- = file, d = directory)\nNext three: owner permissions\nNext three: group permissions\nNext three: others’ permissions\n\n\n\nChanging Permissions\nchmod u+x script.sh   # add execute permission for owner\nchmod u-w file.txt    # remove write permission for owner\n\n\nChanging Ownership\nsudo chown newuser file.txt",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "01_cli_conda/conda.html#conditionals",
    "href": "01_cli_conda/conda.html#conditionals",
    "title": "Bash variables, environments, and conda",
    "section": "Conditionals",
    "text": "Conditionals\nIn Bash, as with any language, conditionals allow you to execute commands only if certain conditions are true. They help control the flow of your script, enabling decisions based on system state, user input, or computation results.\nThe basic syntax for an if statement in Bash is:\nif [ CONDITION ]; then\n  # commands to run if CONDITION is true\nelif [ OTHER_CONDITION ]; then\n  # commands to run if OTHER_CONDITION is true\nelse\n  # commands to run if no conditions are true\nfi\n\nKey points:\n\nThe square brackets [ ] are a synonym for the test command.\nYou must have spaces after [ and before ].\nCommon tests:\n\n-f file — file exists and is a regular file\n-d directory — directory exists\nstring1 = string2 — strings are equal\nn1 -eq n2 — numbers are equal\n! CONDITION — logical NOT (true if CONDITION is false)\n\n\nFor example, to check if a file exists:\nif [ -f \"file.txt\" ]; then\n  echo \"File exists\"\nelse\n  echo \"File does not exist\"\nfi",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments",
      "Bash variables, environments, and `conda`"
    ]
  },
  {
    "objectID": "00_intro/index.html",
    "href": "00_intro/index.html",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "In IST356 we teach you programming for data analytics using the tools and techiques used by those in the industry. As such, you will learn how to setup and configure development environments, use git and github and learn to systematically test your code.\nThis is unlike IST256 where the emphasis is on the programming basics and the motivations behind learning computational thinking.\n\n\n\n\n\n\n\n\n\nIST356\nIST256\n\n\n\n\nProgramming Environment\nInstall vscode + git + python on your computer\nWeb-hosted jupyterhub\n\n\nAssignment Submissions\nStudent learn and use git / github\nBuilt-in assignment submission\n\n\nCoursework, slides, examples\nYou clone / diff the prof repo\nAutodiff and merge at login\n\n\n\nIn this course you’ll do it the real way, nothing is hidden / abstracted away for you.\n\n\nHere’s what you’ll need to install on your computer. Please read this entire section prior to installing. :-)\n\nVisual Studio Code: https://code.visualstudio.com/Download\nThis is a free editor with testing and debugging capabilities. As you install you can accept all defaults, except the last one: DO NOT launch the application when complete! If you do, simply close it.\nGit Source Code Manager: https://git-scm.com/download/\nAs you set it up, you will be asked several questions, for which the default selection is fine. EXCEPTION: choose Visual Studio Code as the default editor.\n\n\n\n\n\n\nWarning\n\n\n\nMac users: you’ll see multiple options for downloading Git Source Code Manager. I recommend using the Xcode version. Clicking on that link will redirect you to the Apple store; from there you will download and install Xcode.\n\n\nMiniconda Do not install this yet! You will install this using the terminal in VSCode after VSCode and Git are installed and setup; see instructions below.\n\n\n\n\nBeing an SU student comes with some perks. One of them is a Github for Education account and accompanying “packpack” of goodies. To use this benefit, your github account must be associated with your @syr.edu email.\n\n\nAssociate your current account with SU, by adding your email:\n\nGo to to https://github.com and click Sign In\nOnce you have logged in, go to: https://github.com/settings/emails\n\nAdd and verify your SU email address.\n\n\n\n\nYou’ll need to create an account:\n\nGo to to https://github.com and click Sign In\nFollow the on-screen instructions to sign up for an account.\nMake sure to use your @syr.edu email for the account.\nAdd your personal email when you’re at it so you don’t lose Github access after you graduate!\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce you have a GitHub account, please email your GitHub username to me at cdcapano@syr.edu.\n\n\n\n\n\n\nGo to the Github Backpack site: https://education.github.com/pack\nand click Sign Up For Student Developer Pack\nOnce your account is verified, you will have backpack access.\nYou’ll know its active when you check your billing plan: https://github.com/settings/billing/summary\nyou should see a credit here.\nOnce you have backpack access, you can enable Github Copilot AI. https://github.com/settings/copilot\n\n\n\n\n\nThe last step is to configure VS Code for Python debugging. Provided everything else is in order, this should be straightforward.\n\nOpen Visual studio code. You will be asked to configure it.\n\nSet up GitHub Copilot. To do so, you’ll need to sign into GitHub by clicking “Continue with GitHub”. After you do, you may be dropped into a “Welcome” tab in VS Code. If so, click the “Walkthrough: Setup VS Code” tab to continue set up.\nPick a theme: Light, Dark, etc…\nAdd the following Extensions: Python, Jupyter.\nWindows users only: we’ll occasionally be using bash in the terminal mode of VSCode. Mac and Linux users have bash natively installed in the operating system. Windows users will have gotten a copy of bash when they downloaded and installed Git SCM. However, you need to tell VSCode to use bash in the Terminal rather than Windows PowerShell. To do that:\n\nClick “Unlock productivity with the Command Palette” in the Walthrough tab, then “Open Command Palette”. Alternatively, if you don’t see that option, open the Command Palette by typing CTRL + SHIFT + P. (Mac users type COMMAND + SHIFT + P).\nType “Select Default Profile” then hit ENTER.\nSelect “Git Bash” from the drop-down options.\nCheck that it worked by opening a new terminal: in the file menu, click “Terminal” -&gt; “New Terminal”. You should see “bash” in the upper right of the terminal screen.\n\nClose the welcome Tab but keep VS Code open.\n\n\n\n\n\n\n\n\nOpen a terminal in VS Code by clicking “Terminal” -&gt; “New Terminal”.\nIn the terminal window, type:\ncd\nThen hit ENTER.\nFollow the instructions Quickstart install instructions on the Miniconda website. For WSL, select “Linux”.\n\n\n\n\n\nDownload the Miniconda installer for Windows. Click the Downlad button under “Miniconda Installers”, not the one under “Distribution Installers”.\nRun the installer by double-clicking on the downloaded file and follow the steps below.\n\nClick “Run”.\nClick on “Next”.\nClick on “I agree”.\nSelect “Just me” and click on “Next”.\nClick “Next” to install to the default destination folder.\n\n\n\n\n\n\nWarning\n\n\n\nIf you had tried to install miniconda previously, you may get an error that says “Directory … is not empty, please choose a different location.” Do not choose a different location. Instead, click “OK”, then rename the previous miniconda3 directory. You can do that by going to VS Code, opening a new terminal window (click “Terminal” -&gt; “New Terminal”), and entering the following (hit ENTER after each line):\ncd\nmv miniconda3 miniconda3-old\nThen go back to the Miniconda installer and click “Next”.\n\n\nCheck the option for “Add Anaconda to my PATH environment variable” and check the option for “Register Anaconda as my default Python 3.x”.\n\nNote that even though the installation is for Miniconda, the installer uses the word Anaconda in these options.\nYou will also see a message in red text that selecting “Add Anaconda to my PATH environment variable” is not recommended; continue with this selection to make using conda easier in Git Bash. If you have questions or concerns, please contact your instructor.\n\nClick on “Install”.\nWhen the install is complete, Click on “Next”.\nClick on “Finish”.\n\nOpen a new terminal window in VS Code by clicking “Terminal” -&gt; “New Terminal”.\nType the following command to initialize conda:\nconda init bash\nOpen a new terminal window in VS Code by either hitting the “+” key next to “bash” in the upper left of the terminal window, or by cliking “Terminal” -&gt; “New Terminal”. If conda installed successfullly, you should see (base) appear next to or above your command prompt.\n\n\n\n\n\n\nIn the bash terminal window in VS Code (make sure it says (base) to the left of or above the prompt), type:\nconda create -n ist356 -c conda-forge -y python=3.11 ipykernel\nThen hit ENTER.\n\n\n\n\n\n\nWarning\n\n\n\nThe first time you run conda, you may bet an error about Terms of Service not being accepted for certain channels. Accept the TOS for those channels by running the commands listed. For example, if one of the channels is https://repo.anaconda.com/pkgs/main, copy and paste the command into the terminal:\nconda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\nFollowed by hitting ENTER (to execute a command in the terminal, you hit ENTER after typing/pasting it in). You’ll need to do this for each of the channels listed.\nOnce you have accepted the TOS for all the channels, run the conda create command above again.\n\n\nCheck to ensure your environment works:\n\nClick File -&gt; “New File…”\nIn the drop down menu, select “Jupyter Notebook”.\nClick “Select Kernel” on the right then type “ist356”.\nIn the first cell, type:\nprint(\"hello world!\")\nHit the run button on the left or type SHIFT + ENTER to execute the code. You should get back “hello world!”.\nClose the ipynb file (you may optionally save it; if you do, be sure to call it something other then “Untitled.ipynb”, maybe “hello.ipynb” instead).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "00_intro/index.html#step-1-software-installs",
    "href": "00_intro/index.html#step-1-software-installs",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "Here’s what you’ll need to install on your computer. Please read this entire section prior to installing. :-)\n\nVisual Studio Code: https://code.visualstudio.com/Download\nThis is a free editor with testing and debugging capabilities. As you install you can accept all defaults, except the last one: DO NOT launch the application when complete! If you do, simply close it.\nGit Source Code Manager: https://git-scm.com/download/\nAs you set it up, you will be asked several questions, for which the default selection is fine. EXCEPTION: choose Visual Studio Code as the default editor.\n\n\n\n\n\n\nWarning\n\n\n\nMac users: you’ll see multiple options for downloading Git Source Code Manager. I recommend using the Xcode version. Clicking on that link will redirect you to the Apple store; from there you will download and install Xcode.\n\n\nMiniconda Do not install this yet! You will install this using the terminal in VSCode after VSCode and Git are installed and setup; see instructions below.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "00_intro/index.html#setup-2-github-account",
    "href": "00_intro/index.html#setup-2-github-account",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "Being an SU student comes with some perks. One of them is a Github for Education account and accompanying “packpack” of goodies. To use this benefit, your github account must be associated with your @syr.edu email.\n\n\nAssociate your current account with SU, by adding your email:\n\nGo to to https://github.com and click Sign In\nOnce you have logged in, go to: https://github.com/settings/emails\n\nAdd and verify your SU email address.\n\n\n\n\nYou’ll need to create an account:\n\nGo to to https://github.com and click Sign In\nFollow the on-screen instructions to sign up for an account.\nMake sure to use your @syr.edu email for the account.\nAdd your personal email when you’re at it so you don’t lose Github access after you graduate!\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce you have a GitHub account, please email your GitHub username to me at cdcapano@syr.edu.\n\n\n\n\n\n\nGo to the Github Backpack site: https://education.github.com/pack\nand click Sign Up For Student Developer Pack\nOnce your account is verified, you will have backpack access.\nYou’ll know its active when you check your billing plan: https://github.com/settings/billing/summary\nyou should see a credit here.\nOnce you have backpack access, you can enable Github Copilot AI. https://github.com/settings/copilot",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "00_intro/index.html#step-3-configure-vs-code",
    "href": "00_intro/index.html#step-3-configure-vs-code",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "The last step is to configure VS Code for Python debugging. Provided everything else is in order, this should be straightforward.\n\nOpen Visual studio code. You will be asked to configure it.\n\nSet up GitHub Copilot. To do so, you’ll need to sign into GitHub by clicking “Continue with GitHub”. After you do, you may be dropped into a “Welcome” tab in VS Code. If so, click the “Walkthrough: Setup VS Code” tab to continue set up.\nPick a theme: Light, Dark, etc…\nAdd the following Extensions: Python, Jupyter.\nWindows users only: we’ll occasionally be using bash in the terminal mode of VSCode. Mac and Linux users have bash natively installed in the operating system. Windows users will have gotten a copy of bash when they downloaded and installed Git SCM. However, you need to tell VSCode to use bash in the Terminal rather than Windows PowerShell. To do that:\n\nClick “Unlock productivity with the Command Palette” in the Walthrough tab, then “Open Command Palette”. Alternatively, if you don’t see that option, open the Command Palette by typing CTRL + SHIFT + P. (Mac users type COMMAND + SHIFT + P).\nType “Select Default Profile” then hit ENTER.\nSelect “Git Bash” from the drop-down options.\nCheck that it worked by opening a new terminal: in the file menu, click “Terminal” -&gt; “New Terminal”. You should see “bash” in the upper right of the terminal screen.\n\nClose the welcome Tab but keep VS Code open.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "00_intro/index.html#step-4-install-miniconda",
    "href": "00_intro/index.html#step-4-install-miniconda",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "Open a terminal in VS Code by clicking “Terminal” -&gt; “New Terminal”.\nIn the terminal window, type:\ncd\nThen hit ENTER.\nFollow the instructions Quickstart install instructions on the Miniconda website. For WSL, select “Linux”.\n\n\n\n\n\nDownload the Miniconda installer for Windows. Click the Downlad button under “Miniconda Installers”, not the one under “Distribution Installers”.\nRun the installer by double-clicking on the downloaded file and follow the steps below.\n\nClick “Run”.\nClick on “Next”.\nClick on “I agree”.\nSelect “Just me” and click on “Next”.\nClick “Next” to install to the default destination folder.\n\n\n\n\n\n\nWarning\n\n\n\nIf you had tried to install miniconda previously, you may get an error that says “Directory … is not empty, please choose a different location.” Do not choose a different location. Instead, click “OK”, then rename the previous miniconda3 directory. You can do that by going to VS Code, opening a new terminal window (click “Terminal” -&gt; “New Terminal”), and entering the following (hit ENTER after each line):\ncd\nmv miniconda3 miniconda3-old\nThen go back to the Miniconda installer and click “Next”.\n\n\nCheck the option for “Add Anaconda to my PATH environment variable” and check the option for “Register Anaconda as my default Python 3.x”.\n\nNote that even though the installation is for Miniconda, the installer uses the word Anaconda in these options.\nYou will also see a message in red text that selecting “Add Anaconda to my PATH environment variable” is not recommended; continue with this selection to make using conda easier in Git Bash. If you have questions or concerns, please contact your instructor.\n\nClick on “Install”.\nWhen the install is complete, Click on “Next”.\nClick on “Finish”.\n\nOpen a new terminal window in VS Code by clicking “Terminal” -&gt; “New Terminal”.\nType the following command to initialize conda:\nconda init bash\nOpen a new terminal window in VS Code by either hitting the “+” key next to “bash” in the upper left of the terminal window, or by cliking “Terminal” -&gt; “New Terminal”. If conda installed successfullly, you should see (base) appear next to or above your command prompt.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "00_intro/index.html#step-5-create-a-conda-environment-and-test-it-in-vs-code",
    "href": "00_intro/index.html#step-5-create-a-conda-environment-and-test-it-in-vs-code",
    "title": "Introduction and computer setup",
    "section": "",
    "text": "In the bash terminal window in VS Code (make sure it says (base) to the left of or above the prompt), type:\nconda create -n ist356 -c conda-forge -y python=3.11 ipykernel\nThen hit ENTER.\n\n\n\n\n\n\nWarning\n\n\n\nThe first time you run conda, you may bet an error about Terms of Service not being accepted for certain channels. Accept the TOS for those channels by running the commands listed. For example, if one of the channels is https://repo.anaconda.com/pkgs/main, copy and paste the command into the terminal:\nconda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\nFollowed by hitting ENTER (to execute a command in the terminal, you hit ENTER after typing/pasting it in). You’ll need to do this for each of the channels listed.\nOnce you have accepted the TOS for all the channels, run the conda create command above again.\n\n\nCheck to ensure your environment works:\n\nClick File -&gt; “New File…”\nIn the drop down menu, select “Jupyter Notebook”.\nClick “Select Kernel” on the right then type “ist356”.\nIn the first cell, type:\nprint(\"hello world!\")\nHit the run button on the left or type SHIFT + ENTER to execute the code. You should get back “hello world!”.\nClose the ipynb file (you may optionally save it; if you do, be sure to call it something other then “Untitled.ipynb”, maybe “hello.ipynb” instead).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "Introduction and computer setup"
    ]
  },
  {
    "objectID": "01_cli_conda/index.html",
    "href": "01_cli_conda/index.html",
    "title": "1. Command line interface and Environments",
    "section": "",
    "text": "In this unit, you will learn how to use the bash shell in a command line interface (CLI), programming environments, and how to use conda to manage environments and install packages.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments"
    ]
  },
  {
    "objectID": "01_cli_conda/index.html#tutorials",
    "href": "01_cli_conda/index.html#tutorials",
    "title": "1. Command line interface and Environments",
    "section": "Tutorials",
    "text": "Tutorials\n\nIntroduction to CLI\nEnvironments and Conda",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments"
    ]
  },
  {
    "objectID": "01_cli_conda/index.html#lecture",
    "href": "01_cli_conda/index.html#lecture",
    "title": "1. Command line interface and Environments",
    "section": "Lecture",
    "text": "Lecture\n\nWednesday, 8/28/2025",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "1. Command line interface and Environments"
    ]
  },
  {
    "objectID": "02_python/index.html",
    "href": "02_python/index.html",
    "title": "2. Python review",
    "section": "",
    "text": "In this unit we will review some basics of Python programming. You will have covered the material here in IST 256.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review"
    ]
  },
  {
    "objectID": "02_python/index.html#tutorials",
    "href": "02_python/index.html#tutorials",
    "title": "2. Python review",
    "section": "Tutorials",
    "text": "Tutorials\n\nInput, output, variables, types, conditionals\nIterations, lists, dictionaries, comprehensions\nFunctions, documentation, strings, files\nModules, import, pip, testing",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review"
    ]
  },
  {
    "objectID": "02_python/index.html#lectures",
    "href": "02_python/index.html#lectures",
    "title": "2. Python review",
    "section": "Lectures",
    "text": "Lectures\n\nWednesday, 9/3/2025\nInstructions on how to use GitHub Classroom; start of Python review.\n\n\n\n\nMonday, 9/8/2025\nContinuation of Python review: conditionals, loops, lists, and dictionaries (2 parts)\n\n\n\n\n\n\nWednesday, 9/10/2025\nContinuation of Python review: functions, strings, and files\n\n\n\n\nMonday, 9/15/2025\nFinishing Python review: modules and testing",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review"
    ]
  },
  {
    "objectID": "02_python/python-2.html",
    "href": "02_python/python-2.html",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "",
    "text": "Iterations are code strucures which allow us to repeat sections of code while a condition is true, or a fixed number of time.\nThey allow us to do more with less code!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-2.html#the-break-command",
    "href": "02_python/python-2.html#the-break-command",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "The break command",
    "text": "The break command\n\nbreak keyword exits the loop immediately.\nCommonly used when there is no longer a reason to loop (you achieved your goal).\nYou can add an else to the for loop to execute when break does not happen.\n\n\n# Example: find a letter in a text string\n\ntext = input(\"Enter Some Text:\")\nfind = input(\"Enter character to find:\")\nfor ch in text:\n    if ch == find:\n        print(f\"Found {find} in {text}!\")\n        break\nelse:\n    print(f\"Unable to find {find} in {text}!\")\n\nEnter Some Text: testing\nEnter character to find: n\nFound n in testing!\n\nEnter Some Text: testing\nEnter character to find: x\nUnable to find x in testing!\n\n\n\n\n\n\n\nCautionCode Challenge 2.1\n\n\n\nWrite a program to accept a password as input. If the password input is “secret” display “access granted”. Otherwise say “invalid password”.\nRepeat the above up to 5 times. When the correct password is entered, stop looping. When 5 loops have exhausted, print “you are locked out”.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nvalid_password = \"secret\"\nfor i in range(5):\n    pw = input(\"Enter Password:\")\n    if pw == valid_password:\n        print(\"Access Granted!\")\n        break\n    else:\n        print(\"Invalid Password.\")\nelse:\n    print(\"You are locked out\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-2.html#the-in-operator",
    "href": "02_python/python-2.html#the-in-operator",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "The in operator",
    "text": "The in operator\nThe in operator checks for existence of an item in a list.\n\n# Example: whats in the list?\nnumbers = [10, 15, 20]\nprint(f\"5 in {numbers}?\", 5 in numbers)\nprint(f\"20 in {numbers}?\", 20 in numbers)\n\n5 in [10, 15, 20]? False\n20 in [10, 15, 20]? True",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-2.html#list-methods",
    "href": "02_python/python-2.html#list-methods",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "List Methods",
    "text": "List Methods\nThere are numerous list methods which allow you to add, remove, and find values in the list, etc…\nhttps://docs.python.org/3/library/stdtypes.html?highlight=list#mutable-sequence-types\n\n# Example: manipulating a list\n\n# An empty list\ncolors = []\n\n# Add \"blue\" to the end\ncolors.append(\"blue\")\n\n# add \"red\" to the beginning\ncolors.insert(0, \"red\")\n\n# add \"white\" in the 2nd position\ncolors.insert(1, \"white\")\n\n# print ['red', 'white', 'blue']\nprint(colors)\n\n# remove the last color\nblue = colors.pop(-1)\n\n# remove \"white\"\nwhite = colors.remove(\"white\")\n\n# print ['red']\nprint(colors)\n\n['red', 'white', 'blue']\n['red']\n\n\n\n\n\n\n\n\nCautionCode Challenge 2.3\n\n\n\nWrite a sentinel controlled loop to input a color until “quit” is entered. Add each color to a list only when the color is not already in the list. Print the list each time in the loop.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\ncolors = []\n\nwhile True:\n    color = input(\"Enter a color:\")\n    if color == 'quit':\n        break\n    if color not in colors:\n        colors.append(color)\n        op = \"added to\"\n    else:\n        op = \"already in\"\n\n    print(f\"{color} {op} {colors}\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-2.html#dictionary-methods",
    "href": "02_python/python-2.html#dictionary-methods",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nLike str and list, the dict type has its own set of built-in functions. https://docs.python.org/3/library/stdtypes.html#mapping-types-dict\n\nfont = {'name': 'Arial','size': 8}\n\nprint(font.keys()) # this is an iterable\n\nprint(font.values()) # this is an iterable\n\nprint(font['name'])\n\nfont['name'] = 'Courier'\n\nprint(font['name'])\n\nprint('size' in font)\n\nprint(font.get('style', 'normal')) # get with default value\n\ndict_keys(['name', 'size'])\ndict_values(['Arial', 8])\nArial\nCourier\nTrue\nnormal",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-2.html#complex-data-stuctures",
    "href": "02_python/python-2.html#complex-data-stuctures",
    "title": "2. Iterations, lists, dictionaries, comprehensions",
    "section": "Complex Data Stuctures",
    "text": "Complex Data Stuctures\nWe can combine lists and dictionaries to create complex data structures in python.\nThese allow us to represent real-world data in code\n\nstudents = [\n    { 'name' : 'abby', 'grades' : [100,80,90] },\n    { 'name' : 'bob', 'grades' : [100,90,90] },\n    { 'name' : 'chris', 'grades' : [90,100,100] }\n]\n\n\n# just print each student name\nfor student in students:\n    print(student['name'])\n\nabby\nbob\nchris\n\n\n\n# print each student name and average grade\nfor student in students:\n    avg_grade = sum(student['grades'])/len(student['grades'])\n    print(f\"{student['name']}  {avg_grade:.2f}\")\n\nabby  90.00\nbob  93.33\nchris  96.67\n\n\n\n\n\n\n\n\nCautionCode Challenge 2.4\n\n\n\nWrite a program to create a shopping list:\n\nloop until “quit” is entered\ninput a grocery item\ninput a quantity\nsave the item as the key in the dictionary and quantity as the value\nif the item is in the dictionary already, add the quantity to the existing value\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nitems = {}\n\nwhile True:\n    item = input(\"Enter shopping item, or 'quit': \")\n    if item == 'quit':\n        break\n    qty = int(input(\"Enter quantity: \"))\n    if item in items.keys():\n        items[item] = items[item] + qty\n    else:\n        items[item] = qty\n\n    print(\"ITEMS:\", items)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "2. Iterations, lists, dictionaries, comprehensions"
    ]
  },
  {
    "objectID": "02_python/python-4.html",
    "href": "02_python/python-4.html",
    "title": "4. Modules, import, pip, testing",
    "section": "",
    "text": "A Module is a file containing Python code.\nThe Code in the module can be included in your code using the import command\nA collection of modules bundled for re-distribution is known as a Package\n\n\n\n\nThe Python language has several modules which are included with the base language: Python Standard Library https://docs.python.org/3/library/\nIn addition you can import other libraries found on the Internet.\nThe Python Package Index is a website which allows you to search for other code avaialbe for use. https://pypi.org/\nOnce you know which package you want, you can install it with the pip command from the terminal.\nExample: pip install &lt;name-of-package&gt; (Make sure you activate your conda environment first!)\nYou can also install packages using conda. For example, conda install -c conda-forge &lt;name-of-package&gt;.\nWhich to use? Basically, conda and pip can be used interchangeably within a conda environment. Some packages may be in PyPI that are not in conda-forge and vice versa.\n\n\n\n\n\nrequirements.txt is a file which stores the names of all the packages a project uses\nadding them to the file is a replacement for installing each of them manually\nto install the packages: pip install -r requirements.txt\n\n\n\n\nCode in the module can be included in your code using the import command. There are a few different ways to import some code:\n\nimport foo imports code from module foo. Functions in that module can then be referenced in the current code by prefixing the function with foo.. For example, if foo has a function bar defined in it, then after the import foo statement you can use the bar function by writing foo.bar(...) (where the ... indicate the arguments you need to pass to bar).\nfrom foo import bar,baz only imports the bar and baz functions from module foo. When imported in this manner, the bar and baz functions are referenced directly, without the module prefix. For example, in this case, to execute the bar function, you run bar(...) instead of foo.bar(...).\n\n\n\n\n\n\n\nNoteConcept check\n\n\n\nWhat’s wrong with the following code?\n\nexp = 'Dear {name}, we would like to express our gratitude for your donation!'\n\nfrom math import exp\n\nprint(exp.format(name=\"Jeff\"))\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe from math import exp line will cause the exp variable in the namespace that’s defined in the first line to be replaced by the exponential function in the math module. This will cause an error in the last line, which expects exp to be a string, not a function.\n\n\n\n\n\n\nRenaming imports: You can avoid namespace collisions by renaming an imported module or functions via: import foo as f. For example, in the Concept check above, doing from math import exp as mathexp would avoid the error.\n\n\n\n\n\nUse the dir(&lt;module&gt;) to list the functions in the module\nUse help(module.function) to get the docstring for a function in a module.\n\n\n# Examples\nimport math\n\n\n# get the functions\ndir(math)\n\n['__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'acos',\n 'acosh',\n 'asin',\n 'asinh',\n 'atan',\n 'atan2',\n 'atanh',\n 'cbrt',\n 'ceil',\n 'comb',\n 'copysign',\n 'cos',\n 'cosh',\n 'degrees',\n 'dist',\n 'e',\n 'erf',\n 'erfc',\n 'exp',\n 'exp2',\n 'expm1',\n 'fabs',\n 'factorial',\n 'floor',\n 'fma',\n 'fmod',\n 'frexp',\n 'fsum',\n 'gamma',\n 'gcd',\n 'hypot',\n 'inf',\n 'isclose',\n 'isfinite',\n 'isinf',\n 'isnan',\n 'isqrt',\n 'lcm',\n 'ldexp',\n 'lgamma',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'modf',\n 'nan',\n 'nextafter',\n 'perm',\n 'pi',\n 'pow',\n 'prod',\n 'radians',\n 'remainder',\n 'sin',\n 'sinh',\n 'sqrt',\n 'sumprod',\n 'tan',\n 'tanh',\n 'tau',\n 'trunc',\n 'ulp']\n\n\n\nhelp(math.pow)\n\nHelp on built-in function pow in module math:\n\npow(x, y, /)\n    Return x**y (x to the power of y).\n\n\n\n\nmath.pow(2,5) # 2*2*2*2*2\n\n32.0\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.1\n\n\n\nUse the datetime module to parse a “Month/Day/Year” string (e.g., “9/15/2025”) into a datetime then print it in YYYY-MM-DD format (e.g., “2025-09-15”).\nYou will need to read through the module with dir() and help() or read the python docs to determine which functions to use.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom datetime import datetime\n\ntext = input(\"Enter date m/d/y: \")\nnow = datetime.strptime(text, \"%m/%d/%Y\")\nnowstr = now.strftime(\"%Y-%m-%d\")\nprint(nowstr)\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.2\n\n\n\nLet’s make the code in Challenge 4.2 more resusable:\n\nRe-write the date parse into a function parsedate_mdy(text: str) -&gt; datetime:.\nRe-write the date format into a function formatdate_ymd(date: datetime) -&gt; str:.\nRe-write the main program to use both functions. input -&gt; parsedate -&gt; formatdate -&gt; output\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom datetime import datetime\n\ndef parsedate_mdy(text: str) -&gt; datetime:\n    dt = datetime.strptime(text, \"%m/%d/%Y\")\n    return dt\n\n\ndef formatdate_ymd(date: datetime) -&gt; str:\n    return date.strftime(\"%Y-%m-%d\")\n\n\ntext = input(\"Enter date m/d/y: \")\ndate = parsedate_mdy(text)\ndate_str = formatdate_ymd(date)\nprint(date_str)\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.3\n\n\n\nLet’s make the code in Challenge 4.2 even more resusable!\n\nMove your functions into a module name dateutils.py.\nImport your functions from dateutils.py into 4-3.py\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nCreate the dateutils.py file in VS Code by clicking “File” -&gt; “New File”. Name it dateutils.py. Then copy the functions you wrote in Challenge 4.2 into it (don’t forget the from datetime import datetime at the top!). Save the file by typing CTRL/CMD + S or clicking File -&gt; Save.\nTo use your functions in a notebook or another python script:\n\nfrom dateutils import parsedate_mdy, formatdate_ymd\n\ntext = input(\"Enter date m/d/y: \")\ndate = parsedate_mdy(text)\ndate_str = formatdate_ymd(date)\nprint(date_str)\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantAvoid wildcard imports!\n\n\n\nIt’s possible to use a wildcard * when importing. For example,\n\nfrom math import *\n\nwill import all functions that are in the math module into the current namespace. This means that you could then call math.pow with just pow, for example.\nDon’t do this! The problem with this is you can accidentally override variables in your program without realizing it. For example, if you defined a variable called e then did from math import *, e would be replaced with the value of the natural exponent, since e is defined in the math module. Doing wildcard imports also makes it difficult to debug code, since it’s a challenge to determine what module a particular variable came from. Wildcard imports should only be used in very limited situations, which you are unlikely to run in to. Long story short… don’t use wildcard imports! I only tell you about it in case you see it in the wild (no pun intended).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#built-in-modules-vs.-external",
    "href": "02_python/python-4.html#built-in-modules-vs.-external",
    "title": "4. Modules, import, pip, testing",
    "section": "",
    "text": "The Python language has several modules which are included with the base language: Python Standard Library https://docs.python.org/3/library/\nIn addition you can import other libraries found on the Internet.\nThe Python Package Index is a website which allows you to search for other code avaialbe for use. https://pypi.org/\nOnce you know which package you want, you can install it with the pip command from the terminal.\nExample: pip install &lt;name-of-package&gt; (Make sure you activate your conda environment first!)\nYou can also install packages using conda. For example, conda install -c conda-forge &lt;name-of-package&gt;.\nWhich to use? Basically, conda and pip can be used interchangeably within a conda environment. Some packages may be in PyPI that are not in conda-forge and vice versa.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#requirements.txt",
    "href": "02_python/python-4.html#requirements.txt",
    "title": "4. Modules, import, pip, testing",
    "section": "",
    "text": "requirements.txt is a file which stores the names of all the packages a project uses\nadding them to the file is a replacement for installing each of them manually\nto install the packages: pip install -r requirements.txt",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#importing-modules",
    "href": "02_python/python-4.html#importing-modules",
    "title": "4. Modules, import, pip, testing",
    "section": "",
    "text": "Code in the module can be included in your code using the import command. There are a few different ways to import some code:\n\nimport foo imports code from module foo. Functions in that module can then be referenced in the current code by prefixing the function with foo.. For example, if foo has a function bar defined in it, then after the import foo statement you can use the bar function by writing foo.bar(...) (where the ... indicate the arguments you need to pass to bar).\nfrom foo import bar,baz only imports the bar and baz functions from module foo. When imported in this manner, the bar and baz functions are referenced directly, without the module prefix. For example, in this case, to execute the bar function, you run bar(...) instead of foo.bar(...).\n\n\n\n\n\n\n\nNoteConcept check\n\n\n\nWhat’s wrong with the following code?\n\nexp = 'Dear {name}, we would like to express our gratitude for your donation!'\n\nfrom math import exp\n\nprint(exp.format(name=\"Jeff\"))\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe from math import exp line will cause the exp variable in the namespace that’s defined in the first line to be replaced by the exponential function in the math module. This will cause an error in the last line, which expects exp to be a string, not a function.\n\n\n\n\n\n\nRenaming imports: You can avoid namespace collisions by renaming an imported module or functions via: import foo as f. For example, in the Concept check above, doing from math import exp as mathexp would avoid the error.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#whats-in-the-module",
    "href": "02_python/python-4.html#whats-in-the-module",
    "title": "4. Modules, import, pip, testing",
    "section": "",
    "text": "Use the dir(&lt;module&gt;) to list the functions in the module\nUse help(module.function) to get the docstring for a function in a module.\n\n\n# Examples\nimport math\n\n\n# get the functions\ndir(math)\n\n['__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'acos',\n 'acosh',\n 'asin',\n 'asinh',\n 'atan',\n 'atan2',\n 'atanh',\n 'cbrt',\n 'ceil',\n 'comb',\n 'copysign',\n 'cos',\n 'cosh',\n 'degrees',\n 'dist',\n 'e',\n 'erf',\n 'erfc',\n 'exp',\n 'exp2',\n 'expm1',\n 'fabs',\n 'factorial',\n 'floor',\n 'fma',\n 'fmod',\n 'frexp',\n 'fsum',\n 'gamma',\n 'gcd',\n 'hypot',\n 'inf',\n 'isclose',\n 'isfinite',\n 'isinf',\n 'isnan',\n 'isqrt',\n 'lcm',\n 'ldexp',\n 'lgamma',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'modf',\n 'nan',\n 'nextafter',\n 'perm',\n 'pi',\n 'pow',\n 'prod',\n 'radians',\n 'remainder',\n 'sin',\n 'sinh',\n 'sqrt',\n 'sumprod',\n 'tan',\n 'tanh',\n 'tau',\n 'trunc',\n 'ulp']\n\n\n\nhelp(math.pow)\n\nHelp on built-in function pow in module math:\n\npow(x, y, /)\n    Return x**y (x to the power of y).\n\n\n\n\nmath.pow(2,5) # 2*2*2*2*2\n\n32.0\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.1\n\n\n\nUse the datetime module to parse a “Month/Day/Year” string (e.g., “9/15/2025”) into a datetime then print it in YYYY-MM-DD format (e.g., “2025-09-15”).\nYou will need to read through the module with dir() and help() or read the python docs to determine which functions to use.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom datetime import datetime\n\ntext = input(\"Enter date m/d/y: \")\nnow = datetime.strptime(text, \"%m/%d/%Y\")\nnowstr = now.strftime(\"%Y-%m-%d\")\nprint(nowstr)\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.2\n\n\n\nLet’s make the code in Challenge 4.2 more resusable:\n\nRe-write the date parse into a function parsedate_mdy(text: str) -&gt; datetime:.\nRe-write the date format into a function formatdate_ymd(date: datetime) -&gt; str:.\nRe-write the main program to use both functions. input -&gt; parsedate -&gt; formatdate -&gt; output\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom datetime import datetime\n\ndef parsedate_mdy(text: str) -&gt; datetime:\n    dt = datetime.strptime(text, \"%m/%d/%Y\")\n    return dt\n\n\ndef formatdate_ymd(date: datetime) -&gt; str:\n    return date.strftime(\"%Y-%m-%d\")\n\n\ntext = input(\"Enter date m/d/y: \")\ndate = parsedate_mdy(text)\ndate_str = formatdate_ymd(date)\nprint(date_str)\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.3\n\n\n\nLet’s make the code in Challenge 4.2 even more resusable!\n\nMove your functions into a module name dateutils.py.\nImport your functions from dateutils.py into 4-3.py\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nCreate the dateutils.py file in VS Code by clicking “File” -&gt; “New File”. Name it dateutils.py. Then copy the functions you wrote in Challenge 4.2 into it (don’t forget the from datetime import datetime at the top!). Save the file by typing CTRL/CMD + S or clicking File -&gt; Save.\nTo use your functions in a notebook or another python script:\n\nfrom dateutils import parsedate_mdy, formatdate_ymd\n\ntext = input(\"Enter date m/d/y: \")\ndate = parsedate_mdy(text)\ndate_str = formatdate_ymd(date)\nprint(date_str)\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantAvoid wildcard imports!\n\n\n\nIt’s possible to use a wildcard * when importing. For example,\n\nfrom math import *\n\nwill import all functions that are in the math module into the current namespace. This means that you could then call math.pow with just pow, for example.\nDon’t do this! The problem with this is you can accidentally override variables in your program without realizing it. For example, if you defined a variable called e then did from math import *, e would be replaced with the value of the natural exponent, since e is defined in the math module. Doing wildcard imports also makes it difficult to debug code, since it’s a challenge to determine what module a particular variable came from. Wildcard imports should only be used in very limited situations, which you are unlikely to run in to. Long story short… don’t use wildcard imports! I only tell you about it in case you see it in the wild (no pun intended).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#assert",
    "href": "02_python/python-4.html#assert",
    "title": "4. Modules, import, pip, testing",
    "section": "Assert",
    "text": "Assert\nassert is a python command which throws an exception if the expression asserted is false.\nWhen an assert fails, it raises an AssertionError exception which alerts us that something did not go as planned.\n\nassert 1 + 1 == 2   # This is true, no worries\n\n\nassert 2 + 2 == 5 # False AssertionError\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 assert 2 + 2 == 5 # False AssertionError\n\nAssertionError:",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "02_python/python-4.html#pytest",
    "href": "02_python/python-4.html#pytest",
    "title": "4. Modules, import, pip, testing",
    "section": "Pytest",
    "text": "Pytest\nPytest is testing framework for Python. It can automatically discover tests in your code; for example, any function that starts with test is executed when you invoke pytest. (See here for the full list of rules that pytest uses to discover test functions.)\nYou can invoke it at the terminal like this:\npython -m pytest &lt;filetotest&gt;\n\n\n\n\n\n\nNote\n\n\n\nPytest is not installed by default when you create a new conda environment. If you have not installed pytest into your ist356 environment yet, you can do so by opening a terminal and running:\nconda activate ist356\npip install pytest\n\n\nThe VS Code test plugin should discover the tests.\n\n\n\n\n\n\nCautionCode Challenge 4.4\n\n\n\nWrite a “round robin” test that tests the parsedate_mdy and formatdate_ymd functions you wrote in Code Challenge 4.3. The test should start with a known input, run the two functions on it, then check if it yields the known output.\nRun pytest or VS Code test to make sure your tests pass!\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nAdd the following to dateutils.py:\n\n\n#! eval: false\ndef test_round_robin():\n    date_str = \"9/15/2025\"\n    date_obj = parsedate_mdy(date_str)\n    assert formatdate_ymd(date_obj) == \"2025-09-15\"\n\n\nTo run the test, open a terminal (in VS Code, select Terminal -&gt; New Terminal). If your ist356 conda environment is not active, activate it by running: conda activate ist356. Now cd into the directory that dateutils.py is in (you’re probably already there; run ls to check), then run:\n\npython -m pytest dateutils.py\nNote: if you not have installed pytest in your environment yet you will get an error No module named pytest. In that case, install pytest by either running pip install pytest or conda install -c conda-forge -y pytest.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "2. Python review",
      "4. Modules, import, pip, testing"
    ]
  },
  {
    "objectID": "03_ui/ui-1.html",
    "href": "03_ui/ui-1.html",
    "title": "1. Interaction in Jupyter using ipywidgets",
    "section": "",
    "text": "Note\n\n\n\nTo do this tutorial you will need ipywidgets installed in your conda environment. To do that:\n\nOpen a terminal (from in VS Code, click “Terminal” -&gt; “New Terminal”).\nActivate your ist356 conda environment by running:\nconda activate ist356\nInstall ipywidgets by either running:\npip install ipywidgets\nor:\nconda install -c conda-forge -y ipywidgets",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "1. Interaction in Jupyter using ipywidgets"
    ]
  },
  {
    "objectID": "03_ui/ui-1.html#notebook-widgets-ipywidgets",
    "href": "03_ui/ui-1.html#notebook-widgets-ipywidgets",
    "title": "1. Interaction in Jupyter using ipywidgets",
    "section": "Notebook widgets: ipywidgets",
    "text": "Notebook widgets: ipywidgets\nThe jupyter notebook widgets create better UI interactions in notebooks. This is called the ipywidgets library. There is a lot to this library but we will keep our interactions simple.\nTo replace input() statements we use the interact_manual decorator function. Like a hat decorates your head, decorator function adds code to another function.\ninteract_manual decorator does the following:\n\ngenerates a textbox for any string input\ngenerates a slider for any int/float input\ngenerates a dropdown for any list input\ngenerates a button titled “Run interact”\n\nWhen the button is clicked the code inside the decorated function is executed and the widget values are used as input. Use display() instead of print() for output.\n\n# Necessary imports to make this work\nfrom IPython.display import display\nfrom ipywidgets import interact_manual\n\n\n# Example:\nvals = [ 'red', 'white', 'blue'] # this is a list type, it will generate a dropdown widget\nmin, max, step = 0, 20, 0.5      # this is the range of the slider, and the steps\ntext = \"testing\"                 # this is a string type, it will generate a textbox\n\n@interact_manual(color=vals, grade=(min,max,step), name=text) # DECORATOR function with values\ndef on_click(color, grade, name):                             # DECORATED function. This code \n    display(color)                                            # runs when the button is clicked \n    display(grade)                                            # (thus the name on_click)\n    display(name)\n\n\n\n\n\n\n\nNote\n\n\n\nFor more complex interactions we will use the streamlit library\n\n\n\n\n\n\n\n\nCautionCode Challenge 1.1\n\n\n\nCreate a simple widget interaction that will display student status for their GPA. The widget should take as inputs:\n\nthe student’s name;\ntheir major: one of “IMT”, “IST”, or “ADA”;\na gpa between 0.0 and 4.0.\n\nThe widget should process:\n\nwhen gpa &lt; 1.8 then status is “probation”\nwhen gpa &gt; 3.4 then status is “deans list”\nelse status is “no list”.\n\nThe widget should then display the following statement:\n\n\"NAME in MAJOR with GPA is on STATUS\"\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom IPython.display import display\nfrom ipywidgets import interact_manual\n\n\n@interact_manual(name=\"\", major=[\"IMT\", \"IST\", \"ADA\"], gpa=(0.0,4.0,0.05))\ndef onclick(name,major,gpa):\n    if gpa &lt; 1.8:\n        status = \"probation\"\n    elif gpa &gt; 3.4:\n        status = \"deans list\"\n    else:\n        status = \"no list\"\n    display(f\"{name} in {major} with gpa of {gpa} is on {status}.\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "3. UI",
      "1. Interaction in Jupyter using ipywidgets"
    ]
  },
  {
    "objectID": "04_data_wrangling/index.html",
    "href": "04_data_wrangling/index.html",
    "title": "4. Data Wrangling",
    "section": "",
    "text": "In this unit we will learn about how to easily manage and manipulate data in Python using NumPy and Pandas.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling/index.html#tutorials",
    "href": "04_data_wrangling/index.html#tutorials",
    "title": "4. Data Wrangling",
    "section": "Tutorials",
    "text": "Tutorials\n\nIntroduction to NumPy\nIntroduction to Pandas: Series and DataFrames\nData I/O with Pandas\nJoining Multiple Pandas Dataframes\nBasic data cleaning with Pandas\nGrouping data and creating pivot tables with Pandas",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling/index.html#lectures",
    "href": "04_data_wrangling/index.html#lectures",
    "title": "4. Data Wrangling",
    "section": "Lectures",
    "text": "Lectures\n\nMonday, 9/22/2025\nInstructions on how to use Git on the command line. Introduction to NumPy.\n\n\n\n\nWednesday, 9/24/2025\nIntroduction to Pandas: Series and DataFrames.\n\n\n\n\nMonday, 9/29/2025\nData I/O with Pandas\n\n\n\n\nWednesday, 10/1/2025\nJoining mulitple DataFrames\n\n\n\n\nMonday, 10/6/2025\nUsing apply to clean data in a Pandas DataFrame\n\n\n\n\nWednesday, 10/8/2025\nGrouping data, merging tables, and creating pivot tables",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html",
    "href": "04_data_wrangling/pandas-1.html",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "",
    "text": "Pandas is a Python library for working with tabular data. Pandas is short for PANeled DAta.\nPandas is like a programmable spreadheet. It is used by programmers to wrangle data (sort, filter, clean, enhance, etc).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html#what-is-pandas",
    "href": "04_data_wrangling/pandas-1.html#what-is-pandas",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "",
    "text": "Pandas is a Python library for working with tabular data. Pandas is short for PANeled DAta.\nPandas is like a programmable spreadheet. It is used by programmers to wrangle data (sort, filter, clean, enhance, etc).",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html#pandas-series-and-dataframe",
    "href": "04_data_wrangling/pandas-1.html#pandas-series-and-dataframe",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "Pandas Series and DataFrame",
    "text": "Pandas Series and DataFrame\nThe two fundamental compoents of Pandas are the Series and DataFrame\n\na Series is a list of values with labels. This creates a column of data\na DataFrame is a collection of series. This creates a table of data\n\n\nNull / No Value\nThe constant np.nan is used to represent “no value”\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html#series",
    "href": "04_data_wrangling/pandas-1.html#series",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "Series",
    "text": "Series\nA Series is a named list of values.\nThe series has an index, too to reference each value. The default index is a zero based, similar to a python list.\n\ngrades = pd.Series(data=[100,80,90,np.nan,100], name=\"Midterm Grades\")\ngrades\n\n0    100.0\n1     80.0\n2     90.0\n3      NaN\n4    100.0\nName: Midterm Grades, dtype: float64\n\n\n\n# The the value at index 2\ngrades[2]\n\nnp.float64(90.0)\n\n\nThe index can be anyting . Here’s the same grades with student names as the index.\n\ngrades2 = pd.Series( data=[100,80,90,np.nan,100], \n                    name=\"Midterm Grades\",\n                    index=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"])\ngrades2\n\nAlice      100.0\nBob         80.0\nCharlie     90.0\nDavid        NaN\nEve        100.0\nName: Midterm Grades, dtype: float64\n\n\n\n# Get Charlie's grade\ngrades2[\"Charlie\"]\n\nnp.float64(90.0)\n\n\n\nSeries Vectorized Functions\nLike NumPy arrays, you can perform element-wise mathematical operations on Pandas series without needing for loops (i.e., vectorization). For example:\n\n# add 5 points to all the grades\ngrades3 = grades2 + 5\nprint(grades3)\n\nAlice      105.0\nBob         85.0\nCharlie     95.0\nDavid        NaN\nEve        105.0\nName: Midterm Grades, dtype: float64\n\n\n\n# square the grades\ngradesq = grades2**2\nprint(gradesq)\n\nAlice      10000.0\nBob         6400.0\nCharlie     8100.0\nDavid          NaN\nEve        10000.0\nName: Midterm Grades, dtype: float64\n\n\n\n# add two series together\ngrades4 = grades2 + grades3\nprint(grades4)\n\nAlice      205.0\nBob        165.0\nCharlie    185.0\nDavid        NaN\nEve        205.0\nName: Midterm Grades, dtype: float64\n\n\nPandas series also have a number of vectorized methods that you can call on the series themselves, again like NumPy arrays. Some examples:\n\nprint(\"Highest grade:\", grades.max())\nprint(\"Average grade:\", grades.mean())\nprint(\"lowest grade:\", grades.min())\nprint(\"Sum of grades:\", grades.sum())\nprint(\"Count of grades\", grades.count())\n\nHighest grade: 100.0\nAverage grade: 92.5\nlowest grade: 80.0\nSum of grades: 370.0\nCount of grades 4\n\n\n\n\nOther Series Functions\nWe can use the unique() method function to return only the non-duplicate values from the series.\nThe value_counts() method function adds up values, creating a new series where the index is the value and the value is the count.\nFor example consider the following series:\n\nvotes = pd.Series(data=[ 'y','y','y','n','y',np.nan,'n','n','y'], name=\"Vote\")\nprint(\"deduplicate the votes:\", votes.unique())\nprint(\"counts by value:\", votes.value_counts())\n\ndeduplicate the votes: ['y' 'n' nan]\ncounts by value: Vote\ny    5\nn    3\nName: count, dtype: int64\n\n\n\n\nComparison to NumPy\nIn many ways, you can think of a Pandas series as being like a NumPy array (in fact, series are built on top of NumPy arrays). It even has similar performance. For example:\n\na = np.arange(1000000)\naseries = pd.Series(a)\n\n\n%timeit a.mean()\n\n537 μs ± 807 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit aseries.mean()\n\n549 μs ± 1.05 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nHowever, unlike NumPy arrays, Pandas series can only be one dimensional. Example:\n\n# 2D NumPy array? No problem!\na = np.ones((1000, 1000))\nprint(a.shape)\n\n(1000, 1000)\n\n\n\n# 2D Pandas series? Nope!\naseries = pd.Series(a)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[15], line 2\n      1 # 2D Pandas series? Nope!\n----&gt; 2 aseries = pd.Series(a)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/series.py:584, in Series.__init__(self, data, index, dtype, name, copy, fastpath)\n    582         data = data.copy()\n    583 else:\n--&gt; 584     data = sanitize_array(data, index, dtype, copy)\n    586     manager = _get_option(\"mode.data_manager\", silent=True)\n    587     if manager == \"block\":\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/construction.py:656, in sanitize_array(data, index, dtype, copy, allow_2d)\n    653             subarr = cast(np.ndarray, subarr)\n    654             subarr = maybe_infer_to_datetimelike(subarr)\n--&gt; 656 subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\n    658 if isinstance(subarr, np.ndarray):\n    659     # at this point we should have dtype be None or subarr.dtype == dtype\n    660     dtype = cast(np.dtype, dtype)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/construction.py:715, in _sanitize_ndim(result, data, dtype, index, allow_2d)\n    713     if allow_2d:\n    714         return result\n--&gt; 715     raise ValueError(\n    716         f\"Data must be 1-dimensional, got ndarray of shape {data.shape} instead\"\n    717     )\n    718 if is_object_dtype(dtype) and isinstance(dtype, ExtensionDtype):\n    719     # i.e. NumpyEADtype(\"O\")\n    721     result = com.asarray_tuplesafe(data, dtype=np.dtype(\"object\"))\n\nValueError: Data must be 1-dimensional, got ndarray of shape (1000, 1000) instead",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html#dataframe",
    "href": "04_data_wrangling/pandas-1.html#dataframe",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "DataFrame",
    "text": "DataFrame\nFor 2D data, you use a Pandas DataFrame. A DataFrame is a table representation of data. It is the primary use case for pandas itself. A DataFrame is simply a collection of Series that share a common index. It’s like a programmable spreadsheet: it has rows and columns which can be accessed and manipulated with Python.\nAn example:\n\nnames = pd.Series( data = ['Allen','Bob','Chris','Dave','Ed','Frank','Gus'])\ngpas = pd.Series( data = [4.0, np.nan, 3.4, 2.8, 2.5, 3.8, 3.0])\nyears = pd.Series( data = ['So', 'Fr', 'Fr', 'Jr', 'Sr', 'Sr', 'Fr'])\nseries_dict = { 'Name':  names, 'GPA': gpas, 'Year' : years }  # dict of Series, keys are the series names\nstudents = pd.DataFrame( series_dict )\nstudents\n\n\n\n\n\n\n\n\nName\nGPA\nYear\n\n\n\n\n0\nAllen\n4.0\nSo\n\n\n1\nBob\nNaN\nFr\n\n\n2\nChris\n3.4\nFr\n\n\n3\nDave\n2.8\nJr\n\n\n4\nEd\n2.5\nSr\n\n\n5\nFrank\n3.8\nSr\n\n\n6\nGus\n3.0\nFr\n\n\n\n\n\n\n\n\nOther Ways to create dataframes:\n\n# Lists of lists\npd.DataFrame([['Tom', 7], ['Mike', 15], ['Tiffany', 3]])\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTom\n7\n\n\n1\nMike\n15\n\n\n2\nTiffany\n3\n\n\n\n\n\n\n\n\n# Dictionary\npd.DataFrame({\"Name\": ['Tom', 'Mike', 'Tiffany'], \"Number\": [7, 15, 3]})\n\n\n\n\n\n\n\n\nName\nNumber\n\n\n\n\n0\nTom\n7\n\n\n1\nMike\n15\n\n\n2\nTiffany\n3\n\n\n\n\n\n\n\nFor more, see the Pandas documentation on DataFrames.\n\n\nDataFrames share the index\nThe dataframe is stitched together from values macthing on their index. For example:\n\ngpas = pd.Series(data=[4.0, np.nan, 3.4, 2.8, 2.5 ], index=['Allen','Bob','Chris','Ed', 'Frank'])\nyrs = pd.Series(data=['So', 'Fr', 'Jr', 'Sr'], index=['Allen','Bob','Dave', 'Frank'])\nstudents = pd.DataFrame( {'GPA': gpas, 'Year': yrs})\nstudents\n\n\n\n\n\n\n\n\nGPA\nYear\n\n\n\n\nAllen\n4.0\nSo\n\n\nBob\nNaN\nFr\n\n\nChris\n3.4\nNaN\n\n\nDave\nNaN\nJr\n\n\nEd\n2.8\nNaN\n\n\nFrank\n2.5\nSr\n\n\n\n\n\n\n\n\n\nAccessing elements with indexing\nYou can access columns in the DataFrame using the names of the series, much in the same way you would a dictionary. For example:\n\nstudents['GPA'] # slicing by row label\n\nAllen    4.0\nBob      NaN\nChris    3.4\nDave     NaN\nEd       2.8\nFrank    2.5\nName: GPA, dtype: float64\n\n\nSince the values in a DataFrame are Series, you can then access a particular value using the Series index. For example, since the Series data in studentsn were indexed by name, we can get Chris’s grade by doing:\n\nstudents['GPA']['Chris']\n\nnp.float64(3.4)\n\n\nMuch like a Series is like a special NumPy array with fancy indexing (and other useful features), a DataFrame is like a special type of dictionary, with some extra features that make handling datasets much easier. In fact, as we’ll see below, a DataFrame is more like a cross between a dictionary and a NumPy array that make it excel at data wrangling. (pun intended)\n\nAccessing elements with loc and iloc\nThe loc[index, col] and iloc[row_pos, col_pos] properties allow you to slice the dataframe. loc uses the index and column names, while iloc uses ordinal positions starting at zero.\nHere are some examples, using studentsn\n\n# Examples using loc\nprint(\"loc: Get the Chris' GPA: \", students.loc['Chris', 'GPA'])\nprint(\"loc: Get the Year of the last student (Frank): \", students.loc['Frank', 'Year'])\n\n# Same examples using iloc\nprint(\"iloc: Get the GPA of the student at row 2 (Chris): \", students.iloc[2, 0])\nprint(\"iloc: Get the Year of the last student (Frank): \", students.iloc[-1, 1])\n\nloc: Get the Chris' GPA:  3.4\nloc: Get the Year of the last student (Frank):  Sr\niloc: Get the GPA of the student at row 2 (Chris):  3.4\niloc: Get the Year of the last student (Frank):  Sr\n\n\n\n# You can also slice using loc and iloc\nprint(\"loc: last two rows:\\n\", students.loc['Ed':, 'GPA':'Year'])\nprint()\nprint(\"iloc: last two rows:\\n\", students.iloc[-2:, 0:2])\n\nloc: last two rows:\n        GPA Year\nEd     2.8  NaN\nFrank  2.5   Sr\n\niloc: last two rows:\n        GPA Year\nEd     2.8  NaN\nFrank  2.5   Sr\n\n\n\n\nNull Checks\nuse isna() to check for np.nan.\n\nstudents[students.GPA.isna()]\n\n\n\n\n\n\n\n\nGPA\nYear\n\n\n\n\nBob\nNaN\nFr\n\n\nDave\nNaN\nJr\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 2.1\n\n\n\nCreate this DataFrame:\n   s1   s2 s3\na   1  2.2  q\nb   2  NaN  q\nc   3  3.0  z\nd   4  1.5  z\nIn other words, the frame should have 3 columns named s1, s2, and s3, and the rows should be indexed with the strings a, b, c, and d. Use Series to create it to make sure the index is correct. Print the full the DataFrame (so that you can get back something like the above), then print the the first 2 rows and columns using loc or iloc.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ns1 = pd.Series(data = [1, 2, 3, 4], index=['a', 'b', 'c', 'd'], name='s1')\ns2 = pd.Series(data = [2.2, np.nan, 3.0, 1.5], index=['a', 'b', 'c', 'd'], name='s2')\ns3 = pd.Series(data = ['q', 'q', 'z', 'z'], index=['a', 'b', 'c', 'd'], name='s3')\n\ndf = pd.DataFrame({'s1':s1,'s2':s2,'s3':s3})\nprint(df)\n\nprint(df.loc['a':'b', 's1':'s2'])\n\n   s1   s2 s3\na   1  2.2  q\nb   2  NaN  q\nc   3  3.0  z\nd   4  1.5  z\n   s1   s2\na   1  2.2\nb   2  NaN",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-1.html#basic-dataframe-operations",
    "href": "04_data_wrangling/pandas-1.html#basic-dataframe-operations",
    "title": "2. Introduction to Pandas: Series and DataFrames",
    "section": "Basic Dataframe operations",
    "text": "Basic Dataframe operations\n\ninfo() provide names of columns, counts of non-null values in each columns, and data types.\ndescribe() for each numerical column provide some basic statistics (min, max, mean, and quartiles).\nhead(n=5) view the FIRST n rows in the dataframe (defaults to 5)\ntail(n=5) view the LAST n rows in the dataframe (defaults to 5)\nsample(n=1) view a random n rows from the dataframe (defautls to 1)\n.columns retrieve a list of columns in the dataframe\n\nTo illustrate this, we’ll load a comma-separated-value (CSV) file customers.csv that containing some customer data. We can load the file directly as a DataFrame using Panda’s read_csv function. Notice that we can pass a URL to the function. We don’t need to first download, Pandas will take care of that for us all under the hood!\n\ncustomers = pd.read_csv('https://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv')\nprint(customers)\n\n      First        Last                     Email Gender  Last IP Address  \\\n0        Al      Fresco        afresco@dayrep.com      M    74.111.18.161   \n1      Abby        Kuss           akuss@rhyta.com      F    23.80.125.101   \n2     Arial       Photo         aphoto@dayrep.com      F       24.0.14.56   \n3     Bette       Alott          balott@rhyta.com      F   56.216.127.219   \n4     Barb       Barion     bbarion@superrito.com      F     38.68.15.223   \n5     Barry  DeHatchett    bdehatchett@dayrep.com      M    23.192.215.78   \n6      Bill     Melator       bmelator@einrot.com      M     24.11.125.10   \n7     Candi       Cayne          ccayne@rhyta.com      F      24.39.14.15   \n8     Carol        Ling       cling@superrito.com      F    23.180.242.66   \n9       Cam         Rha           crha@einrot.com      M      24.1.25.140   \n10      Dan     Delyons       ddelyons@dayrep.com      M    24.38.224.161   \n11     Erin     Detyers       edetyers@dayrep.com      F     70.209.14.54   \n12    Euron   Tasomthin  etasomthin@superrito.com      M    68.199.40.156   \n13   Justin        Case          jcase@dayrep.com      M    23.192.215.44   \n14     Jean       Poole         jpoole@dayrep.com      F     23.182.25.40   \n15      Lee    Hvmeehom      lhvmeehom@einrot.com      F      215.82.23.2   \n16     Lisa  Karfurless    lkarfurless@dayrep.com      F    172.189.252.8   \n17     Mary     Melator        mmelator@rhyta.com      F       23.88.15.5   \n18     Mike      Rofone        mrofone@dayrep.com      M     23.224.160.4   \n19     Oren     Jouglad       ojouglad@einrot.com      M  128.122.140.238   \n20     Phil       Meaup         pmeaup@dayrep.com      M    23.83.132.200   \n21    Rowan      Deboat        rdeboat@dayrep.com      M      23.84.32.22   \n22      Ray     Ovlight       rovlight@dayrep.com      M     74.111.18.59   \n23     Sara      Bellum     sbellum@superrito.com      F     74.111.6.173   \n24      Sal        Ladd       sladd@superrito.com      M    23.112.202.16   \n25  Seymour       Ofewe         sofewe@dayrep.com      M      98.29.25.44   \n26       Ty       Anott          tanott@rhyta.com      M      23.230.12.5   \n27    Tally       Itupp      titupp@superrito.com      F    24.38.114.105   \n28      Tim        Pani       tpani@superrito.com      M    23.84.132.226   \n29   Victor        Rhee          vrhee@einrot.com      M   23.112.232.160   \n\n           City State  Total Orders  Total Purchased  Months Customer  \n0      Syracuse    NY             1               45                1  \n1       Phoenix    AZ             1               25                2  \n2        Newark    NJ             1              680                1  \n3       Raleigh    NC             6              560               18  \n4        Dallas    TX             4             1590                1  \n5        Boston    MA             1               15                6  \n6          Orem    UT             9             6090               35  \n7      Portland    ME             1              620                2  \n8      Syracuse    NY             2              440                6  \n9       Chicago    IL             0                0                1  \n10    Greenwich    CT             2             2570               10  \n11        Tampa    FL             5             1105               38  \n12    Hempstead    NY            13             4630               28  \n13       Boston    MA             3             1050                1  \n14     Kingston    NY             7              185               12  \n15     Columbus    OH             9              207               18  \n16      Fairfax    VA             6              250               27  \n17  Los Angeles    CA             8             4275               40  \n18     Cheyenne    WY             0                0                0  \n19     New York    NY            12             4500               36  \n20      Phoenix    AZ             4              930               24  \n21       Topeka    KS             1             3500               42  \n22     Syracuse    NY             6              125               42  \n23   Alexandria    VA             2              189                2  \n24    Rochester    NY            14              594               10  \n25    Cleveland    OH             9             1190                3  \n26     San Jose    CA             1               50                3  \n27    Sea Cliff    NY            11              380               42  \n28      Buffalo    NY             0                0                1  \n29    Green Bay    WI             0                0                2  \n\n\n\nDisplay the dataframe in Streamlit\nYou can use the st.dataframe() function to display a DataFrame in Streamlit.\nHere is an example:\n\nimport streamlit as st\nimport pandas as pd\n\nst.title(\"Dataframe Example\")\n\ncustomers = pd.read_csv('https://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv')\n\nst.dataframe(customers.head(20))\nst.dataframe(customers.describe())\n\n\n\n\n\n\n\nCautionCode Challenge 2.2\n\n\n\nSimilar to the previous example, load this file into a customers dataframe:\nhttps://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv\nThen create a radio widget to allow the user to select Head or Tail and a number input widget to enter a number of lines.\nOutput the head or tail of the dataframe and only show the number of lines input.\nHint: Use Streamlit’s radio and number_input functions.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\nimport pandas as pd\n\nst.title('My first dataframe')\n\ncustomers = pd.read_csv('https://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv')\n\nradio = st.radio('Show:', options=[ 'Head', 'Tail'], index=0)\nrows = st.number_input('Rows:', min_value=1, max_value=len(customers), value=5)\nif radio == 'Head':\n    st.dataframe(customers.head(rows))\nelse:\n    st.dataframe(customers.tail(rows))\n\n\n\n\n\n\n\n\nSelecting Rows and Columns\nWe can pair down the output of a dataframe by using:\n\na list of column names to select columns.\na boolean index to select matching rows.\n\n\ndata_dict = { \n    'Name':  ['Allen','Bob','Chris','Dave','Ed','Frank','Gus'], \n    'GPA': [4.0, np.nan, 3.4, 2.8, 2.5, 3.8, 3.0], \n    'Year' : ['So', 'Fr', 'Fr', 'Jr', 'Sr', 'Sr', 'Fr'] } \nstudents = pd.DataFrame( data_dict )\nstudents\n\n\n\n\n\n\n\n\nName\nGPA\nYear\n\n\n\n\n0\nAllen\n4.0\nSo\n\n\n1\nBob\nNaN\nFr\n\n\n2\nChris\n3.4\nFr\n\n\n3\nDave\n2.8\nJr\n\n\n4\nEd\n2.5\nSr\n\n\n5\nFrank\n3.8\nSr\n\n\n6\nGus\n3.0\nFr\n\n\n\n\n\n\n\n\n\nSelecting Columns\nThis example just gets the name and GPA columns\n\ncolumns_to_show = ['Name', 'GPA']\nstudents[columns_to_show]\n\n\n\n\n\n\n\n\nName\nGPA\n\n\n\n\n0\nAllen\n4.0\n\n\n1\nBob\nNaN\n\n\n2\nChris\n3.4\n\n\n3\nDave\n2.8\n\n\n4\nEd\n2.5\n\n\n5\nFrank\n3.8\n\n\n6\nGus\n3.0\n\n\n\n\n\n\n\n\nGetting the freshmen using a boolean index\nconsider the following:\n\nstudents['Year'] == 'Fr'\n\n0    False\n1     True\n2     True\n3    False\n4    False\n5    False\n6     True\nName: Year, dtype: bool\n\n\nThis it called a boolean index. The boolean expression is evaluted for each index in the DataFrame. It’s similar to the boolean “mask” array we used for extracting values from an array in the NumPy unit.\nWhen we apply the boolean index to the dataframe, only the rows where the index evaluates to True are returned.\n\nstudents[students['Year'] == 'Fr'] \n\n\n\n\n\n\n\n\nName\nGPA\nYear\n\n\n\n\n1\nBob\nNaN\nFr\n\n\n2\nChris\n3.4\nFr\n\n\n6\nGus\n3.0\nFr\n\n\n\n\n\n\n\nLikewise we can assign these variables for clarity\n\nonly_freshmen_index = students['Year'] == 'Fr'\nonly_freshmen = students[only_freshmen_index]\nonly_freshmen\n\n\n\n\n\n\n\n\nName\nGPA\nYear\n\n\n\n\n1\nBob\nNaN\nFr\n\n\n2\nChris\n3.4\nFr\n\n\n6\nGus\n3.0\nFr\n\n\n\n\n\n\n\n\n\nAnd Or and Not with Boolean indexes\nWhat if we want freshmen or seniors? We cannot use or in this case, instead we must use the python bitwise or operator. This is because the series contains multiple values.\nBitwise Operators\n\nand &\nor |\nnot ~\n\nNote: () are required between each bitwise operator.\n\n# freshmen and seniors\nonly_freshmen_seniors = (students['Year'] == 'Fr') | (students['Year'] == 'Sr')\nstudents[only_freshmen_seniors]\n\n\n\n\n\n\n\n\nName\nGPA\nYear\n\n\n\n\n1\nBob\nNaN\nFr\n\n\n2\nChris\n3.4\nFr\n\n\n4\nEd\n2.5\nSr\n\n\n5\nFrank\n3.8\nSr\n\n\n6\nGus\n3.0\nFr\n\n\n\n\n\n\n\n\n\n\nPutting it Together\nGet the name and GPA of only the freshmen that have a GPA stored (i.e., for which the GPA is not a NaN):\n\ncols = ['Name', 'GPA']\nfr_with_gpa = (students['Year'] == 'Fr') & (students['GPA'].notna())\nstudents[fr_with_gpa][cols]\n\n\n\n\n\n\n\n\nName\nGPA\n\n\n\n\n2\nChris\n3.4\n\n\n6\nGus\n3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 2.3\n\n\n\nSimilar to the previous example, load this file into a customers dataframe:\nhttps://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv\nThen:\n\nCreate a radio widget to allow the user to select “M” or “F” for gender,\na multi-select widget to pick which columns to display (Hint: use Streamlit’s multiselect method),\nand filter the rows to match the gender and selected columns.\n\nDisplay the dataframe in the Streamlit app.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\nimport pandas as pd\n\nst.title('Customers')\ncustomers = pd.read_csv('https://su-ist356-m003-fall-2025.github.io/course-home/04_data_wrangling/customers.csv')\nradio = st.radio('Gender:', options=[ 'M', 'F'], index=0)\ncols = st.multiselect('Columns:', options=customers.columns)\ngender_index = customers['Gender'] == radio\nst.dataframe(customers[gender_index][cols])",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "2. Introduction to Pandas: Series and DataFrames"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-3.html",
    "href": "04_data_wrangling/pandas-3.html",
    "title": "4. Joining Multiple Pandas Dataframes",
    "section": "",
    "text": "In this unit we will discuss strategies for dealing with multiple dataframes and combing them into a single dataframe.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "4. Joining Multiple Pandas Dataframes"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-3.html#concatenation",
    "href": "04_data_wrangling/pandas-3.html#concatenation",
    "title": "4. Joining Multiple Pandas Dataframes",
    "section": "Concatenation",
    "text": "Concatenation\nConcatenation appends the rows of one or more dataframes together. This is a row-oriented operation so the resulting datafram will be longer. For example, if a 50-row dataframe is concatenated with a 40-row dataframe, you will have a 90-row dataframe.\nThe pd.concat() function is used to concatenate frames. It takes several arguments, but the most often-used ones are: pd.concat(items: list[pd.DataFrame], ignore_index=False). The first argument is a list of Dataframes to concat. The ignore_index keyword argument governs what happens to the indices in the combined frame: we can choose to keep the current index in each dataframe ignore_index=False or create a new index ignore_index=True.\nAn example:\n\nimport pandas as pd\ncampus_students = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/master/delimited/campus-students.csv\")\ncampus_students\n\n\n\n\n\n\n\n\nName\nGrade\nYear\n\n\n\n\n0\nHelen\nNaN\nSophomore\n\n\n1\nIris\n10.0\nSenior\n\n\n2\nJimmy\n8.0\nFreshman\n\n\n3\nKaren\nNaN\nFreshman\n\n\n4\nLynne\n10.0\nSophomore\n\n\n5\nMike\n10.0\nSophomore\n\n\n6\nNico\nNaN\nJunior\n\n\n7\nPete\n8.0\nFreshman\n\n\n\n\n\n\n\n\nonline_students = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/master/delimited/online-students.csv\")\nonline_students\n\n\n\n\n\n\n\n\nName\nGrade\nYear\nLocation\n\n\n\n\n0\nAbby\n7.0\nFreshman\nNY\n\n\n1\nBob\n9.0\nSophomore\nCA\n\n\n2\nChris\n10.0\nSenior\nCA\n\n\n3\nDave\n8.0\nFreshman\nNY\n\n\n4\nEllen\n7.0\nSophomore\nTX\n\n\n5\nFran\n10.0\nSenior\nFL\n\n\n6\nGreg\n8.0\nFreshman\nNY\n\n\n\n\n\n\n\n\ncombined_students = pd.concat([campus_students, online_students])\ncombined_students\n\n\n\n\n\n\n\n\nName\nGrade\nYear\nLocation\n\n\n\n\n0\nHelen\nNaN\nSophomore\nNaN\n\n\n1\nIris\n10.0\nSenior\nNaN\n\n\n2\nJimmy\n8.0\nFreshman\nNaN\n\n\n3\nKaren\nNaN\nFreshman\nNaN\n\n\n4\nLynne\n10.0\nSophomore\nNaN\n\n\n5\nMike\n10.0\nSophomore\nNaN\n\n\n6\nNico\nNaN\nJunior\nNaN\n\n\n7\nPete\n8.0\nFreshman\nNaN\n\n\n0\nAbby\n7.0\nFreshman\nNY\n\n\n1\nBob\n9.0\nSophomore\nCA\n\n\n2\nChris\n10.0\nSenior\nCA\n\n\n3\nDave\n8.0\nFreshman\nNY\n\n\n4\nEllen\n7.0\nSophomore\nTX\n\n\n5\nFran\n10.0\nSenior\nFL\n\n\n6\nGreg\n8.0\nFreshman\nNY\n\n\n\n\n\n\n\n\nconcat() - Ignoring the index\nAs you can see from the code above the index from the original DataFrames was used. For example Helen and Abby both share the index 0. This means that if you provide index 0, you’ll get both of them, e.g.:\n\ncombined_students['Name'][0]\n\n0    Helen\n0     Abby\nName: Name, dtype: object\n\n\nWhile this is acceptable, there are situations where a new index based on combined values is desirable. We will encounter this later when grouping or pivioting data.\nTo make this happen include the ignore_index=True named argument. This will create a new index from the output DataFrame.\n\nstudents = pd.concat([campus_students, online_students], ignore_index=True)\nstudents\n\n\n\n\n\n\n\n\nName\nGrade\nYear\nLocation\n\n\n\n\n0\nHelen\nNaN\nSophomore\nNaN\n\n\n1\nIris\n10.0\nSenior\nNaN\n\n\n2\nJimmy\n8.0\nFreshman\nNaN\n\n\n3\nKaren\nNaN\nFreshman\nNaN\n\n\n4\nLynne\n10.0\nSophomore\nNaN\n\n\n5\nMike\n10.0\nSophomore\nNaN\n\n\n6\nNico\nNaN\nJunior\nNaN\n\n\n7\nPete\n8.0\nFreshman\nNaN\n\n\n8\nAbby\n7.0\nFreshman\nNY\n\n\n9\nBob\n9.0\nSophomore\nCA\n\n\n10\nChris\n10.0\nSenior\nCA\n\n\n11\nDave\n8.0\nFreshman\nNY\n\n\n12\nEllen\n7.0\nSophomore\nTX\n\n\n13\nFran\n10.0\nSenior\nFL\n\n\n14\nGreg\n8.0\nFreshman\nNY\n\n\n\n\n\n\n\n\n\nBest practice - data lineage\nWhen combining datasets, its a really good idea to retain data lineage, or a record of where the data came from. This can be done by added a column to each dataframe before concatenating.\nIn this example we create a student \"type\" column to track lineage.\n\ncampus_students = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/master/delimited/campus-students.csv\")\ncampus_students['type'] = 'campus'\nonline_students = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/master/delimited/online-students.csv\")\nonline_students['type'] = 'online'\nstudents = pd.concat([campus_students, online_students], ignore_index=True)\nstudents\n\n\n\n\n\n\n\n\nName\nGrade\nYear\ntype\nLocation\n\n\n\n\n0\nHelen\nNaN\nSophomore\ncampus\nNaN\n\n\n1\nIris\n10.0\nSenior\ncampus\nNaN\n\n\n2\nJimmy\n8.0\nFreshman\ncampus\nNaN\n\n\n3\nKaren\nNaN\nFreshman\ncampus\nNaN\n\n\n4\nLynne\n10.0\nSophomore\ncampus\nNaN\n\n\n5\nMike\n10.0\nSophomore\ncampus\nNaN\n\n\n6\nNico\nNaN\nJunior\ncampus\nNaN\n\n\n7\nPete\n8.0\nFreshman\ncampus\nNaN\n\n\n8\nAbby\n7.0\nFreshman\nonline\nNY\n\n\n9\nBob\n9.0\nSophomore\nonline\nCA\n\n\n10\nChris\n10.0\nSenior\nonline\nCA\n\n\n11\nDave\n8.0\nFreshman\nonline\nNY\n\n\n12\nEllen\n7.0\nSophomore\nonline\nTX\n\n\n13\nFran\n10.0\nSenior\nonline\nFL\n\n\n14\nGreg\n8.0\nFreshman\nonline\nNY\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.1: classic use case for concatenation\n\n\n\nConsider the JSON file here: https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/json-samples/employees-dict.json\nLet’s take a look at the structure of this file in a dictionary format:\n\nimport requests\nresponse = requests.get(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/json-samples/employees-dict.json\")\nemployees = response.json()\nemployees\n\n{'accounting': [{'firstName': 'John', 'lastName': 'Doe', 'age': 23},\n  {'firstName': 'Mary', 'lastName': 'Smith', 'age': 32}],\n 'sales': [{'firstName': 'Sally', 'lastName': 'Green', 'age': 27},\n  {'firstName': 'Jim', 'lastName': 'Galley', 'age': 41}],\n 'marketing': [{'firstName': 'Tom', 'lastName': 'Brown', 'age': 28}]}\n\n\nThe issue with the JSON data is that there are employees under keys by department \"accounting\", \"sales\", \"marketing\":\n\nprint(\"departments\", employees.keys())\n\ndepartments dict_keys(['accounting', 'sales', 'marketing'])\n\n\nThis is the classic use-case for pd.concat() as there is no practical way to use pd.json_normalize() to get all the employees under each department.\nChallenge: for each department:\n\nCreate a dataframe for that department.\nAdd lineage to the dataframe (i.e., add the department name).\nAdd the dataframe to a list of departments.\nUse pd.concat on the list of departments to create one dataframe. Print the dataframe. The output should look like:\n\n    firstName lastName  age        dept\n    0      John      Doe   23  accounting\n    1      Mary    Smith   32  accounting\n    2     Sally    Green   27       sales\n    3       Jim   Galley   41       sales\n    4       Tom    Brown   28   marketing\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport pandas as pd\nimport requests\n\nimport requests\nresponse = requests.get(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/json-samples/employees-dict.json\")\nemployees = response.json()\n\ndepartments = []\nfor dept_name in employees.keys():\n    # convert the department dictionary into a DataFrame\n    dept_employees = pd.DataFrame(employees[dept_name])\n    # Note: this also would work:\n    # dept_employees = pd.json_normalize(employees,  record_path=dept_name)\n    # Add the department name to the columns (data lineage):\n    dept_employees['dept'] = dept_name\n    departments.append(dept_employees)\n\ncombined = pd.concat(departments, ignore_index=True)\ncombined\n\n\n\n\n\n\n\n\nfirstName\nlastName\nage\ndept\n\n\n\n\n0\nJohn\nDoe\n23\naccounting\n\n\n1\nMary\nSmith\n32\naccounting\n\n\n2\nSally\nGreen\n27\nsales\n\n\n3\nJim\nGalley\n41\nsales\n\n\n4\nTom\nBrown\n28\nmarketing",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "4. Joining Multiple Pandas Dataframes"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-3.html#de-duplication",
    "href": "04_data_wrangling/pandas-3.html#de-duplication",
    "title": "4. Joining Multiple Pandas Dataframes",
    "section": "De-duplication",
    "text": "De-duplication\nSometimes after a pd.concat() you will have duplicate rows.\nYou can use df.drop_duplicates() to remove repeated rows of data.\nWithout an argument, this will scan the entire row of data to determine if the row is the same.\nIf your data has a natural key, you can specify that with the subset= named argument. This will improve performance. An example:\n\no1 = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/dedupe/orders1.csv\")\no2 = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/dedupe/orders2.csv\")\norders = pd.concat([o1, o2], ignore_index=True)\norders.sort_values('orderid')\n\n\n\n\n\n\n\n\norderid\norderdate\ncustname\ncustemail\ncustcountry\norderstatus\nordertotal\nordercreditcard\nordershipvia\nshippingtotal\n\n\n\n\n0\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n10\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n11\n3\n2020-02-23\nLoy Siberry\nlsiberry1@so-net.ne.jp\nCanada\ndelivered\n76.87\nDiscover\nUSPS\n6.27\n\n\n1\n4\n2022-04-28\nCarree Henworth\nNaN\nCanada\npending\n152.30\nDiscover\nUSPS\n12.74\n\n\n12\n4\n2022-04-28\nCarree Henworth\nNaN\nCanada\npending\n152.30\nDiscover\nUSPS\n12.74\n\n\n2\n5\n2019-11-22\nGoldina Godsafe\nggodsafe3@dailymail.co.uk\nUnited States\nshipped\n182.17\nAmex\nUPS\n5.44\n\n\n13\n6\n2022-05-03\nMarris Chatten\nmchatten4@csmonitor.com\nMexico\npending\n208.28\nDiscover\nRPS\n2.16\n\n\n3\n6\n2022-05-03\nMarris Chatten\nmchatten4@csmonitor.com\nMexico\npending\n208.28\nDiscover\nRPS\n2.16\n\n\n14\n7\n2022-12-19\nLogan Jacobsson\nljacobsson5@wufoo.com\nUnited States\ndelivered\n112.15\nAmex\nUSPS\n11.52\n\n\n4\n7\n2022-12-19\nLogan Jacobsson\nljacobsson5@wufoo.com\nUnited States\ndelivered\n112.15\nAmex\nUSPS\n11.52\n\n\n15\n8\n2019-06-05\nLilli Feares\nlfeares6@shop-pro.jp\nMexico\npending\n237.90\nDiscover\nFedEX\n4.48\n\n\n16\n9\n2019-02-17\nLowrance Sigsworth\nlsigsworth7@youtube.com\nUnited States\ndelivered\n141.94\nDiscover\nUSPS\n7.31\n\n\n17\n10\n2023-04-19\nLibbi Spadari\nlspadari8@dot.gov\nMexico\npending\n160.79\nDiscover\nRPS\n16.52\n\n\n5\n10\n2023-04-19\nLibbi Spadari\nlspadari8@dot.gov\nMexico\npending\n160.79\nDiscover\nRPS\n16.52\n\n\n6\n11\n2020-01-20\nRenato Hue\nrhue9@un.org\nCanada\ndelivered\n120.52\nVisa\nUSPS\n5.57\n\n\n7\n12\n2022-03-03\nLucky Helstrip\nlhelstripa@tmall.com\nMexico\ndelivered\n202.07\nAmex\nUPS\n18.57\n\n\n18\n12\n2022-03-03\nLucky Helstrip\nlhelstripa@tmall.com\nMexico\ndelivered\n202.07\nAmex\nUPS\n18.57\n\n\n8\n13\n2021-09-04\nDebi Myrie\ndmyrieb@unc.edu\nUnited States\ndelivered\n131.62\nAmex\nUPS\n2.37\n\n\n19\n13\n2021-09-04\nDebi Myrie\ndmyrieb@unc.edu\nUnited States\ndelivered\n131.62\nAmex\nUPS\n2.37\n\n\n20\n14\n2022-02-27\nHyacinth Aveyard\nhaveyardc@ucoz.com\nUnited States\npending\n209.86\nAmex\nUSPS\n8.69\n\n\n9\n15\n2019-01-11\nCrin Blanket\ncblanketd@newsvine.com\nUnited States\ndelivered\n85.46\nVisa\nUPS\n14.22\n\n\n\n\n\n\n\nHere, we use the entire row to check for duplicates:\n\norders.drop_duplicates().sort_values(\"orderid\")\n\n\n\n\n\n\n\n\norderid\norderdate\ncustname\ncustemail\ncustcountry\norderstatus\nordertotal\nordercreditcard\nordershipvia\nshippingtotal\n\n\n\n\n0\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n11\n3\n2020-02-23\nLoy Siberry\nlsiberry1@so-net.ne.jp\nCanada\ndelivered\n76.87\nDiscover\nUSPS\n6.27\n\n\n1\n4\n2022-04-28\nCarree Henworth\nNaN\nCanada\npending\n152.30\nDiscover\nUSPS\n12.74\n\n\n2\n5\n2019-11-22\nGoldina Godsafe\nggodsafe3@dailymail.co.uk\nUnited States\nshipped\n182.17\nAmex\nUPS\n5.44\n\n\n3\n6\n2022-05-03\nMarris Chatten\nmchatten4@csmonitor.com\nMexico\npending\n208.28\nDiscover\nRPS\n2.16\n\n\n4\n7\n2022-12-19\nLogan Jacobsson\nljacobsson5@wufoo.com\nUnited States\ndelivered\n112.15\nAmex\nUSPS\n11.52\n\n\n15\n8\n2019-06-05\nLilli Feares\nlfeares6@shop-pro.jp\nMexico\npending\n237.90\nDiscover\nFedEX\n4.48\n\n\n16\n9\n2019-02-17\nLowrance Sigsworth\nlsigsworth7@youtube.com\nUnited States\ndelivered\n141.94\nDiscover\nUSPS\n7.31\n\n\n5\n10\n2023-04-19\nLibbi Spadari\nlspadari8@dot.gov\nMexico\npending\n160.79\nDiscover\nRPS\n16.52\n\n\n6\n11\n2020-01-20\nRenato Hue\nrhue9@un.org\nCanada\ndelivered\n120.52\nVisa\nUSPS\n5.57\n\n\n7\n12\n2022-03-03\nLucky Helstrip\nlhelstripa@tmall.com\nMexico\ndelivered\n202.07\nAmex\nUPS\n18.57\n\n\n8\n13\n2021-09-04\nDebi Myrie\ndmyrieb@unc.edu\nUnited States\ndelivered\n131.62\nAmex\nUPS\n2.37\n\n\n20\n14\n2022-02-27\nHyacinth Aveyard\nhaveyardc@ucoz.com\nUnited States\npending\n209.86\nAmex\nUSPS\n8.69\n\n\n9\n15\n2019-01-11\nCrin Blanket\ncblanketd@newsvine.com\nUnited States\ndelivered\n85.46\nVisa\nUPS\n14.22\n\n\n\n\n\n\n\nHere, we just use the orderid to check for duplicates:\n\norders.drop_duplicates(subset=\"orderid\").sort_values(\"orderid\")\n\n\n\n\n\n\n\n\norderid\norderdate\ncustname\ncustemail\ncustcountry\norderstatus\nordertotal\nordercreditcard\nordershipvia\nshippingtotal\n\n\n\n\n0\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n11\n3\n2020-02-23\nLoy Siberry\nlsiberry1@so-net.ne.jp\nCanada\ndelivered\n76.87\nDiscover\nUSPS\n6.27\n\n\n1\n4\n2022-04-28\nCarree Henworth\nNaN\nCanada\npending\n152.30\nDiscover\nUSPS\n12.74\n\n\n2\n5\n2019-11-22\nGoldina Godsafe\nggodsafe3@dailymail.co.uk\nUnited States\nshipped\n182.17\nAmex\nUPS\n5.44\n\n\n3\n6\n2022-05-03\nMarris Chatten\nmchatten4@csmonitor.com\nMexico\npending\n208.28\nDiscover\nRPS\n2.16\n\n\n4\n7\n2022-12-19\nLogan Jacobsson\nljacobsson5@wufoo.com\nUnited States\ndelivered\n112.15\nAmex\nUSPS\n11.52\n\n\n15\n8\n2019-06-05\nLilli Feares\nlfeares6@shop-pro.jp\nMexico\npending\n237.90\nDiscover\nFedEX\n4.48\n\n\n16\n9\n2019-02-17\nLowrance Sigsworth\nlsigsworth7@youtube.com\nUnited States\ndelivered\n141.94\nDiscover\nUSPS\n7.31\n\n\n5\n10\n2023-04-19\nLibbi Spadari\nlspadari8@dot.gov\nMexico\npending\n160.79\nDiscover\nRPS\n16.52\n\n\n6\n11\n2020-01-20\nRenato Hue\nrhue9@un.org\nCanada\ndelivered\n120.52\nVisa\nUSPS\n5.57\n\n\n7\n12\n2022-03-03\nLucky Helstrip\nlhelstripa@tmall.com\nMexico\ndelivered\n202.07\nAmex\nUPS\n18.57\n\n\n8\n13\n2021-09-04\nDebi Myrie\ndmyrieb@unc.edu\nUnited States\ndelivered\n131.62\nAmex\nUPS\n2.37\n\n\n20\n14\n2022-02-27\nHyacinth Aveyard\nhaveyardc@ucoz.com\nUnited States\npending\n209.86\nAmex\nUSPS\n8.69\n\n\n9\n15\n2019-01-11\nCrin Blanket\ncblanketd@newsvine.com\nUnited States\ndelivered\n85.46\nVisa\nUPS\n14.22\n\n\n\n\n\n\n\nThat gives the same result as using the whole row because the orderid is one-to-one with the uniqueness of the row. It’s faster doing this though, even on this small dataset (use %timeit to compare)!\nNotice if we had used a different subset which is not representative of the row, we lose data. For example:\n\norders.drop_duplicates(subset=\"ordercreditcard\").sort_values(\"orderid\")\n\n\n\n\n\n\n\n\norderid\norderdate\ncustname\ncustemail\ncustcountry\norderstatus\nordertotal\nordercreditcard\nordershipvia\nshippingtotal\n\n\n\n\n0\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n2\n5\n2019-11-22\nGoldina Godsafe\nggodsafe3@dailymail.co.uk\nUnited States\nshipped\n182.17\nAmex\nUPS\n5.44\n\n\n6\n11\n2020-01-20\nRenato Hue\nrhue9@un.org\nCanada\ndelivered\n120.52\nVisa\nUSPS\n5.57\n\n\n\n\n\n\n\n\nReturning a dataframe of duplicates\nTo get a dataframe of just the duplicated values you can use df.duplicated(). This returns a boolean series that you can use to extract the duplicated rows from the concatenated dataframe. Example:\n\ndupes = orders.duplicated(subset=['orderid'])\norders[dupes]\n\n\n\n\n\n\n\n\norderid\norderdate\ncustname\ncustemail\ncustcountry\norderstatus\nordertotal\nordercreditcard\nordershipvia\nshippingtotal\n\n\n\n\n10\n2\n2023-03-24\nFrayda Pepperd\nfpepperd0@sciencedaily.com\nCanada\ndelivered\n228.39\nDiscover\nRPS\n12.05\n\n\n12\n4\n2022-04-28\nCarree Henworth\nNaN\nCanada\npending\n152.30\nDiscover\nUSPS\n12.74\n\n\n13\n6\n2022-05-03\nMarris Chatten\nmchatten4@csmonitor.com\nMexico\npending\n208.28\nDiscover\nRPS\n2.16\n\n\n14\n7\n2022-12-19\nLogan Jacobsson\nljacobsson5@wufoo.com\nUnited States\ndelivered\n112.15\nAmex\nUSPS\n11.52\n\n\n17\n10\n2023-04-19\nLibbi Spadari\nlspadari8@dot.gov\nMexico\npending\n160.79\nDiscover\nRPS\n16.52\n\n\n18\n12\n2022-03-03\nLucky Helstrip\nlhelstripa@tmall.com\nMexico\ndelivered\n202.07\nAmex\nUPS\n18.57\n\n\n19\n13\n2021-09-04\nDebi Myrie\ndmyrieb@unc.edu\nUnited States\ndelivered\n131.62\nAmex\nUPS\n2.37",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "4. Joining Multiple Pandas Dataframes"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-3.html#merges",
    "href": "04_data_wrangling/pandas-3.html#merges",
    "title": "4. Joining Multiple Pandas Dataframes",
    "section": "Merges",
    "text": "Merges\nA merge combines two dataframes based on a common column. The resulting dataframe is wider (has more columns) than the original dataframe. The function to do this is pd.merge(). It’s most common arguments:\npd.merge(left: pd.DataFrame, right:pd.Dataframe, how:str, left_on:str, right_on:str)\n\nhow specifies the join operation:\n\n\"inner\" - returns ONLY rows when values of left_on match right_on\n\"left\" - returns ALL rows from left and ONLY rows from right when values of left_on match right_on\n\"right\" - returns ALL rows from right and ONLY rows from left when values of left_on match right_on\n\"outer\" - returns ALL rows from left and right and rows when values of left_on match right_on\n\n\nTo illustrate the differences between these, consider merging the following two dataframes, one representing a roster of basketball players, and another a list of teams:\n\nbbplayers = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited/bbplayers.csv\")\nbbplayers\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\ncareer_pts\nplayer_team_id\n\n\n\n\n0\n101\nJordan\n32292\n1.0\n\n\n1\n102\nPippen\n18940\n1.0\n\n\n2\n103\nBryant\n33643\n2.0\n\n\n3\n104\nO'Neal\n28596\n2.0\n\n\n4\n105\nFudge\n0\nNaN\n\n\n\n\n\n\n\n\nbbteams = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/delimited/bbteams.csv\")\nbbteams\n\n\n\n\n\n\n\n\nteam_id\nteam_name\nteam_location\n\n\n\n\n0\n1\nBulls\nChicago, IL\n\n\n1\n2\nLakers\nLos Angeles, CA\n\n\n2\n3\nTropics\nFlint, MI\n\n\n\n\n\n\n\nInner join: Only rows that match the bbplayer.player_team_id and bbteam.team_id and bbteam will be included. Note that in this case we lose Player 105 and team 3 because there are no matches:\n\ncombined = pd.merge(bbplayers, bbteams, left_on='player_team_id', right_on='team_id', how='inner')\ncombined\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\ncareer_pts\nplayer_team_id\nteam_id\nteam_name\nteam_location\n\n\n\n\n0\n101\nJordan\n32292\n1.0\n1\nBulls\nChicago, IL\n\n\n1\n102\nPippen\n18940\n1.0\n1\nBulls\nChicago, IL\n\n\n2\n103\nBryant\n33643\n2.0\n2\nLakers\nLos Angeles, CA\n\n\n3\n104\nO'Neal\n28596\n2.0\n2\nLakers\nLos Angeles, CA\n\n\n\n\n\n\n\nLeft join: All rows from the left (every player) and ONLY rows from team that match. Now we see Player 105 despite no team match:\n\ncombined = pd.merge(bbplayers, bbteams, left_on='player_team_id', right_on='team_id', how='left')\ncombined\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\ncareer_pts\nplayer_team_id\nteam_id\nteam_name\nteam_location\n\n\n\n\n0\n101\nJordan\n32292\n1.0\n1.0\nBulls\nChicago, IL\n\n\n1\n102\nPippen\n18940\n1.0\n1.0\nBulls\nChicago, IL\n\n\n2\n103\nBryant\n33643\n2.0\n2.0\nLakers\nLos Angeles, CA\n\n\n3\n104\nO'Neal\n28596\n2.0\n2.0\nLakers\nLos Angeles, CA\n\n\n4\n105\nFudge\n0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nRight join: All rows from the right (every team) and ONLY rows from players that match. We now see Team 3 despite no player match:\n\ncombined = pd.merge(bbplayers, bbteams, left_on='player_team_id', right_on='team_id', how='right')\ncombined\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\ncareer_pts\nplayer_team_id\nteam_id\nteam_name\nteam_location\n\n\n\n\n0\n101.0\nJordan\n32292.0\n1.0\n1\nBulls\nChicago, IL\n\n\n1\n102.0\nPippen\n18940.0\n1.0\n1\nBulls\nChicago, IL\n\n\n2\n103.0\nBryant\n33643.0\n2.0\n2\nLakers\nLos Angeles, CA\n\n\n3\n104.0\nO'Neal\n28596.0\n2.0\n2\nLakers\nLos Angeles, CA\n\n\n4\nNaN\nNaN\nNaN\nNaN\n3\nTropics\nFlint, MI\n\n\n\n\n\n\n\nOuter join: All rows from both tables are included. This is equivalent to doing an inner join + all non-matching rows from both tables. In this case, we see Team 3 and Player 105:\n\ncombined = pd.merge(bbplayers, bbteams, left_on='player_team_id', right_on='team_id', how='outer')\ncombined\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\ncareer_pts\nplayer_team_id\nteam_id\nteam_name\nteam_location\n\n\n\n\n0\n101.0\nJordan\n32292.0\n1.0\n1.0\nBulls\nChicago, IL\n\n\n1\n102.0\nPippen\n18940.0\n1.0\n1.0\nBulls\nChicago, IL\n\n\n2\n103.0\nBryant\n33643.0\n2.0\n2.0\nLakers\nLos Angeles, CA\n\n\n3\n104.0\nO'Neal\n28596.0\n2.0\n2.0\nLakers\nLos Angeles, CA\n\n\n4\nNaN\nNaN\nNaN\nNaN\n3.0\nTropics\nFlint, MI\n\n\n5\n105.0\nFudge\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 4.2: who is not buying from MiniMart?\n\n\n\nConsider the following data from a grocery store: https://github.com/mafudge/datasets/tree/master/minimart. In that directory you’ll see customers.csv, which is a list of customers, and a separate CSV file of purchases made in the first four months of the year.\nYou have been hired to build a UI to display names of customers who did not buy from minimart in any given month. Write a Streamlit app that displays a dataframe of customers who did not buy anything in a given month. The app should have a drop down menu that allows the user to select the month to display.\nThe URL for the location of raw data that you can use in your app is:\nhttps://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/minimart/\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\nimport pandas as pd\n\nbase = \"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/minimart/\"\nmonths = ['jan', 'feb', 'mar', 'apr']\n\nst.title(\"Who's not Buying from MiniMart?\")\nmonth = st.selectbox('Select Month:', months)\n\npurchases = pd.read_csv(f\"{base}/purchases-{month}.csv\")\ncustomers = pd.read_csv(f\"{base}/customers.csv\")\ncombined = pd.merge(customers, purchases, left_on='customer_id', right_on='customer_id', how='left')\ncols = [\"customer_id\", \"firstname\", \"lastname\"]\ndid_not_buy = combined[\"order_id\"].isnull()\ncustomers_who_did_not_buy = combined[did_not_buy][cols]\nst.header(f\"These people did not buy anything in {month.capitalize()}.:\")\nst.dataframe(customers_who_did_not_buy, hide_index=True)\n\n# You can add the following to double check the results:\n#st.divider()\n#st.write(\"debug\")\n#st.dataframe(combined)\n# That will display the full combined frame for the selected month. The ones\n# without entries for the order id should be what's displayed in the table above.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "4. Joining Multiple Pandas Dataframes"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-5.html",
    "href": "04_data_wrangling/pandas-5.html",
    "title": "6. Grouping data and creating pivot tables with Pandas",
    "section": "",
    "text": "In this lesson you will learn how to group data together in a DataFrame and create summary statistics of the grouped data. You will also learn how to reshape a DataFrame, and create a pivot table.\nFor this lesson we will use the following data of (fake) exam scores:\nimport pandas as pd\nexams_data = 'https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/exam-scores/exam-scores.csv'\nexams = pd.read_csv(exams_data)\nexams.sample(10)\n\n\n\n\n\n\n\n\nClass_Section\nExam_Version\nCompletion_Time\nMade_Own_Study_Guide\nDid_Exam_Prep Assignment\nStudied_In_Groups\nStudent_Score\nPercentage\nLetter_Grade\n\n\n\n\n62\nM02\nD\n45\n?\n?\n?\n24\n80.00%\nB\n\n\n64\nM02\nD\n60\nN\nN\nY\n24\n80.00%\nB\n\n\n0\nM01\nA\n20\nN\nN\nY\n24\n80.00%\nB\n\n\n7\nM01\nB\n15\nY\nY\nY\n26\n86.70%\nB+\n\n\n44\nM02\nB\n45\nY\nY\nY\n25\n83.30%\nB\n\n\n26\nM01\nD\n60\nY\nY\nN\n19\n63.30%\nC-\n\n\n39\nM02\nB\n15\nN\nN\nY\n19\n63.30%\nC-\n\n\n46\nM02\nB\n55\nY\nN\nN\n17\n56.70%\nD\n\n\n23\nM01\nD\n35\n?\n?\n?\n13\n43.30%\nF\n\n\n52\nM02\nC\n40\nY\nY\nN\n16\n53.30%\nD\nTo follow along with the commands below, download the following scratch notebook to your ist356 directory, then open it with VS Code:\nDownload scratch-pandas-5.ipynb\nThis has the exams_data URL in it.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "6. Grouping data and creating pivot tables with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-5.html#group-by",
    "href": "04_data_wrangling/pandas-5.html#group-by",
    "title": "6. Grouping data and creating pivot tables with Pandas",
    "section": "Group By",
    "text": "Group By\nWith large data sets we frequently want to group common data together and summarize those groups in some way. For example, you might want to know:\n\nAverage exam score by section\nNumber of students who took each exam\nAverage grade based on whether students studied in groups\nTotal completion time by letter grade\n\nThe Pandas groupby method allows us to quickly group data like this. Using it involves two steps:\n\nRun the groupby method on a dataframe, giving it a column or list of columns to group the data by.\nThe groupby method returns an DataFrameGroupBy object. On its own, this object isn’t terrible informative. To get something useful, we then use the aggregate method (agg) to summarize the grouped data and display it as a DataFrame. When aggregating the data, we need to provide the name of an operation (sum, min, max, mean, std, quartile, count) by which to summarizie the data.\n\nHere’s an example using our exams data:\n\n# Example: Total number of exams take by section and the average score in each section:\nexams_by_section = exams.groupby(by=['Class_Section']).agg({ 'Class_Section': 'count', 'Student_Score': 'mean' })\nexams_by_section\n\n\n\n\n\n\n\n\nClass_Section\nStudent_Score\n\n\nClass_Section\n\n\n\n\n\n\nM01\n29\n23.000000\n\n\nM02\n36\n22.527778\n\n\n\n\n\n\n\nHere, we’ve chosen to group the exams by the class section. We’ve then produced a DataFrame in which the class sections are summarized by the number of students and their average score. The column names are a little confusing, however. Let’s rename them to make the output more readable:\n\nexams_by_section = exams.groupby(by=['Class_Section']).agg({ 'Class_Section': 'count', 'Student_Score': 'mean' })\nexams_by_section = exams_by_section.rename(columns={'Class_Section': 'Exam_Count', 'Student_Score': 'Average_Score'})\nexams_by_section\n\n\n\n\n\n\n\n\nExam_Count\nAverage_Score\n\n\nClass_Section\n\n\n\n\n\n\nM01\n29\n23.000000\n\n\nM02\n36\n22.527778\n\n\n\n\n\n\n\nNote that the grouped columns end up in the index method to add the grouped columns back as a column:\n\nexams_by_section['Class_Section'] = exams_by_section.index\nexams_by_section\n\n\n\n\n\n\n\n\nExam_Count\nAverage_Score\nClass_Section\n\n\nClass_Section\n\n\n\n\n\n\n\nM01\n29\n23.000000\nM01\n\n\nM02\n36\n22.527778\nM02\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 6.1\n\n\n\nCreate a Streamlit app that will load the exams csv file above and will allow the user to select one of the following: Made_Own_Study_Guide, Did_Exam_Prep Assignment, Studied_In_Groups. After the selection is made, display a dataframe that summarizes the count of students and the average student score for the selection.\nHint: For offering the selection, use Streamlit’s selectbox method.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\nimport pandas as pd\n\nst.title(\"Exam Scores\")\n\n\noptions = ['Made_Own_Study_Guide', 'Did_Exam_Prep Assignment', 'Studied_In_Groups']\n\nexams = pd.read_csv('https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/exam-scores/exam-scores.csv')\n\noption = st.selectbox('Select Exam:', options)\n\nsummary_df = exams.groupby(by=option).agg({'Class_Section': 'count', 'Student_Score' :'mean'})\nsummary_df = summary_df.rename(columns={'Class_Section': 'Student Count', 'Student_Score': 'Mean Score'})\n\nst.dataframe(summary_df)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "6. Grouping data and creating pivot tables with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-5.html#pivot-and-melt",
    "href": "04_data_wrangling/pandas-5.html#pivot-and-melt",
    "title": "6. Grouping data and creating pivot tables with Pandas",
    "section": "Pivot and Melt",
    "text": "Pivot and Melt\nPivot and melt are inverse operations:\n\npivot makes “long” data “wide” moving rows into columns.\nmelt makes “wide” data “long” moving columns into rows. Basically, the opposite of pivot.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThese functions only move data, they are unable to summarize it.\nThe intersection of row/column must contain a single value. Multiple values under the same row/column will fail.\n\n\n\nTo set this up this example from exams let’s create a dataframe that summarizes the data. We will add the index columns back to the dataframe for clarity. Please note this is not something that needs to be done typically. We are just re-using the dataset for this example. (In fact, the following block of code is basically creating a pivot table from the exams data. We’ll see how to create pivot tables using a much simpler way below.)\n\n# Get average scores by section and exam version:\navg_scores_by_section_and_version = exams.groupby(\n    by=['Class_Section', 'Exam_Version']).agg({'Student_Score': 'mean'})\n\n# add section and exam version back to dataframe\navg_scores_by_section_and_version['Class_Section'] = avg_scores_by_section_and_version.index.get_level_values('Class_Section')\navg_scores_by_section_and_version['Exam_Version'] = avg_scores_by_section_and_version.index.get_level_values('Exam_Version')\n# reset the index\navg_scores_by_section_and_version = avg_scores_by_section_and_version.reset_index(drop=True)\n#rename the Student_score to average score\navg_scores_by_section_and_version =  avg_scores_by_section_and_version.rename(columns={'Student_Score': 'Average_Score'})\n#reorder the columns\navg_scores_by_section_and_version = avg_scores_by_section_and_version[['Class_Section', 'Exam_Version', 'Average_Score']]\n\n#show\navg_scores_by_section_and_version\n\n\n\n\n\n\n\n\nClass_Section\nExam_Version\nAverage_Score\n\n\n\n\n0\nM01\nA\n25.428571\n\n\n1\nM01\nB\n23.571429\n\n\n2\nM01\nC\n23.714286\n\n\n3\nM01\nD\n19.750000\n\n\n4\nM02\nA\n22.400000\n\n\n5\nM02\nB\n23.222222\n\n\n6\nM02\nC\n21.777778\n\n\n7\nM02\nD\n22.750000\n\n\n\n\n\n\n\n\nPivot()\nLet’s pivot this data two different ways:\n\nexam_version_in_col - a pivot where the exam version is in the column\nclass_section_in_col - a pivot where the class section is in the column\n\n\nexam_version_in_col = avg_scores_by_section_and_version.pivot(\n    index='Class_Section', columns='Exam_Version', values='Average_Score')\nexam_version_in_col\n\n\n\n\n\n\n\nExam_Version\nA\nB\nC\nD\n\n\nClass_Section\n\n\n\n\n\n\n\n\nM01\n25.428571\n23.571429\n23.714286\n19.75\n\n\nM02\n22.400000\n23.222222\n21.777778\n22.75\n\n\n\n\n\n\n\n\nclass_section_in_col = avg_scores_by_section_and_version.pivot(\n    index='Exam_Version', columns='Class_Section', values='Average_Score')\nclass_section_in_col\n\n\n\n\n\n\n\nClass_Section\nM01\nM02\n\n\nExam_Version\n\n\n\n\n\n\nA\n25.428571\n22.400000\n\n\nB\n23.571429\n23.222222\n\n\nC\n23.714286\n21.777778\n\n\nD\n19.750000\n22.750000\n\n\n\n\n\n\n\n\n\nMelt()\nWe will now melt the data back into its original shape. Melt requires:\n\nid_vars=list list of columns which remain in the melt\nvar_name=str column name of the columns to unpivot\nvalue_name column name of the values to unpivot\n\nFirst, to get this this example to work, we need to add the index values as a column (here called Class_Section):\n\nexam_version_in_col['Class_Section'] = exam_version_in_col.index\nmelted1 = exam_version_in_col.melt(id_vars=[\"Class_Section\"],\n                                   var_name=\"Exam_Version\",\n                                   value_name='Average_Score')\nmelted1\n\n\n\n\n\n\n\n\nClass_Section\nExam_Version\nAverage_Score\n\n\n\n\n0\nM01\nA\n25.428571\n\n\n1\nM02\nA\n22.400000\n\n\n2\nM01\nB\n23.571429\n\n\n3\nM02\nB\n23.222222\n\n\n4\nM01\nC\n23.714286\n\n\n5\nM02\nC\n21.777778\n\n\n6\nM01\nD\n19.750000\n\n\n7\nM02\nD\n22.750000\n\n\n\n\n\n\n\nDoing the same with the class_section_in_col:\n\nclass_section_in_col['Exam_Version'] = class_section_in_col.index\nmelted2 = class_section_in_col.melt(id_vars=[\"Exam_Version\"],\n                                    var_name=\"Class_Section\",\n                                    value_name='Average_Score')\nmelted2\n\n\n\n\n\n\n\n\nExam_Version\nClass_Section\nAverage_Score\n\n\n\n\n0\nA\nM01\n25.428571\n\n\n1\nB\nM01\n23.571429\n\n\n2\nC\nM01\n23.714286\n\n\n3\nD\nM01\n19.750000\n\n\n4\nA\nM02\n22.400000\n\n\n5\nB\nM02\n23.222222\n\n\n6\nC\nM02\n21.777778\n\n\n7\nD\nM02\n22.750000",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "6. Grouping data and creating pivot tables with Pandas"
    ]
  },
  {
    "objectID": "04_data_wrangling/pandas-5.html#pivot_table",
    "href": "04_data_wrangling/pandas-5.html#pivot_table",
    "title": "6. Grouping data and creating pivot tables with Pandas",
    "section": "Pivot_table()",
    "text": "Pivot_table()\nThe pd.pivot_table() function combines a groupby() with a pivot(). Its intended for when you need to pivot and aggregate in the pivot, avoiding a lot of extra code such as adding indexes as columns (as we had to above).\nHere’s the examples above, but with a pivot_table on the original examsdata. We can skip the processing building avg_scores_by_section_and_version because pivot_table() allows us to summarize data.\n\nexam_version_in_col = exams.pivot_table(index='Class_Section',\n                                        columns='Exam_Version',\n                                        values='Student_Score',\n                                        aggfunc='mean')\nexam_version_in_col\n\n\n\n\n\n\n\nExam_Version\nA\nB\nC\nD\n\n\nClass_Section\n\n\n\n\n\n\n\n\nM01\n25.428571\n23.571429\n23.714286\n19.75\n\n\nM02\n22.400000\n23.222222\n21.777778\n22.75\n\n\n\n\n\n\n\n\nclass_section_in_col = exams.pivot_table(index='Exam_Version',\n                                         columns='Class_Section',\n                                         values='Student_Score',\n                                         aggfunc='mean')\nclass_section_in_col\n\n\n\n\n\n\n\nClass_Section\nM01\nM02\n\n\nExam_Version\n\n\n\n\n\n\nA\n25.428571\n22.400000\n\n\nB\n23.571429\n23.222222\n\n\nC\n23.714286\n21.777778\n\n\nD\n19.750000\n22.750000\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCode Challenge 6.2\n\n\n\nLet’s build an interactive pivot table in streamlit! Using the exams csv file from above:\n\nAdd a selection widget (use Steamlit’s selectbox method) that lets the user select one of the following fields to put in the row of the pivot table:\n::: {#c33fc35d .cell execution_count=13} {.python .cell-code}  fields = ['Class_Section', 'Exam_Version', 'Made_Own_Study_Guide', 'Did_Exam_Prep Assignment', 'Studied_In_Groups','Letter_Grade'] :::\nAdd another selection widget that allows the user to select which field to display in the columns. Note: you will need to remove the field the user selected for the row in the list of options, else you’ll get an error if they select the same field twice.\nCreate another selection widget that allows the user to select what data to populate the pivot table with. The pivot table should display the average of the selected value. The options for the displayed data should be:\n\nmeasures = ['Completion_Time','Student_Score']\n\nBuild the pivot table dataframe from the inputs. Use the average for the aggfunc\nDisplay the pivot table.\n\nHere’s a screen shot of what your app should like:\n\nBonus: Cache the exams data so that it is not reloaded every time you interact with the app! Refer back to the Session State section of the Streamlit tutorial for help.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport streamlit as st\nimport pandas as pd\n\nst.title(\"Exam Scores Pivot Table\")\n\n# load the data: we'll only do this once, then cache\nif 'exams' not in st.session_state:\n    st.session_state.exams = pd.read_csv('https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/exam-scores/exam-scores.csv')\n\n# set up the options\nfields = [ 'Class_Section', 'Exam_Version', 'Made_Own_Study_Guide', 'Did_Exam_Prep Assignment', 'Studied_In_Groups','Letter_Grade']\nmeasures = ['Student_Score', 'Completion_Time']\nrow = st.selectbox('Display in Row:', fields)\nfields.remove(row)\ncol = st.selectbox('Display in Column:', fields)\nvalue = st.selectbox('Display Average of:', measures)\n\npivot_df = st.session_state.exams.pivot_table(index=row, columns=col, values=value, aggfunc='mean')\nst.dataframe(pivot_df)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "4. Data Wrangling",
      "6. Grouping data and creating pivot tables with Pandas"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html",
    "href": "05_web_apis/webapi-1.html",
    "title": "1. REST APIs",
    "section": "",
    "text": "A REST API (REpresentational State Transfer API) is a set of functions that can be accessed over the internet. The functions are organized in a way that they can be accessed using the common protocol of the Web, HTTP (Hypertext-Transport Protocol).\nBy design, REST APIs are stateless, meaning:\n\ncalls can be made independently of one another,\neach call contains all of the data needed to complete itself successfully, and\nno one call depends on the next.\n\nREST APIs are designed around resources, which consists of a URI (usually a URL), and an HTTP request method.\nThe methods are:\n\nGET: retrieve a resource\nPOST: create a new resource\nPUT: update a resource\nDELETE: remove a resource\nPATCH: update a resource with partial data",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#what-is-a-web-api-rest-api",
    "href": "05_web_apis/webapi-1.html#what-is-a-web-api-rest-api",
    "title": "1. REST APIs",
    "section": "",
    "text": "A REST API (REpresentational State Transfer API) is a set of functions that can be accessed over the internet. The functions are organized in a way that they can be accessed using the common protocol of the Web, HTTP (Hypertext-Transport Protocol).\nBy design, REST APIs are stateless, meaning:\n\ncalls can be made independently of one another,\neach call contains all of the data needed to complete itself successfully, and\nno one call depends on the next.\n\nREST APIs are designed around resources, which consists of a URI (usually a URL), and an HTTP request method.\nThe methods are:\n\nGET: retrieve a resource\nPOST: create a new resource\nPUT: update a resource\nDELETE: remove a resource\nPATCH: update a resource with partial data",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#http-without-a-rest-api-example",
    "href": "05_web_apis/webapi-1.html#http-without-a-rest-api-example",
    "title": "1. REST APIs",
    "section": "HTTP without a rest API Example",
    "text": "HTTP without a rest API Example\nLet’s start with an example of invoking a simple HTTP request without using a REST API.\nLet’s retrieve the content of the course website programmatically.\nNotice the response is HTML content. This is a markup language used to create web pages and is intended for humans.\n\nimport requests\nuri = \"https://su-ist356-m003-fall-2025.github.io/course-home/\"\nresponse = requests.get(uri)\n# Note: we're just printing the first 1000 characters here to avoid\n# overwhelming output\nprint(response.text[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\n\n&lt;meta charset=\"utf-8\"&gt;\n&lt;meta name=\"generator\" content=\"quarto-1.8.25\"&gt;\n\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\n\n\n&lt;title&gt;IST 356: Programming Techniques for Data Analytics – IST 356 Fall 2025&lt;/title&gt;\n&lt;style&gt;\ncode{white-space: pre-wrap;}\nspan.smallcaps{font-variant: small-caps;}\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\ndiv.column{flex: auto; overflow-x: auto;}\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\nul.task-list{list-style: none;}\nul.task-list li input[type=\"checkbox\"] {\n  width: 0.8em;\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \n  vertical-align: middle;\n}\n&lt;/style&gt;\n\n\n&lt;script src=\"site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\n&lt;script src=\"site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#http-rest-api-example",
    "href": "05_web_apis/webapi-1.html#http-rest-api-example",
    "title": "1. REST APIs",
    "section": "HTTP REST API Example",
    "text": "HTTP REST API Example\nThis example will use the funny names API to demonstrate how to retrieve data from a REST API. Note that the code is exactly the same as the previous example, but the URL is different.\n\nuri = \"https://cent.ischool-iot.net/api/funnyname/random\"\nresponse = requests.get(uri)\nprint(response.text)\n\n[{\"first\": \"Rip\", \"last\": \"Itupp\"}]",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#parsing-the-json-response",
    "href": "05_web_apis/webapi-1.html#parsing-the-json-response",
    "title": "1. REST APIs",
    "section": "Parsing the JSON response",
    "text": "Parsing the JSON response\nYou can see from the example above, the response is in JSON format. Recall that JSON is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.\nSince REST API’s are for machines, it makes sense to use the JSON format. We can deserialize the JSON response into a Python dictionary or list using the json() method on requests.\n\nuri = \"https://cent.ischool-iot.net/api/funnyname/random\"\nresponse = requests.get(uri)\nfunny_person = response.json()\nprint(funny_person) # list of dict\nprint(funny_person[0]['first'], funny_person[0]['last'])\n\n[{'first': 'Kenny', 'last': 'Doit'}]\nKenny Doit",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#response-codes-for-handling-errors",
    "href": "05_web_apis/webapi-1.html#response-codes-for-handling-errors",
    "title": "1. REST APIs",
    "section": "Response Codes for Handling Errors",
    "text": "Response Codes for Handling Errors\nWhen the server returns a response to a request, included in the response is the HTTP status code, which tells the client whether or not the request worked. Unlike a regular API / function call such as print() this is necessary because there is a lot that can go wrong when you call a function over the open internet. Status codes are 3 digit numbers and the first number indicates the type of response:\n\n1xx - codes are informational. These are seldom used in web APIs.\n2xx - codes which begin with a 2 indicate success. The most common code is 200 - OK.\n3xx - codes which begin with a 3 indicate redirection - the response is not comming from the request URL you requested. For -example a 304 - Not modified means your response is coming from the browser’s cache (content already downloaded).\n4xx - codes which begin with a 4 indicate a client error. The most common code here is 404 - Not Found. Any 4xx errors mean the requestor did something wrong. (In this case, that’s you!)\n5xx - codes which begin with a 5 indicate a server error. The most common code here is 500 - Internal server error, which indicates the server could not process the request. When this happens it could be the web API’s problem or the way you made the request.\n\nWe handle errors using the raise_for_status() method on the response object. This method will raise an exception if the response is any status code other than 2xx. It’s good to raise an exception here because it will stop the program from continuing and potentially causing more problems.\n\n# This intentionally fails with 404 - not found\nuri = \"https://cent.ischool-iot.net/api/funnynamez/random\"\nresponse = requests.get(uri)\nresponse.raise_for_status()\n# none of this code is relevant if the status is not 2xx\nfunny_person = response.json()\nprint(funny_person) # list of dict\nprint(funny_person[0]['first'], funny_person[0]['last'])\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[4], line 4\n      2 uri = \"https://cent.ischool-iot.net/api/funnynamez/random\"\n      3 response = requests.get(uri)\n----&gt; 4 response.raise_for_status()\n      5 # none of this code is relevant if the status is not 2xx\n      6 funny_person = response.json()\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/requests/models.py:1026, in Response.raise_for_status(self)\n   1021     http_error_msg = (\n   1022         f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n   1023     )\n   1025 if http_error_msg:\n-&gt; 1026     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 404 Client Error: NOT FOUND for url: https://cent.ischool-iot.net/api/funnynamez/random",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#algorithm-for-calling-any-rest-api-in-python",
    "href": "05_web_apis/webapi-1.html#algorithm-for-calling-any-rest-api-in-python",
    "title": "1. REST APIs",
    "section": "Algorithm for calling any REST API in Python",
    "text": "Algorithm for calling any REST API in Python\n\nPrepare the request URI\n\nheaders\nquery parameters\nbody\n\nMake the request with URI and appropriate method\nCheck the response status code with rise_for_status()\nDeserialize the response into a Python object\n\nThe process is always the same, only the way the requrest is prepares and your handling of the response content will change.\n\nExamples\nIn these examples we use the JSONPlaceholder API, which is a mock API for fake data. It’s mostly meant to test out API calls.\nExample 1:\nIn this example, we get a single user’s street address.\n\nex1_uri = \"https://jsonplaceholder.typicode.com/users/1\"\nresponse = requests.get(ex1_uri)\nresponse.raise_for_status()\nuser = response.json()\nprint(\"STREET:\", user['address']['street'])\n\nSTREET: Kulas Light\n\n\nExample 2:\nHere, we get a (random) title for a fake blog post.\n\nex2_uri = \"https://jsonplaceholder.typicode.com/posts/1\"\nresponse = requests.get(ex2_uri)\nresponse.raise_for_status()\npost = response.json()\nprint(\"TITLE:\", post['title'])\n\nTITLE: sunt aut facere repellat provident occaecati excepturi optio reprehenderit\n\n\n\n\n\n\n\n\nCautionCode Challenge 1.1\n\n\n\nWrite some Python that will:\n\nRead from the URL https://jsonplaceholder.typicode.com/users/\nDisplay the returned data in a Pandas DataFrame.\n\nHints:\n\nUse the requests library to get the data.\nUse Panda’s json_normalize() to convert the nested json data into a dataframe. Refer back to the second Pandas tutorial if you forget how to use that.\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests\nimport pandas as pd\n\nuri = \"https://jsonplaceholder.typicode.com/users/\"\nresponse = requests.get(uri)\nresponse.raise_for_status()\ndata = response.json()\ndf = pd.json_normalize(data)\ndf\n\n\n\n\n\n\n\n\nid\nname\nusername\nemail\nphone\nwebsite\naddress.street\naddress.suite\naddress.city\naddress.zipcode\naddress.geo.lat\naddress.geo.lng\ncompany.name\ncompany.catchPhrase\ncompany.bs\n\n\n\n\n0\n1\nLeanne Graham\nBret\nSincere@april.biz\n1-770-736-8031 x56442\nhildegard.org\nKulas Light\nApt. 556\nGwenborough\n92998-3874\n-37.3159\n81.1496\nRomaguera-Crona\nMulti-layered client-server neural-net\nharness real-time e-markets\n\n\n1\n2\nErvin Howell\nAntonette\nShanna@melissa.tv\n010-692-6593 x09125\nanastasia.net\nVictor Plains\nSuite 879\nWisokyburgh\n90566-7771\n-43.9509\n-34.4618\nDeckow-Crist\nProactive didactic contingency\nsynergize scalable supply-chains\n\n\n2\n3\nClementine Bauch\nSamantha\nNathan@yesenia.net\n1-463-123-4447\nramiro.info\nDouglas Extension\nSuite 847\nMcKenziehaven\n59590-4157\n-68.6102\n-47.0653\nRomaguera-Jacobson\nFace to face bifurcated interface\ne-enable strategic applications\n\n\n3\n4\nPatricia Lebsack\nKarianne\nJulianne.OConner@kory.org\n493-170-9623 x156\nkale.biz\nHoeger Mall\nApt. 692\nSouth Elvis\n53919-4257\n29.4572\n-164.2990\nRobel-Corkery\nMulti-tiered zero tolerance productivity\ntransition cutting-edge web services\n\n\n4\n5\nChelsey Dietrich\nKamren\nLucio_Hettinger@annie.ca\n(254)954-1289\ndemarco.info\nSkiles Walks\nSuite 351\nRoscoeview\n33263\n-31.8129\n62.5342\nKeebler LLC\nUser-centric fault-tolerant solution\nrevolutionize end-to-end systems\n\n\n5\n6\nMrs. Dennis Schulist\nLeopoldo_Corkery\nKarley_Dach@jasper.info\n1-477-935-8478 x6430\nola.org\nNorberto Crossing\nApt. 950\nSouth Christy\n23505-1337\n-71.4197\n71.7478\nConsidine-Lockman\nSynchronised bottom-line interface\ne-enable innovative applications\n\n\n6\n7\nKurtis Weissnat\nElwyn.Skiles\nTelly.Hoeger@billy.biz\n210.067.6132\nelvis.io\nRex Trail\nSuite 280\nHowemouth\n58804-1099\n24.8918\n21.8984\nJohns Group\nConfigurable multimedia task-force\ngenerate enterprise e-tailers\n\n\n7\n8\nNicholas Runolfsdottir V\nMaxime_Nienow\nSherwood@rosamond.me\n586.493.6943 x140\njacynthe.com\nEllsworth Summit\nSuite 729\nAliyaview\n45169\n-14.3990\n-120.7677\nAbernathy Group\nImplemented secondary concept\ne-enable extensible e-tailers\n\n\n8\n9\nGlenna Reichert\nDelphine\nChaim_McDermott@dana.io\n(775)976-6794 x41206\nconrad.com\nDayna Park\nSuite 449\nBartholomebury\n76495-3109\n24.6463\n-168.8889\nYost and Sons\nSwitchable contextually-based project\naggregate real-time technologies\n\n\n9\n10\nClementina DuBuque\nMoriah.Stanton\nRey.Padberg@karina.biz\n024-648-3804\nambrose.net\nKattie Turnpike\nSuite 198\nLebsackbury\n31428-2261\n-38.2386\n57.2232\nHoeger LLC\nCentralized empowering task-force\ntarget end-to-end models",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#query-string",
    "href": "05_web_apis/webapi-1.html#query-string",
    "title": "1. REST APIs",
    "section": "Query String",
    "text": "Query String\nThe Query String is a part of the URL that is used to pass data to the server on the URL. It is appended to the end of the URL and begins with a question mark “?”. The query string is made up of a series of key-value pairs separated by an ampersand (&).\nExamples:\n\n/sample?x=bar =&gt; {'x': 'bar'}\n/sample?name=John&age=30 =&gt; {'name': 'John', 'age': 'go'}\n/sample?name=John&age=30&count=4 =&gt; {'name': 'John', 'age': '30', count: '4'}\n\nIn the Python requests library, you can pass the query string as a dictionary under the params named argument.\nTo illustrate, here’s an example where we use the arXiv’s REST API to retrieve information about the paper Attention Is All You Need. In the first example, we put the query information directly into the URL:\n\nurl = \"http://export.arxiv.org/api/query?id_list=1706.03762\"\nresponse = requests.get(url)\nresponse.raise_for_status()\nprint(response.text[:1000])\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"&gt;\n  &lt;link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1706.03762%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/&gt;\n  &lt;title type=\"html\"&gt;ArXiv Query: search_query=&amp;id_list=1706.03762&amp;start=0&amp;max_results=10&lt;/title&gt;\n  &lt;id&gt;http://arxiv.org/api/zUwBFJ+vAUSpXAR7QFveSY/bZos&lt;/id&gt;\n  &lt;updated&gt;2025-10-27T00:00:00-04:00&lt;/updated&gt;\n  &lt;opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;1&lt;/opensearch:totalResults&gt;\n  &lt;opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;0&lt;/opensearch:startIndex&gt;\n  &lt;opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;10&lt;/opensearch:itemsPerPage&gt;\n  &lt;entry&gt;\n    &lt;id&gt;http://arxiv.org/abs/1706.03762v7&lt;/id&gt;\n    &lt;updated&gt;2023-08-02T00:41:18Z&lt;/updated&gt;\n    &lt;published&gt;2017-06-12T17:57:34Z&lt;/published&gt;\n    &lt;title&gt;Attention Is All You Need&lt;/title&gt;\n    &lt;summary&gt;  The domin\n\n\nNote: The arXiv API returns XML rather than JSON. XML is just another format for transmitting data.\nHere’s the same query, but this time passing the query string as a dictionary to the params argument:\n\nurl = \"http://export.arxiv.org/api/query\"\nresponse = requests.get(url, params={'id_list': '1706.03762'})\nresponse.raise_for_status()\nprint(response.text[:1000])\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"&gt;\n  &lt;link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1706.03762%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/&gt;\n  &lt;title type=\"html\"&gt;ArXiv Query: search_query=&amp;id_list=1706.03762&amp;start=0&amp;max_results=10&lt;/title&gt;\n  &lt;id&gt;http://arxiv.org/api/zUwBFJ+vAUSpXAR7QFveSY/bZos&lt;/id&gt;\n  &lt;updated&gt;2025-10-27T00:00:00-04:00&lt;/updated&gt;\n  &lt;opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;1&lt;/opensearch:totalResults&gt;\n  &lt;opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;0&lt;/opensearch:startIndex&gt;\n  &lt;opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\"&gt;10&lt;/opensearch:itemsPerPage&gt;\n  &lt;entry&gt;\n    &lt;id&gt;http://arxiv.org/abs/1706.03762v7&lt;/id&gt;\n    &lt;updated&gt;2023-08-02T00:41:18Z&lt;/updated&gt;\n    &lt;published&gt;2017-06-12T17:57:34Z&lt;/published&gt;\n    &lt;title&gt;Attention Is All You Need&lt;/title&gt;\n    &lt;summary&gt;  The domin",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#the-syracuse-university-iot-portal",
    "href": "05_web_apis/webapi-1.html#the-syracuse-university-iot-portal",
    "title": "1. REST APIs",
    "section": "The Syracuse University Iot Portal",
    "text": "The Syracuse University Iot Portal\nThe Syracuse University Center for Emerging Network Technologies (CENT) created an Internet of Things (IoT) portal. The portal makes REST APIs available to IoT devices which commonly do not have the computing powert to perform these tasks, and to students so they don’t have to pay for a cloud service to use REST APIs in their projects.\nTo use it, go to:\nhttps://cent.ischool-iot.net/\nThen sign in with your SU NetID and password. When you sign in, you will see a box with “Your API Key”. This key is unique to you. You can use this to do API requests, as illustrated below.\n\nSwagger and Curl\nThe portal has a Swagger interface at:\nhttps://cent.ischool-iot.net/doc\nThis allows you to test the API’s in the browser. Swagger is a tool that helps you design, build, document, and consume REST APIs. The swagger interface shows how the API is called with a curl command, which allows you to make the same request from the command line.\nTo use it, you’ll need to authorize your API key:\n\nCopy your API key from cent.ischool-iot.net\nNow on the Swagger page click the “Authorize” button in the upper right. You will be prompted for your API key. Paste the API key you copied.\nYou should now be able to use any of the APIs listed on the page. To try it, click one of the drop downs, then Execute. For example, click the “google” drop down, then the one for /api/google/geocode. This API provides latitude and longitude for a given location. Hit the big Execute button. Type in a location in the “location” box (e.g., “Syracuse University”) then hit the Execute button (now smaller). In the Response body, you should see a JSON string that gives information about the location, including the latitude and longitude.\n\n\nExample\nLet’s use swagger to call the funny names API for 10 random names, then translate the curl command into Python code.\n\n# Translate the following code to use requests\n# curl -X 'GET' \\\n#   'https://cent.ischool-iot.net/api/funnyname/random?n=10' \\\n#   -H 'accept: application/json'\n\nuri = \"https://cent.ischool-iot.net/api/funnyname/random\"\nparams = {'n': 10}\nresponse = requests.get(uri, params=params)\nresponse.raise_for_status()\nfunny_people = response.json()\nfor person in funny_people:\n    print(person['first'], person['last'])\n\nYolanda Smyland\nGinger Snaps\nSal Ladd\nSally Mander\nLoren Dabucket\nIvana Sandwich\nCam Payne\nJoy Touda-World\nOliver Thingz\nOren Jouglad",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-1.html#http-headers",
    "href": "05_web_apis/webapi-1.html#http-headers",
    "title": "1. REST APIs",
    "section": "HTTP Headers",
    "text": "HTTP Headers\nHTTP headers are the key / value pairs that are sent in the request or response. They are used to pass additional information about the request or response. Unlike the query string, they are not part of the URL and are not visible to the user.\nIn the IoT portal, the headers are used to pass the API Key which verifies who you are.\nExample:\nUse the random API to get 10 intergers between 1 and 100.\n\n# Note: replace apikey with your API from the IoT portal\napikey = \"GETYOUROWNKEYFROMIOTPORTAL\"\nuri = \"https://cent.ischool-iot.net/api/random/int?\"\nparams = { 'count': 10, 'm\"in': 1, 'max': 100 }\nheaders = { \"X-API-KEY\": apikey} # goes in the header\nresponse.raise_for_status()\nprint(response.url)     # see the full URL no API key there\nnumbers = response.json()\nprint(numbers)\n\n\n\n\n\n\n\nCautionCode Challenge 1.2\n\n\n\nFigure out how to call these in the IoT portal:\n\nGoogle geocode API to take a location and get a latitute and longitude\nWeather API to get the weather for a latitude and longitude\n\nWrite a Streamlit app to input a location (e.g., “Syracuse University”) and return the current weather conditions. Use the st.metric to display the temperature and humidity with units. e.g. 56°F and 80% humidity.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nimport requests\nimport streamlit as st\n\nst.title(\"Streamlit Weather\")\nlocation = st.text_input(\"Enter a location\")\nif location:\n    apikey = \"\" # replace with your API key copied from the CENT IoT portal\n    headers = { \"X-Api-Key\": apikey }\n    geourl = \"https://cent.ischool-iot.net/api/google/geocode\"\n    params = { \"location\": location }\n    response = requests.get(geourl, params=params, headers=headers)\n    response.raise_for_status()\n    geodata = response.json()\n    \n    lat = geodata['results'][0]['geometry']['location'][\"lat\"]\n    lon = geodata['results'][0]['geometry']['location'][\"lng\"]\n    weatherurl = \"https://cent.ischool-iot.net/api/weather/current\"\n    params = { \"lat\": lat, \"lon\": lon, \"units\": \"imperial\" }\n    response = requests.get(weatherurl, params=params, headers=headers)\n    response.raise_for_status()\n    weatherdata = response.json()\n    temp = weatherdata['current']['temperature_2m']\n    tunits = weatherdata['current_units']['temperature_2m']\n    humidity = weatherdata['current']['relative_humidity_2m']\n    hunits = weatherdata['current_units']['relative_humidity_2m']\n    coltemp, colhumid = st.columns(2)\n    coltemp.metric(\"Temperature\", f\"{temp}{tunits}\")\n    colhumid.metric(\"Humidity\", f\"{humidity}{hunits}\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "1. REST APIs"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-3.html",
    "href": "05_web_apis/webapi-3.html",
    "title": "3. Creating your own APIs with FastAPI",
    "section": "",
    "text": "Important\n\n\n\nIn order to do this tutorial, you will need to install fastapi. To do that, open a terminal, activate your ist356 conda environment, then run:\npip install \"fastapi[standard]\"\nMake sure you put fastapi[standard] in quotes, as shown here.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "3. Creating your own APIs with FastAPI"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-3.html#api-vs-module",
    "href": "05_web_apis/webapi-3.html#api-vs-module",
    "title": "3. Creating your own APIs with FastAPI",
    "section": "API vs Module",
    "text": "API vs Module\nYou might be asking why host an API versus just writing a module? After all they have similar functionality of sharing and interacting with code and data.\nThere are some key differences between an API and a module:\n\nYou can hide the implementation details of your code with an API. Unlike a module, people only see the interface, not way to code was implemented.\nAn API can be accessed over the internet, while a module is typically used locally.\nAPI’s work across different programming languages, while modules are either restricted to the language they were written in (in our case, Python), or need to be wrapped with an intermediate (e.g., Cython for C-&gt;Python).\nSince an API is run remotely, it can be run on large high performance computing clusters, whereas modules are typically limited to whatever machine your are running on. This is particularly important for LLMs, which need large, power intensive GPU resources to work.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "3. Creating your own APIs with FastAPI"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-3.html#fast-api",
    "href": "05_web_apis/webapi-3.html#fast-api",
    "title": "3. Creating your own APIs with FastAPI",
    "section": "Fast API",
    "text": "Fast API\nhttps://fastapi.tiangolo.com/\n\nFast API is an easy-to-learn Python framework for building APIs.\nIt has a lot of features that make it a great choice for building APIs.\nWe will focus on a few simple functions so that you can understand how it works.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "3. Creating your own APIs with FastAPI"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-3.html#query-string-path-and-header-parameters",
    "href": "05_web_apis/webapi-3.html#query-string-path-and-header-parameters",
    "title": "3. Creating your own APIs with FastAPI",
    "section": "Query String Path, and Header Parameters",
    "text": "Query String Path, and Header Parameters\nFast API makes it easy to add parameters to your API.\n\nPython type hints are used to define the type of the parameter (int, str, etc.). If not type hints are provided, FastAPI will assume the input is string. This can lead to unintended consequences!\nThe Query() function is used to define a query parameter\nThe Header() function is used to define a header parameter\nPath parameters are defined by including the parameter in the URL path\n\nHere’s an example:\n\nfrom fastapi import FastAPI, Header, Query\n\napp = FastAPI()\n\n@app.get(\"/calculator/{operator}\")  # &lt;== path parameter\ndef read_item(operator: str, \n              a: int = Query(),     # &lt;== query parameter\n              b: int = Query(),     # &lt;== query parameter\n              h: str = Header()):   # &lt;== header parameter\n    if operator == \"add\":\n        result = a + b\n    elif operator == \"sub\":\n        result = a - b\n    elif operator == \"mul\":\n        result = a * b\n    elif operator == \"div\":\n        result = a / b\n    return {\n        \"operator\": operator,\n        \"a\": a,\n        \"b\": b,\n        \"result\": result,\n        \"h\": h\n    }\n\nSave that to a file called paramsapi.py, then start the server by opening a terminal and running:\npython -m fastapi dev paramsapi.py\nAgain, copy the URL. Now in your Jupyter notebook, run the following (replacing the base URL with what you copied):\n\nbaseurl = 'REPLACE_WITH_COPIED_URL'\noperator = 'add'\nurl = baseurl + 'calculator/' + operator\nparams = {'a': 1, 'b': 2}\nheaders = {'h': 'A header'}\nresponse = requests.get(url, params=params, headers=headers)\nresponse.raise_for_status()\nprint(response.json())\n\nThis will test the add method. To try the other methods, change what operator is set to. Notice that in doing so, we’re changing the URL.\n\n\n\n\n\n\nNote\n\n\n\nTo see the signficance of type hints in defining API functions, try removing the type hint from your paramsapi.py then run the requests.get function in your Jupyter notebook again (note that you don’t have to restart the server: FastAPI will detect the change to the code as soon as you save the changes, and automatically restart the server). In other words, try changing:\n\ndef read_item(operator: str,\n              a: int = Query(),\n              b: int = Query(),\n              h: str = Header()):\n\nto\n\ndef read_item(operator,\n              a = Query(),\n              b = Query(),\n              h = Header()):\n\nsave the changes, then re-run the requests.get for the add function in your notebook. Note the type of the returned a, b, and what the result is now: you should see that the returned a, b are strings (even though you passed in ints) and the result is now the string \"12\" rather than the integer 3. This is because FastAPI has assumed all the inputs are strings when you don’t provide type hints, so the function does \"1\" + \"2\" = \"12\" (why \"12\"?) instead of 1 + 2 = 3. Now add back the type hints to fix your code.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "3. Creating your own APIs with FastAPI"
    ]
  },
  {
    "objectID": "05_web_apis/webapi-3.html#handling-errors",
    "href": "05_web_apis/webapi-3.html#handling-errors",
    "title": "3. Creating your own APIs with FastAPI",
    "section": "Handling Errors",
    "text": "Handling Errors\nRemember status codes are a way to communicate the status of a request to the client. Here is list of the standard codes and what they mean:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Status\nError codes are a design contract which means you SHOULD use them as they are EXPECTED, but they are not REQURED.\nFastAPI makes it easy to handle errors. You can use the HTTPException (which you need to import from fastapi) to raise an error with a specific status code and message.\nTo illustrate, here’s our operator API from above, with a check added to make sure the provided operator is recognized:\n\nfrom fastapi import FastAPI, Header, Query, HTTPException\n\napp = FastAPI()\n\n@app.get(\"/calculator/{operator}\")  # &lt;== path parameter\ndef read_item(operator: str, \n              a: int = Query(),     # &lt;== query parameter\n              b: int = Query(),     # &lt;== query parameter\n              h: str = Header()):   # &lt;== header parameter\n    if operator == \"add\":\n        result = a + b\n    elif operator == \"sub\":\n        result = a - b\n    elif operator == \"mul\":\n        result = a * b\n    elif operator == \"div\":\n        result = a / b\n    else:\n        raise HTTPException(status_code=404, \n            detail=\"Operator not found. should be: add, sub, mul, div\")\n    return {\n        \"operator\": operator,\n        \"a\": a,\n        \"b\": b,\n        \"result\": result,\n        \"h\": h\n    }\n\n\n\n\n\n\n\nCautionCode Challenge 3.1\n\n\n\nDesign and build an API to search for flights (depart/arrive) by Airport Code.\nUse this dataset to get the source of your flights:\nhttps://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/flights/sample-flights.csv\nHere’s some examples of the the API endpoint you need to build:\n/api/flights/search?type=dep&code=OKA\n/api/flights/search?type=arr&code=KEY\nHave your code raise an HTTP Error with status code 400 if the requested type is not dep (for departures) or arr (for arrivals).\nTest your API using the Swagger UI.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom fastapi import FastAPI, Body, HTTPException\nimport pandas as pd\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/flights/sample-flights.csv\")\napp = FastAPI()\n\n\n@app.get(\"/api/flights/search\")\ndef search_flights(type: str, code: str):\n    '''\n    Search for flights by origin and destination\n    '''\n    if type == \"dep\":\n        flights = df[df[\"departure_airport_code\"] == code]\n    elif type == \"arr\":\n        flights = df[df[\"arrival_airport_code\"] == code]\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid type. Must be 'dep' or 'arr'\")\n    return flights.to_dict(orient=\"records\")",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "5. Web APIs",
      "3. Creating your own APIs with FastAPI"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html",
    "href": "06_web_scraping/scraping-1.html",
    "title": "1. Introduction to Web Scraping",
    "section": "",
    "text": "HTML (HyperText Markup Language) is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.\nCSS (Cascading Style Sheets) is a style sheet language used for describing the presentation of a document written in HTML or XML (including XML dialects such as SVG, MathML or XHTML).\nJavaScript is a programming language that conforms to the ECMAScript specification. JavaScript is high-level, often just-in-time compiled, and multi-paradigm. It has curly-bracket syntax, dynamic typing, prototype-based object-orientation, and first-class functions.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#the-anatomy-of-a-webpage-html-css-and-javascript",
    "href": "06_web_scraping/scraping-1.html#the-anatomy-of-a-webpage-html-css-and-javascript",
    "title": "1. Introduction to Web Scraping",
    "section": "",
    "text": "HTML (HyperText Markup Language) is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.\nCSS (Cascading Style Sheets) is a style sheet language used for describing the presentation of a document written in HTML or XML (including XML dialects such as SVG, MathML or XHTML).\nJavaScript is a programming language that conforms to the ECMAScript specification. JavaScript is high-level, often just-in-time compiled, and multi-paradigm. It has curly-bracket syntax, dynamic typing, prototype-based object-orientation, and first-class functions.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#inspecting-and-selecting",
    "href": "06_web_scraping/scraping-1.html#inspecting-and-selecting",
    "title": "1. Introduction to Web Scraping",
    "section": "Inspecting and Selecting",
    "text": "Inspecting and Selecting\nYou don’t need to be an HTML/CSS expert to scrape a webpage.\nYou just need to know how to inspect the webpage and select the elements you want to scrape.\nAs an illustration:\n\nOpen Google Chrome\nGo to this site: https://www.imdb.com/chart/top/\nRight click on the title of the first movie.\nClick on “Inspect”.\nObserve the class= attribute.\nNow right-click on the title of another movie.\nClick on “Inspect”.\nObserve the class= attribute.\n\nAre the class attirbutes the same? (Ans: yes!) Then we should be able to scrape the titles easily!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#playwright",
    "href": "06_web_scraping/scraping-1.html#playwright",
    "title": "1. Introduction to Web Scraping",
    "section": "Playwright",
    "text": "Playwright\nTo programatically interact with websites (i.e., to web scrape) we will use Playwright.\nPlaywright is a Node library to automate the Chromium, WebKit and Firefox browsers with a single API. It enables cross-browser web automation that is ever-green, capable, reliable and fast.\nYou can do everything a human can do in a web browser, just programmatically!\n\n\n\n\n\n\nNoteInstalling Playwright\n\n\n\nYou can install playwright using pip:\n\nOpen a Terminal.\nActivate your ist356 conda environment.\nRun pip install playwright\n\n\n\n\nInstalling the Chromium browser\nTo render and interact with web sites programattically playwright needs an open source browser. For that we will use Chromium. Chromium is an open-source browser that was developed by Google. Chrome (which is closed source) and many other popular browsers (such as Microsoft Edge) are based on Chromium.\nWe can install Chromium using playwright. Open a new terminal, activate your ist356 environment, then run:\npython -m playwright  install chromium --with-deps\nThis will install the chromium browser and all the dependencies needed to run it with playwright.\n\n\nMaking sure everything is working\nTo make sure its working, let’s take a screenshot with playwright from the command line. In the terminal run:\npython -m playwright screenshot https://www.google.com google.png\nThis should create a file called google.png in your current directory. Open it; you should see a screenshot of Google’s home page!",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#playwright-boilerplate-code",
    "href": "06_web_scraping/scraping-1.html#playwright-boilerplate-code",
    "title": "1. Introduction to Web Scraping",
    "section": "Playwright Boilerplate Code",
    "text": "Playwright Boilerplate Code\nThe following code will open a browser, navigate to a page and get the contents of the page.\n\n# pw-boilerplate.py\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://www.imdb.com/chart/top/\")\n    content = page.content()\n    print(content)\n\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\nTo run this, save it to a file on your computer (call it pw-boilerplate.py), then in your terminal run:\npython pw-boilerplate.py\nYou should briefly see a web page open then close. In your terminal, you’ll see a bunch of text get written out. That is the content of the website (what you see when you use the Inspect tool in your browser). The content is what we’ll be reading programatically.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#selectors",
    "href": "06_web_scraping/scraping-1.html#selectors",
    "title": "1. Introduction to Web Scraping",
    "section": "Selectors",
    "text": "Selectors\nTo scrape, you need to learn about selectors:\n\n\n\n\n\n\n\n\nExample\nTag\nSelector\n\n\n\n\nClass Selection\n&lt;div class=\"something\"&gt;...&lt;/div&gt;\n\"div.something\"\n\n\nId Selection\n&lt;table id=\"tid\"&gt;...&lt;/table&gt;\n\"table#tid\"\n\n\nTag Heirarchy Selection\n&lt;h1&gt;&lt;span&gt;...&lt;/span&gt;&lt;/h1&gt;\n\"h1 &gt; span\"\n\n\nMultiple Tag Selection\n&lt;h1&gt;...&lt;/h1&gt;&lt;h2&gt;...&lt;/h2&gt;\n\"h1, h2\"\n\n\nNext Selector\n&lt;h1&gt;&lt;/h1&gt;&lt;h2&gt;...&lt;/h2&gt;\n\"~ *\"\n\n\n\nhttps://www.w3schools.com/css/css_selectors.asp",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#getting-the-select-elements-tag-name",
    "href": "06_web_scraping/scraping-1.html#getting-the-select-elements-tag-name",
    "title": "1. Introduction to Web Scraping",
    "section": "Getting the Select Element’s tag name:",
    "text": "Getting the Select Element’s tag name:\nThere’s going to be times when you need to access the selected tag’s name.\nThis is useful when building out the page structure.\nWe need to fall back to JavaScript to accomplish this. evaluate() executes a JavaScript function in the context of the selected element.\n\nselected = page.query_selector(\"h1\")\ntag = selected.evaluate(\"el =&gt; el.tagName\")\ntext = selected.inner_text()\n\nprint(tag, text)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#example-selecting-the-title",
    "href": "06_web_scraping/scraping-1.html#example-selecting-the-title",
    "title": "1. Introduction to Web Scraping",
    "section": "Example: Selecting the title",
    "text": "Example: Selecting the title\nThis example will select the “title” from the IMBD Page (the &lt;h1&gt; tag):\n\n# pw-scrape_h1.py\n\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://www.imdb.com/chart/top/\")\n\n    # Let's scrape the heading off the page!\n    heading = page.query_selector(\"h1\")\n\n    # the tag name of the element\n    tag = heading.evaluate(\"el =&gt; el.tagName\")\n    print(tag)\n\n    # the contents of the element\n    print(heading.inner_text())\n    \n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\nAgain, to run this, save it to a file on your computer (call it pw-scrape_h1.py), then in your terminal run:\npython pw-scrape_h1.py\nYou should see Chromium briefly open at the website then close. In your terminal you should see the title of the page.\n\n\n\n\n\n\nCautionCode Challenge 1.1\n\n\n\nScrape the title off the course webiste: https://su-ist356-m003-fall-2025.github.io/course-home/\nHint: Open your browser at the course website and inspect the title. You should see that is in a class element set to “title”. This means you need to use the class selector to get the title.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://su-ist356-m003-fall-2025.github.io/course-home/\")\n\n    # Let's scrape the heading off the page!\n    heading = page.query_selector(\"h1.title\")\n    print(heading.inner_text())\n    \n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#scraping-multiple-elements",
    "href": "06_web_scraping/scraping-1.html#scraping-multiple-elements",
    "title": "1. Introduction to Web Scraping",
    "section": "Scraping Multiple Elements",
    "text": "Scraping Multiple Elements\nTo scrape multiple elements, you can use the query_selector_all method.\nEvery matching element will be returned in a list.\nThis example gets all the movie titles from the IMDB page.\n\n# pw-selectall_example.py\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://www.imdb.com/chart/top/\")\n    \n    # select the title by selector\n    elements_on_page = page.query_selector_all(\"h3.ipc-title__text\")\n\n    # loop through the elements and print the title\n    for element in elements_on_page:\n        title = element.inner_text()\n        print(title)\n\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\nRun that:\npython pw-selectall_example.py\nYou should see the list of all the movies listed on the website in your terminal. However, there’s also some extra stuff we don’t want. This illustrates one of the challenges of web scraping: customizing your script to get exactly what you want.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#challenges-of-scraping",
    "href": "06_web_scraping/scraping-1.html#challenges-of-scraping",
    "title": "1. Introduction to Web Scraping",
    "section": "Challenges of scraping",
    "text": "Challenges of scraping\n\nNothing is easy: Selecting exactly what you need from the page can be a challenge.\nNothing stays the same: When a website changes its layout, your scraper will break.\nNothing is consistent: Very little reuse from one page to the next.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#getting-only-what-we-want",
    "href": "06_web_scraping/scraping-1.html#getting-only-what-we-want",
    "title": "1. Introduction to Web Scraping",
    "section": "Getting only what we want",
    "text": "Getting only what we want\nTo get only the titles, we need to be more specific in our selector. Here’s a modified version of the code above:\n\n# pw-selectall_example2.py\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://www.imdb.com/chart/top/\")\n\n    # outer element that contains the list of 250 top movies\n    top_250_list = page.query_selector(\"ul.ipc-metadata-list\")\n\n    # same selector from there\n    elements_on_page = top_250_list.query_selector_all(\"h3.ipc-title__text\")\n    for element in elements_on_page:\n        title = element.inner_text()\n        print(title)\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n\nRunning the above, you should now get just the movie titles in your terminal.",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "06_web_scraping/scraping-1.html#using-playwright-in-a-jupyter-notebook",
    "href": "06_web_scraping/scraping-1.html#using-playwright-in-a-jupyter-notebook",
    "title": "1. Introduction to Web Scraping",
    "section": "Using playwright in a Jupyter notebook",
    "text": "Using playwright in a Jupyter notebook\nIn all the examples above we’ve run playwright in a Python script. If you tried running the same python in a Jupyter notebook cell, you’ll get an Error, Error: It looks like you are using Playwright Sync API inside the asyncio loop. Please use the Async API instead.. This has to do with differences between asynchronous and synchronous, and how Jupyter works. Synchronous vs asynchronous programming is a subject unto itself (if you’re interested, this page has a pretty good explainer), but long story short, to run playwright commands in a Jupyter notebook, you need to use their async API. You also need to prepend calls with the await command. For example, to load the IMDB top 250 page in a Jupyter notebook:\n\nfrom playwright.async_api import async_playwright\n\npw = await async_playwright().start()\nbrowser = await pw.chromium.launch(headless = False)\npage = await browser.new_page()\n\nawait page.goto(\"https://www.imdb.com/chart/top/\")\n\nYou can now inspect various elements in your notebook with this. For example, to get the top 250 list from the page:\n\ntop_250_list = await page.query_selector(\"ul.ipc-metadata-list\")\n\n\n\n\n\n\n\nCautionCode Challenge 1.2\n\n\n\nCreate an outline!\nScrape the Sections H2 and H3 from this page: https://ist256.com/fall2023/syllabus/\nPrint the titles, and detect the tag name so that you indent the H3 tags under the H2 tags.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\nfrom playwright.sync_api import Playwright, sync_playwright, expect\n\ndef run(playwright: Playwright) -&gt; None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"https://ist256.com/fall2023/syllabus/\")\n\n    # Let's scrape the heading off the page!\n    headings = page.query_selector_all(\"h2, h3\")\n    for heading in headings:\n        tag = heading.evaluate('el =&gt; el.tagName').lower()\n        text = heading.inner_text()\n        if tag == \"h2\":\n            print(text)\n        else:\n            print(f\"\\t{text}\")    \n\n    context.close()\n    browser.close()",
    "crumbs": [
      "<img src='/images/ischool_logo_reverseX2.png' height='30'>",
      "6. Web Scraping",
      "1. Introduction to Web Scraping"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IST 356: Programming Techniques for Data Analytics",
    "section": "",
    "text": "Room: Hinds Hall 021\nTime: M/W 3:45PM - 5:05PM\nInstructor: Collin Capano\nEmail: cdcapano@syr.edu\nOffice hours: Tuesdays 12PM - 1PM and Wednesdays 2PM - 3PM in Hinds Hall 323G\nSyllabus"
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "IST 356: Programming Techniques for Data Analytics",
    "section": "",
    "text": "Room: Hinds Hall 021\nTime: M/W 3:45PM - 5:05PM\nInstructor: Collin Capano\nEmail: cdcapano@syr.edu\nOffice hours: Tuesdays 12PM - 1PM and Wednesdays 2PM - 3PM in Hinds Hall 323G\nSyllabus"
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "IST 356: Programming Techniques for Data Analytics",
    "section": "Class Schedule",
    "text": "Class Schedule\nNote: schedule subject to change as we progress through the semester.\n\n\n\nWeek\nDates\nTopic\n\n\n\n\n1\n8/25, 8/29\nIntro; CLI and Conda\n\n\n2\n9/3\nPython review - 1\n\n\n3\n9/8, 9/10\nPython review - 2\n\n\n4\n9/15, 9/17\nUI\n\n\n5\n9/22, 9/24\nData wrangling - 1\n\n\n6\n9/29, 10/1\nData wrangling - 2\n\n\n7\n10/6, 10/8\nData wrangling - 3\n\n\n8\n10/15\nData wrangling - 4\n\n\n9\n10/20, 10/22\nExam 1 (10/20); Web APIs - 1\n\n\n10\n10/27, 10/29\nWeb APIs - 2\n\n\n11\n11/3, 11/5\nWeb scraping - 1\n\n\n12\n11/10, 11/12\nWeb scraping - 2\n\n\n13\n11/17, 11/19\nData visualization - 1\n\n\n14\n12/1, 12/3\nData visualization - 2\n\n\n15\n12/8\nExam 2\n\n\n16\n12/15\nProject due"
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "IST 356: Programming Techniques for Data Analytics",
    "section": "Important dates",
    "text": "Important dates\n\n\n\n\n\n\n\nDate\n\n\n\n\n\nMon. 8/25\nFirst day of class\n\n\nMon. 9/1\nLabor day - No class\n\n\nMon. 9/15\nAcademic/Financial drop deadline; Religious observance notification deadline\n\n\nMon. 10/13\nFall break - No class\n\n\nFri. 11/21\nWithdrawal deadline\n\n\n11/23-11/30\nThanksgiving Break - No class\n\n\nMon. 12/8\nLast day of class"
  }
]